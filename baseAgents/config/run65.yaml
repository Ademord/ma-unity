behaviors:
  Drone:
    trainer_type: ppo
    hyperparameters:
      batch_size: 1024
      buffer_size: 10240
      learning_rate: 0.0003
      beta: 0.01
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 256
      num_layers: 2
      vis_encode_type: simple
      # no lstm
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
      curiosity:
        gamma: 0.99
        strength: 0.02
        network_settings:
          hidden_units: 256
        learning_rate: 0.0003
    keep_checkpoints: 5
    max_steps: 20000000
    time_horizon: 64
    summary_freq: 30000

environment_parameters:
  # agent paremeters
  EnvironmentType: 0 # open, fire, forest
  steuernModus: 1
  m_EnableVFX: 0
  m_EnableTrainDebuggingLogs: 0
  # config for the specific behavior to be trained 
  # do not modify these lines below: use templates
  #################  BEHAVIOR: SPEED  ##################
  # base
  m_TrainMovingForward: 1
  m_TrainTargetSpeed: 1
  # octree
  leafNodeSize: 4
  m_AddOctreeObservations: 0
  m_TrainOctreeDiscovery: 0
  m_TrainLingerPolicy: 0
  m_AddPigeonObservations: 0
  # voxel
  m_TrainVoxelCollection: 1
  allowedToSeeVoxels: 0
  m_VoxelRewardStrength: 0.5 # <<<<<<<<<<<<<<<<
  NormalizeVoxelReward: 1 #  <<<<<<<<<<<<<<<<<<<<
  m_SpeedSensitivityToTargetsInFOV: 1 # all voxel-detecting behaviors should have this
  # shortest
  m_AddShortestPathObservations: 0
  m_TrainShortestPath: 0
  # detector
  m_LoadDetector: 1
  m_AddDetectorObservations: 1
  m_TrainObjectDetectionMaximization: 1
  NormalizeDetectionsReward: 1
  # curiosity
  m_AddSemanticCuriosityObservations: 0
  m_TrainSemanticCuriosity: 0
  # entropy
  m_AddSemanticEntropyObservations: 0
  m_TrainSemanticEntropy: 0
  #################    END BEHAVIOR   ##################


  
