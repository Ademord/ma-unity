{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f9f89eb-2d35-426d-ae3e-bba27489dd3d",
   "metadata": {},
   "source": [
    "# 🚀 Install, Import, and Log In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e55fe183-657a-4323-8641-7655c8a8f7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import random\n",
    "from functools import wraps\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1e49908-fed1-47a2-89b0-829c1e527d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(func):\n",
    "    @wraps(func)\n",
    "    def _time_it(*args, **kwargs):\n",
    "        start = int(round(time() * 1000))\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        finally:\n",
    "            end_ = int(round(time() * 1000)) - start\n",
    "            print(f\"Total execution time: {end_ if end_ > 0 else 0} ms\")\n",
    "    return _time_it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfbd7d2-c9c7-41f8-b272-178742073096",
   "metadata": {},
   "source": [
    "### 0️⃣ Step 0: Install W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8143d7fa-840c-4f68-94b9-f39d8f0775f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec61dda-606d-45c0-bbd6-6193665559b1",
   "metadata": {},
   "source": [
    "### 1️⃣ Step 1: Import W&B and Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef585afd-2998-4b9c-b4e9-65454d72e3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mademord\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72848fd9-bcdb-486b-83a5-13abf7e1bfb7",
   "metadata": {},
   "source": [
    "# 👩‍🔬 Define the Experiment and Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5314474d-5562-4e65-bf6b-7cad3756473e",
   "metadata": {},
   "source": [
    "### 2️⃣ Step 2: Track metadata and hyperparameters with `wandb.init`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa146a4e-9987-4728-9e73-afc9d8e6eec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    epochs=5,\n",
    "    classes=10,\n",
    "    kernels=[16, 32],\n",
    "    batch_size=128,\n",
    "    learning_rate=0.005,\n",
    "    dataset=\"MNIST\",\n",
    "    architecture=\"CNN\",\n",
    "    NUM_TRAINING_STEPS = 5, #10000000\n",
    "    NUM_TEST_STEPS = 1,\n",
    "    NUM_NEW_EXP = 1000,\n",
    "    BUFFER_SIZE = 10000,\n",
    "    worker_id=4,\n",
    "    time_scale=20\n",
    "    )\n",
    "env = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2308ec7-05f9-4f7d-8ea9-f6c1170d4c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters):\n",
    "    global env\n",
    "    \n",
    "    try:\n",
    "        env.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"pytorch-demo1\", config=hyperparameters):\n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "\n",
    "        # make the model, data, and optimization problem\n",
    "        model, env, criterion, optimizer = make(config)\n",
    "        print(model)\n",
    "\n",
    "        # and use them to train the model\n",
    "        train(model, env, criterion, optimizer, config)\n",
    "\n",
    "        # and test its final performance\n",
    "        test(model, env)\n",
    "\n",
    "    return model, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bb60f26-0808-4f56-b6ef-0cc41c3033f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make(config):\n",
    "    # Make the data\n",
    "    env, spec = make_env(config)\n",
    "    \n",
    "    # Make the model\n",
    "    # model = Agent(config.kernels, config.classes).to(device)\n",
    "    model = Agent(spec)\n",
    "\n",
    "    # Make the loss and optimizer\n",
    "    criterion = None # nn.CrossEntropyLoss()\n",
    "    optimizer = None # torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    \n",
    "    return model, env, criterion, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a15c7c2-720c-4c1e-b005-0707e9b16a9c",
   "metadata": {},
   "source": [
    "# 📡 Define the Env Loading and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb6b9020-dcf3-4ffc-80a5-5217d0fe5937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(config):\n",
    "    channel = EngineConfigurationChannel()\n",
    "    env = UE(\"run32_training\", seed=1, worker_id=config.worker_id, side_channels=[channel])\n",
    "    channel.set_configuration_parameters(time_scale = config.time_scale)\n",
    "    print(\"Environment created.\")\n",
    "    \n",
    "    env.reset()\n",
    "    behavior_name = list(env.behavior_specs)[0]\n",
    "    print(f\"Name of the behavior : {behavior_name}\")\n",
    "    \n",
    "    spec = env.behavior_specs[behavior_name]\n",
    "    print(f\"Type of the spec : {spec}\")\n",
    "    \n",
    "    return env, spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dc6a727-901a-4175-b712-a1d6364ec1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conventional and convolutional neural network\n",
    "class Agent():\n",
    "    def __init__(self, spec):\n",
    "        self.spec = spec\n",
    "    def get_action(self, decision_steps):\n",
    "        return self.spec.action_spec.random_action(len(decision_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1edc74-99d3-4177-8d78-1db24cb7fa77",
   "metadata": {},
   "source": [
    "# 👟 Define Training Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e807cbd-e6f9-4c6e-aceb-811495f82bdc",
   "metadata": {},
   "source": [
    "### 3️⃣ Step 3. Track gradients with `wandb.watch` and everything else with `wandb.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bdd68cb-68dd-44d6-9d7c-03f022136d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure\n",
    "def train(model, env, criterion, optimizer, config):\n",
    "#     wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "    behavior_name = list(env.behavior_specs)[0]\n",
    "    cumulative_rewards: List[float] = []\n",
    "        \n",
    "    for episode in range(config.NUM_TRAINING_STEPS):\n",
    "        env.reset()\n",
    "        decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "        tracked_agent = -1 # -1 indicates not yet tracking\n",
    "        done = False # For the tracked_agent\n",
    "        episode_rewards = 0 # For the tracked_agent\n",
    "        \n",
    "        while not done:\n",
    "            # Track the first agent we see if not tracking \n",
    "            # Note : len(decision_steps) = [numberb of agents that requested a decision]\n",
    "            if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                tracked_agent = decision_steps.agent_id[0] \n",
    "\n",
    "            # Generate an action for all agents\n",
    "            # these are the observations\n",
    "#             print(decision_steps[0])\n",
    "            action = model.get_action(decision_steps)\n",
    "    #         print(action.discrete)\n",
    "            # Set the actions\n",
    "            env.set_actions(behavior_name, action)\n",
    "\n",
    "            # Move the simulation forward\n",
    "            env.step()\n",
    "\n",
    "            # Get the new simulation results\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            if tracked_agent in decision_steps: # The agent requested a decision\n",
    "                episode_rewards += decision_steps[tracked_agent].reward\n",
    "            if tracked_agent in terminal_steps: # The agent terminated its episode\n",
    "                print(\"reward on terminal step:\", terminal_steps[tracked_agent].reward)\n",
    "                episode_rewards += terminal_steps[tracked_agent].reward\n",
    "                done = True\n",
    "        print(\"Training step \", episode + 1, \"\\treward \", episode_rewards)\n",
    "        cumulative_rewards.append(episode_rewards)\n",
    "        train_log(episode_rewards, episode)\n",
    "\n",
    "\n",
    "    # Show the training graph\n",
    "    plt.plot(range(NUM_TRAINING_STEPS), cumulative_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af41c215-fdab-4a17-a306-dee1b85f5a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_log(reward, episode):\n",
    "#     loss = float(loss)\n",
    "    reward = float(reward)\n",
    "\n",
    "    # where the magic happens\n",
    "    wandb.log({\"episode\": episode, \"reward\": reward}) #, step=example_ct\n",
    "    print(f\"Reward after \" + str(episode).zfill(5) + f\" episodes: {reward:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13b8c33-5d2d-457c-9abb-9361e273f50f",
   "metadata": {},
   "source": [
    "# 🧪 Define Testing Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b01425d-94c6-4573-9f9b-7f502b615a97",
   "metadata": {},
   "source": [
    "#### 4️⃣ Optional Step 4: Call `wandb.save`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08a9a3d0-7baf-4a78-900c-64b47e0b3d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    # Run the model on some test examples\n",
    "    with torch.no_grad():\n",
    "        behavior_name = list(env.behavior_specs)[0]\n",
    "        cumulative_rewards: List[float] = []\n",
    "\n",
    "        for episode in range(config.NUM_TEST_STEPS):\n",
    "            env.reset()\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            tracked_agent = -1 # -1 indicates not yet tracking\n",
    "            done = False # For the tracked_agent\n",
    "            episode_rewards = 0 # For the tracked_agent\n",
    "\n",
    "            while not done:\n",
    "                # Track the first agent we see if not tracking \n",
    "                # Note : len(decision_steps) = [number of agents that requested a decision]\n",
    "                if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                    tracked_agent = decision_steps.agent_id[0] \n",
    "\n",
    "                # Generate an action for all agents\n",
    "                action = model.get_action(decision_steps)\n",
    "\n",
    "                # Set the actions\n",
    "                env.set_actions(behavior_name, action)\n",
    "\n",
    "                # Move the simulation forward\n",
    "                env.step()\n",
    "\n",
    "                # Get the new simulation results\n",
    "                decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "                if tracked_agent in decision_steps: # The agent requested a decision\n",
    "                    episode_rewards += decision_steps[tracked_agent].reward\n",
    "                if tracked_agent in terminal_steps: # The agent terminated its episode\n",
    "                    print(\"reward on terminal step:\", terminal_steps[tracked_agent].reward)\n",
    "                    episode_rewards += terminal_steps[tracked_agent].reward\n",
    "                    done = True\n",
    "            cumulative_rewards.append(episode_rewards)\n",
    "\n",
    "        print(f\"Average reward of the model after {config.NUM_TEST_STEPS} \" +\n",
    "              f\"test episodes: {numpy.average(cumulative_rewards)}%\")\n",
    "\n",
    "        wandb.log({\"test_average_reward\": numpy.average(cumulative_rewards)})\n",
    "\n",
    "    # Save the model in the exchangeable ONNX format\n",
    "    torch.onnx.export(model, [], \"model.onnx\")\n",
    "    wandb.save(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e3150d-5d2b-48dd-a229-b9d9f8eb8b7f",
   "metadata": {},
   "source": [
    "# 🏃‍♀️ Run training and watch your metrics live on [wandb.ai](https://wandb.ai)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cb9620-5a48-4fc2-9f2c-422623703720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">ethereal-frost-17</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ademord/pytorch-demo1\" target=\"_blank\">https://wandb.ai/ademord/pytorch-demo1</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ademord/pytorch-demo1/runs/ff9wmrgj\" target=\"_blank\">https://wandb.ai/ademord/pytorch-demo1/runs/ff9wmrgj</a><br/>\n",
       "                Run data is saved locally in <code>/host/unity_builds/run32_training/wandb/run-20210604_144858-ff9wmrgj</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment created.\n",
      "Name of the behavior : Hummingbird?team=0\n",
      "Type of the spec : BehaviorSpec(observation_specs=[ObservationSpec(shape=(44,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='RayPerceptionSensor'), ObservationSpec(shape=(3,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='StackingSensor_size3_VectorSensor_size1')], action_spec=ActionSpec(continuous_size=0, discrete_branches=(3, 3, 3)))\n",
      "<__main__.Agent object at 0x7f035f7e9c40>\n",
      "reward on terminal step: 0.0\n",
      "Training step  1 \treward  0.0\n",
      "Reward after 00000 episodes: 0.000\n"
     ]
    }
   ],
   "source": [
    "# Build, train and analyze the model with the pipeline\n",
    "model, env = model_pipeline(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ce5513-e1eb-453b-a0b4-689330842816",
   "metadata": {},
   "source": [
    "# 🧹 Test Hyperparameters with Sweeps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13526307-58d0-4263-8b59-714525a51117",
   "metadata": {},
   "source": [
    "## [Check out Hyperparameter Optimization in PyTorch using W&B Sweep $\\rightarrow$](https://colab.research.google.com/drive/1QTIK23LBuAkdejbrvdP5hwBGyYlyEJpT?usp=sharing)\n",
    "\n",
    "Running a hyperparameter sweep with Weights & Biases is very easy. There are just 3 simple steps:\n",
    "\n",
    "1. **Define the sweep:** We do this by creating a dictionary or a [YAML file](https://docs.wandb.com/library/sweeps/configuration) that specifies the parameters to search through, the search strategy, the optimization metric et all.\n",
    "\n",
    "2. **Initialize the sweep:** \n",
    "`sweep_id = wandb.sweep(sweep_config)`\n",
    "\n",
    "3. **Run the sweep agent:** \n",
    "`wandb.agent(sweep_id, function=train)`\n",
    "\n",
    "And voila! That's all there is to running a hyperparameter sweep!\n",
    "<img src=\"https://imgur.com/UiQKg0L.png\" alt=\"Weights & Biases\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215b498a-9711-46b3-afc7-4cd3742df753",
   "metadata": {},
   "source": [
    "# 🤓 Advanced Setup\n",
    "1. [Environment variables](https://docs.wandb.com/library/environment-variables): Set API keys in environment variables so you can run training on a managed cluster.\n",
    "2. [Offline mode](https://docs.wandb.com/library/technical-faq#can-i-run-wandb-offline): Use `dryrun` mode to train offline and sync results later.\n",
    "3. [On-prem](https://docs.wandb.com/self-hosted): Install W&B in a private cloud or air-gapped servers in your own infrastructure. We have local installations for everyone from academics to enterprise teams.\n",
    "4. [Sweeps](https://docs.wandb.com/sweeps): Set up hyperparameter search quickly with our lightweight tool for tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9360baa-2b59-4cb6-86f9-8dca4deb31f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n in range(NUM_TRAINING_STEPS):\n",
    "#   new_exp,_ = Trainer.generate_trajectories(env, qnet, NUM_NEW_EXP, epsilon=0.1)\n",
    "#   random.shuffle(experiences)\n",
    "#   if len(experiences) > BUFFER_SIZE:\n",
    "#     experiences = experiences[:BUFFER_SIZE]\n",
    "#   experiences.extend(new_exp)\n",
    "#   Trainer.update_q_net(qnet, optim, experiences, 3)\n",
    "#   _, rewards = Trainer.generate_trajectories(env, qnet, 100, epsilon=0)\n",
    "#   cumulative_rewards.append(rewards)\n",
    "#   print(\"Training step \", n+1, \"\\treward \", rewards)\n",
    "#   print()\n",
    "\n",
    "\n",
    "# env.close()\n",
    "\n",
    "# # Show the training graph\n",
    "# plt.plot(range(NUM_TRAINING_STEPS), cumulative_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f84bef9-4e0b-4829-98f6-9d5548a5b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  class UnityEnv(gym.Env):\n",
    "#     \"\"\"\n",
    "#     Provides Gym wrapper for Unity Learning Environments.\n",
    "#     Multi-agent environments use lists for object types, as done here:\n",
    "#     https://github.com/openai/multiagent-particle-envs\n",
    "#     \"\"\"\n",
    " \n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         environment_filename: str,\n",
    "#         dimensions: int = [],   #Added\n",
    "#         timescale: int = 1,     #Added\n",
    "#         worker_id: int = 0,\n",
    "#         use_visual: bool = False,\n",
    "#         uint8_visual: bool = False,\n",
    "#         multiagent: bool = False,\n",
    "#         flatten_branched: bool = False,\n",
    "#         no_graphics: bool = False,\n",
    "#         allow_multiple_visual_obs: bool = False,\n",
    "#         set_config: bool = True,    #Added\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Environment initialization\n",
    "#         :param environment_filename: The UnityEnvironment path or file to be wrapped in the gym.\n",
    "#         :param worker_id: Worker number for environment.\n",
    "#         :param use_visual: Whether to use visual observation or vector observation.\n",
    "#         :param uint8_visual: Return visual observations as uint8 (0-255) matrices instead of float (0.0-1.0).\n",
    "#         :param multiagent: Whether to run in multi-agent mode (lists of obs, reward, done).\n",
    "#         :param flatten_branched: If True, turn branched discrete action spaces into a Discrete space rather than\n",
    "#             MultiDiscrete.\n",
    "#         :param no_graphics: Whether to run the Unity simulator in no-graphics mode\n",
    "#         :param allow_multiple_visual_obs: If True, return a list of visual observations instead of only one.\n",
    "#         \"\"\"\n",
    "#         base_port = 5005\n",
    "#         if environment_filename is None:\n",
    "#             base_port = UnityEnvironment.DEFAULT_EDITOR_PORT\n",
    " \n",
    "#         channel = EngineConfigurationChannel()        # Added\n",
    " \n",
    " \n",
    "#         #Added\n",
    "#         if set_config == True:\n",
    "#             channel.set_configuration_parameters(time_scale=timescale, width=dimensions[0], height=dimensions[1])\n",
    "#         #Added\n",
    " \n",
    "#         self._env = UnityEnvironment(\n",
    "#             environment_filename,\n",
    "#             worker_id,\n",
    "#             base_port=base_port,\n",
    "#             no_graphics=no_graphics,\n",
    "#             side_channels=[channel],        # Added\n",
    "#         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
