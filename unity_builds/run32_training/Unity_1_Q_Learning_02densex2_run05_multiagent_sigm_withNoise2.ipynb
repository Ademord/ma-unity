{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# # wandb.init(config=args)a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml-agents already installed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import mlagents\n",
    "    from mlagents_envs.environment import UnityEnvironment as UE\n",
    "    from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "    print(\"ml-agents already installed\")\n",
    "except ImportError:\n",
    "#     !pip install mlagents==0.26.0\n",
    "    print(\"Installed ml-agents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "from math import floor\n",
    "\n",
    "\n",
    "class VisualQNetwork(torch.nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    input_shape: Tuple[int], \n",
    "    encoding_size: int, \n",
    "    output_size: int\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Creates a neural network that takes as input a batch of images (3\n",
    "    dimensional tensors) and outputs a batch of outputs (1 dimensional\n",
    "    tensors)\n",
    "    \"\"\"\n",
    "    super(VisualQNetwork, self).__init__()\n",
    "#     height = input_shape[0]\n",
    "#     width = input_shape[1]\n",
    "#     initial_channels = input_shape[2]\n",
    "#     conv_1_hw = self.conv_output_shape((height, width), 8, 4)\n",
    "#     conv_2_hw = self.conv_output_shape(conv_1_hw, 4, 2)\n",
    "#     self.final_flat = conv_2_hw[0] * conv_2_hw[1] * 32\n",
    "    \n",
    "    \n",
    "#     self.conv1 = torch.nn.Conv2d(initial_channels, 16, [8, 8], [4, 4])\n",
    "#     self.conv2 = torch.nn.Conv2d(16, 32, [4, 4], [2, 2])\n",
    "#     self.dense1 = torch.nn.Linear(self.final_flat, encoding_size)\n",
    "\n",
    "    \n",
    "    self.dense1 = torch.nn.Linear(input_shape[0], encoding_size)\n",
    "    self.dense2 = torch.nn.Linear(encoding_size, encoding_size)\n",
    "    \n",
    "    self.dense2_x1 = torch.nn.Linear(encoding_size, output_size)\n",
    "    self.dense2_x2 = torch.nn.Linear(encoding_size, output_size)\n",
    "    self.dense2_x3 = torch.nn.Linear(encoding_size, output_size)\n",
    "   \n",
    "    self.act = torch.nn.Sigmoid() # ReLU\n",
    "\n",
    "    \n",
    "    \n",
    "  def forward(self, visual_obs: torch.tensor):\n",
    "#     print(\"torch input size:\", visual_obs.size())\n",
    "#     visual_obs = visual_obs.permute(0, 3, 1, 2)\n",
    "#     conv_1 = torch.relu(self.conv1(visual_obs))\n",
    "#     conv_2 = torch.relu(self.conv2(conv_1))\n",
    "#     hidden = self.dense1(conv_2.reshape([-1, self.final_flat]))\n",
    "\n",
    "    hidden = self.dense1(visual_obs)\n",
    "    hidden = self.act(hidden)\n",
    "\n",
    "    hidden = self.dense2(hidden)\n",
    "    hidden = self.act(hidden)\n",
    "\n",
    "    x1 = self.dense2_x1(hidden)\n",
    "    x1 = self.act(x1)\n",
    "    x2 = self.dense2_x2(hidden)\n",
    "    x2 = self.act(x2)\n",
    "    x3 = self.dense2_x3(hidden)\n",
    "    x3 = self.act(x3)\n",
    "\n",
    "    return x1, x2, x3\n",
    "\n",
    "  @staticmethod\n",
    "  def conv_output_shape(\n",
    "    h_w: Tuple[int, int],\n",
    "    kernel_size: int = 1,\n",
    "    stride: int = 1,\n",
    "    pad: int = 0,\n",
    "    dilation: int = 1,\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Computes the height and width of the output of a convolution layer.\n",
    "    \"\"\"\n",
    "    h = floor(\n",
    "      ((h_w[0] + (2 * pad) - (dilation * (kernel_size - 1)) - 1) / stride) + 1\n",
    "    )\n",
    "    w = floor(\n",
    "      ((h_w[1] + (2 * pad) - (dilation * (kernel_size - 1)) - 1) / stride) + 1\n",
    "    )\n",
    "    return h, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import NamedTuple, List\n",
    "\n",
    "\n",
    "class Experience(NamedTuple):\n",
    "  \"\"\"\n",
    "  An experience contains the data of one Agent transition.\n",
    "  - Observation\n",
    "  - Action\n",
    "  - Reward\n",
    "  - Done flag\n",
    "  - Next Observation\n",
    "  \"\"\"\n",
    "\n",
    "  obs: np.ndarray\n",
    "  action: np.ndarray\n",
    "  reward: float\n",
    "  done: bool\n",
    "  next_obs: np.ndarray\n",
    "\n",
    "# A Trajectory is an ordered sequence of Experiences\n",
    "Trajectory = List[Experience]\n",
    "\n",
    "# A Buffer is an unordered list of Experiences from multiple Trajectories\n",
    "Buffer = List[Experience]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import ActionTuple, BaseEnv\n",
    "from typing import Dict\n",
    "import random\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "  @staticmethod\n",
    "  def generate_trajectories(\n",
    "    env: BaseEnv, q_net: VisualQNetwork, buffer_size: int, epsilon: float\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Given a Unity Environment and a Q-Network, this method will generate a\n",
    "    buffer of Experiences obtained by running the Environment with the Policy\n",
    "    derived from the Q-Network.\n",
    "    :param BaseEnv: The UnityEnvironment used.\n",
    "    :param q_net: The Q-Network used to collect the data.\n",
    "    :param buffer_size: The minimum size of the buffer this method will return.\n",
    "    :param epsilon: Will add a random normal variable with standard deviation.\n",
    "    epsilon to the value heads of the Q-Network to encourage exploration.\n",
    "    :returns: a Tuple containing the created buffer and the average cumulative\n",
    "    the Agents obtained.\n",
    "    \"\"\"\n",
    "    # Create an empty Buffer\n",
    "    buffer: Buffer = []\n",
    "\n",
    "    # Reset the environment\n",
    "    env.reset()\n",
    "    # Read and store the Behavior Name of the Environment\n",
    "    behavior_name = list(env.behavior_specs)[0]\n",
    "    # Read and store the Behavior Specs of the Environment\n",
    "    spec = env.behavior_specs[behavior_name]\n",
    "\n",
    "    # Create a Mapping from AgentId to Trajectories. This will help us create\n",
    "    # trajectories for each Agents\n",
    "    dict_trajectories_from_agent: Dict[int, Trajectory] = {}\n",
    "    # Create a Mapping from AgentId to the last observation of the Agent\n",
    "    dict_last_obs_from_agent: Dict[int, np.ndarray] = {}\n",
    "    # Create a Mapping from AgentId to the last observation of the Agent\n",
    "    dict_last_action_from_agent: Dict[int, np.ndarray] = {}\n",
    "    # Create a Mapping from AgentId to cumulative reward (Only for reporting)\n",
    "    dict_cumulative_reward_from_agent: Dict[int, float] = {}\n",
    "    # Create a list to store the cumulative rewards obtained so far\n",
    "    cumulative_rewards: List[float] = []\n",
    "    \n",
    "    \n",
    "    entered_terminal = False\n",
    "    while len(buffer) < buffer_size:  # While not enough data in the buffer\n",
    "      # Get the Decision Steps and Terminal Steps of the Agents\n",
    "      decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    \n",
    "        # For all Agents with a Terminal Step:\n",
    "      for agent_id_terminated in terminal_steps:\n",
    "#         print(\"entered agent with terminal step\")\n",
    "#         print(agent_id_terminated)\n",
    "\n",
    "        # Create its last experience (is last because the Agent terminated)\n",
    "        last_experience = Experience(\n",
    "          obs=dict_last_obs_from_agent[agent_id_terminated].copy(),\n",
    "          reward=terminal_steps[agent_id_terminated].reward,\n",
    "          done=not terminal_steps[agent_id_terminated].interrupted,\n",
    "          action=dict_last_action_from_agent[agent_id_terminated].copy(),\n",
    "          next_obs=terminal_steps[agent_id_terminated].obs[0],\n",
    "        )\n",
    "        # Clear its last observation and action (Since the trajectory is over)\n",
    "        dict_last_obs_from_agent.pop(agent_id_terminated)\n",
    "        dict_last_action_from_agent.pop(agent_id_terminated)\n",
    "        # Report the cumulative reward\n",
    "        cumulative_reward = (\n",
    "          dict_cumulative_reward_from_agent.pop(agent_id_terminated)\n",
    "          + terminal_steps[agent_id_terminated].reward\n",
    "        )\n",
    "#         print(\"cumulative reward: \", cumulative_reward)\n",
    "        cumulative_rewards.append(cumulative_reward) #  - 50\n",
    "        # Add the Trajectory and the last experience to the buffer\n",
    "        buffer.extend(dict_trajectories_from_agent.pop(agent_id_terminated))\n",
    "        buffer.append(last_experience)\n",
    "        entered_terminal = True\n",
    "\n",
    "      # For all Agents with a Decision Step:\n",
    "      for agent_id_decisions in decision_steps:\n",
    "        # If the Agent does not have a Trajectory, create an empty one\n",
    "        if agent_id_decisions not in dict_trajectories_from_agent:\n",
    "          dict_trajectories_from_agent[agent_id_decisions] = []\n",
    "          dict_cumulative_reward_from_agent[agent_id_decisions] = 0\n",
    "\n",
    "        # If the Agent requesting a decision has a \"last observation\"\n",
    "        if agent_id_decisions in dict_last_obs_from_agent:\n",
    "          # Create an Experience from the last observation and the Decision Step\n",
    "          exp = Experience(\n",
    "            obs=dict_last_obs_from_agent[agent_id_decisions].copy(),\n",
    "            reward=decision_steps[agent_id_decisions].reward, #  - 0.05\n",
    "            done=False,\n",
    "            action=dict_last_action_from_agent[agent_id_decisions].copy(),\n",
    "            next_obs=decision_steps[agent_id_decisions].obs[0],\n",
    "          )\n",
    "          # Update the Trajectory of the Agent and its cumulative reward\n",
    "          dict_trajectories_from_agent[agent_id_decisions].append(exp)\n",
    "          dict_cumulative_reward_from_agent[agent_id_decisions] += (\n",
    "            decision_steps[agent_id_decisions].reward\n",
    "          )\n",
    "        # Store the observation as the new \"last observation\"\n",
    "        dict_last_obs_from_agent[agent_id_decisions] = (\n",
    "          decision_steps[agent_id_decisions].obs[0]\n",
    "        )\n",
    "\n",
    "      # Generate an action for all the Agents that requested a decision\n",
    "      # Compute the values for each action given the observation    \n",
    "      act1, act2, act3 = q_net(torch.from_numpy(decision_steps.obs[0]))\n",
    "    \n",
    "      if len(decision_steps) == 0:\n",
    "#             print(\"error: no more observations ! \")\n",
    "            env.step()\n",
    "            continue\n",
    "#       if act1.size == 0:\n",
    "#             print(\"error: Action space received = 0\")\n",
    "#             env.step()\n",
    "#             continue\n",
    "            \n",
    "      # get actions as arrays\n",
    "      act1 = act1.detach().numpy()\n",
    "      act2 = act2.detach().numpy()\n",
    "      act3 = act3.detach().numpy()\n",
    "    \n",
    "#       print(\"action received from QNetwork: \", act1)\n",
    "#       print(\"action received from QNetwork: \", act2)\n",
    "#       print(\"action received from QNetwork: \", act3)\n",
    "      act1 += epsilon * np.random.randn(act1.shape[0], act1.shape[1]).astype(np.float32)\n",
    "      act2 += epsilon * np.random.randn(act1.shape[0], act1.shape[1]).astype(np.float32)\n",
    "      act3 += epsilon * np.random.randn(act1.shape[0], act1.shape[1]).astype(np.float32)\n",
    "      \n",
    "      # pick the best action using argmax\n",
    "      act1 = np.argmax(act1, axis=1)\n",
    "      act2 = np.argmax(act2, axis=1)\n",
    "      act3 = np.argmax(act3, axis=1)\n",
    "#       print(\"action received from argmax: \", act1)\n",
    "#       print(\"action received from argmax: \", act1.shape)\n",
    "#       act1 = np.array([act1])\n",
    "#       act2 = np.array([act2])\n",
    "#       act3 = np.array([act3])\n",
    "      act1 = np.expand_dims(act1, axis=1)\n",
    "      act2 = np.expand_dims(act2, axis=1)\n",
    "      act3 = np.expand_dims(act3, axis=1)\n",
    "#       print(\"action received from argmax expanded: \", act1)\n",
    "#       print(\"action received from argmax expanded: \", act1.shape)\n",
    "\n",
    "      # map action index 2 to -1 for the agent to move backwards, left, and rotate left\n",
    "      act1[act1 > 1] = -1\n",
    "      act2[act2 > 1] = -1\n",
    "      act3[act3 > 1] = -1\n",
    "\n",
    "      # format to numpy arrays\n",
    "#       print(\"action received from mapping: \", act1)\n",
    "#       try:\n",
    "# #         actions_values = np.array([act1, act2, act3]).reshape(3,3)\n",
    "#         actions_values = np.vstack((act1, act2, act3))\n",
    "#       except:\n",
    "#         actions_values = np.zeros((3,3))\n",
    "#         print(\"error: network received an input of size 0 and i caught the error :/\")\n",
    "\n",
    "        #      0-8nt(type(actions_values))\n",
    "    \n",
    "    \n",
    "#       actions =  np.vstack((act1, act2, act3))\n",
    "#       actions =  np.vstack((act1, act2, act3))\n",
    "\n",
    "      temp = np.hstack((act1, act2, act3))\n",
    "#       print(\"actions stacked with hstack:\", temp)\n",
    "#       print(\"actions stacked with hstack: shape:\", temp.shape)\n",
    "#       temp = np.concatenate((act1, act2, act3), axis=1)\n",
    "#       print(\"actions stacked with concat:\", temp)\n",
    "#       print(\"actions stacked with concat: shape:\", temp.shape)\n",
    "        \n",
    "#       actions_values = np.zeros((3,3))\n",
    "#       # Add some noise with epsilon to the values\n",
    "#       actions_values += epsilon * np.random.randn(actions_values.shape[0], actions_values.shape[1]).astype(np.float32)\n",
    "#       actions = np.argmax(actions_values, axis=1)\n",
    "      \n",
    "      actions = temp\n",
    "#       print(\"final actions: \", actions)\n",
    "#       print(\"final actions shape: \", actions.shape)\n",
    "      actions.resize((len(decision_steps), 3))\n",
    "#       print(\"decision steps size:\", len(decision_steps))\n",
    "#       print(\"final actions after resize: \", actions)\n",
    "#       print(\"final actions shape: \", actions.shape)\n",
    "\n",
    "\n",
    "      # Store the action that was picked, it will be put in the trajectory later\n",
    "      for agent_index, agent_id in enumerate(decision_steps.agent_id):\n",
    "        dict_last_action_from_agent[agent_id] = actions[agent_index]\n",
    "#       print(\"dict last action: \", dict_last_action_from_agent)\n",
    "\n",
    "        \n",
    "      # Set the actions in the environment\n",
    "      # Unity Environments expect ActionTuple instances.\n",
    "      action_tuple = ActionTuple()\n",
    "      action_tuple.add_discrete(actions)\n",
    "#       print(\"filtered action received from QNetwork: \", action_tuple.discrete)\n",
    "      env.set_actions(behavior_name, action_tuple)\n",
    "      # Perform a step in the simulation\n",
    "      env.step()\n",
    "    return buffer, np.mean(cumulative_rewards)\n",
    "\n",
    "  @staticmethod\n",
    "  def update_q_net(\n",
    "    q_net: VisualQNetwork, \n",
    "    optimizer: torch.optim, \n",
    "    buffer: Buffer, \n",
    "    action_size: int\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Performs an update of the Q-Network using the provided optimizer and buffer\n",
    "    \"\"\"\n",
    "    def calculate_bellman_loss(next_pred_action, pred_action, reward, done, GAMMA, batch, action_size, action):\n",
    "        # Use the Bellman equation to update the Q-Network\n",
    "        target = (\n",
    "          reward\n",
    "          + (1.0 - done)\n",
    "          * GAMMA\n",
    "          * torch.max(next_pred_action.detach(), dim=1, keepdim=True).values\n",
    "        ).double()\n",
    "#         print(\"next_act_prediction:\", next_pred_action.detach().numpy())\n",
    "        \n",
    "#         print(\"Target:\", target)\n",
    "#         print(\"Target shape:\", target.shape)\n",
    "\n",
    "#         print(\"action:\", action)\n",
    "#         print(\"action shape: \", action.shape)\n",
    "        assert(action.shape[0] == len(batch))\n",
    "        action[action < 0] = 2\n",
    "#         print(\"action after correction:\", action)\n",
    "        \n",
    "        mask = np.eye(action_size)[action]\n",
    "#         mask = torch.zeros((len(batch), action_size))  \n",
    "#         print(\"mask: \", mask)\n",
    "#         print(\"mask shape: \", mask.shape)\n",
    "#         mask.scatter_(1, action, 1)\n",
    "#         print(\"mask after scatter: \", mask)\n",
    "        mask = torch.from_numpy(mask).double()\n",
    "#         print(\"pred_action: error\", pred_action)\n",
    "#         print(type(pred_action))\n",
    "#         print(pred_action.dtype)\n",
    "#         print(type(mask))\n",
    "#         print(mask.dtype)\n",
    "        prediction = torch.sum(pred_action.double() * mask, dim=1, keepdim=True)\n",
    "#         print(\"act_prediction:\", pred_action.detach().numpy())\n",
    "#         print(\"prediction: \", prediction)\n",
    "#         print(\"prediction shaPE: \", prediction.shape)\n",
    "#         print(\"prediction type: \", type(prediction))\n",
    "#         print(\"prediction dtype: \", prediction.dtype)\n",
    "        \n",
    "        criterion = torch.nn.MSELoss()\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    BATCH_SIZE = 1000\n",
    "    NUM_EPOCH = 3\n",
    "    GAMMA = 0.9\n",
    "    batch_size = min(len(buffer), BATCH_SIZE)\n",
    "    random.shuffle(buffer)\n",
    "    # Split the buffer into batches\n",
    "    batches = [\n",
    "      buffer[batch_size * start : batch_size * (start + 1)]\n",
    "      for start in range(int(len(buffer) / batch_size))\n",
    "    ]\n",
    "    for _ in range(NUM_EPOCH):\n",
    "      for batch in batches:\n",
    "        # Create the Tensors that will be fed in the network\n",
    "        obs = torch.from_numpy(np.stack([ex.obs for ex in batch]))\n",
    "        reward = torch.from_numpy(\n",
    "          np.array([ex.reward for ex in batch], dtype=np.float32).reshape(-1, 1)\n",
    "        )\n",
    "        done = torch.from_numpy(\n",
    "          np.array([ex.done for ex in batch], dtype=np.float32).reshape(-1, 1)\n",
    "        )\n",
    "        action = torch.from_numpy(np.stack([ex.action for ex in batch]))\n",
    "        next_obs = torch.from_numpy(np.stack([ex.next_obs for ex in batch]))\n",
    "        \n",
    "        # Prerequisite: collect outputs\n",
    "        pnext_a1, pnext_a2, pnext_a3 = q_net(next_obs)\n",
    "        p_a1, p_a2, p_a3 = q_net(obs)\n",
    "        \n",
    "        # bellman equation for each loss\n",
    "        loss1 = calculate_bellman_loss(pnext_a1, p_a1, reward, done, GAMMA, batch, action_size, action[:, 0])\n",
    "        loss2 = calculate_bellman_loss(pnext_a2, p_a2, reward, done, GAMMA, batch, action_size, action[:, 1])\n",
    "        loss3 = calculate_bellman_loss(pnext_a3, p_a3, reward, done, GAMMA, batch, action_size, action[:, 2])\n",
    "        loss = loss1 + loss2 + loss3\n",
    "        \n",
    "        # Perform the backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridWorld environment created.\n",
      "Training step  1 \treward  0.0\n",
      "\n",
      "Training step  2 \treward  0.0\n",
      "\n",
      "Training step  3 \treward  0.0\n",
      "\n",
      "Training step  4 \treward  0.0\n",
      "\n",
      "Training step  5 \treward  0.0\n",
      "\n",
      "Training step  6 \treward  0.0\n",
      "\n",
      "Training step  7 \treward  0.0\n",
      "\n",
      "Training step  8 \treward  0.0\n",
      "\n",
      "Training step  9 \treward  0.0\n",
      "\n",
      "Training step  10 \treward  0.0\n",
      "\n",
      "Training step  11 \treward  19.129937410354614\n",
      "\n",
      "Training step  12 \treward  37.534525498747826\n",
      "\n",
      "Training step  13 \treward  31.186563849449158\n",
      "\n",
      "Training step  14 \treward  31.229666997989018\n",
      "\n",
      "Training step  15 \treward  31.094921847184498\n",
      "\n",
      "Training step  16 \treward  -1.0\n",
      "\n",
      "Training step  17 \treward  39.805367867151894\n",
      "\n",
      "Training step  18 \treward  33.36532882849375\n",
      "\n",
      "Training step  19 \treward  31.578939000765484\n",
      "\n",
      "Training step  20 \treward  21.16919473807017\n",
      "\n",
      "Training step  21 \treward  32.689006884892784\n",
      "\n",
      "Training step  22 \treward  13.328981596231461\n",
      "\n",
      "Training step  23 \treward  37.36705195903778\n",
      "\n",
      "Training step  24 \treward  23.916837453842163\n",
      "\n",
      "Training step  25 \treward  59.19723051786423\n",
      "\n",
      "Training step  26 \treward  28.823106358448666\n",
      "\n",
      "Training step  27 \treward  0.0\n",
      "\n",
      "Training step  28 \treward  0.0\n",
      "\n",
      "Training step  29 \treward  0.0\n",
      "\n",
      "Training step  30 \treward  0.0\n",
      "\n",
      "Training step  31 \treward  0.0\n",
      "\n",
      "Training step  32 \treward  0.0\n",
      "\n",
      "Training step  33 \treward  0.0\n",
      "\n",
      "Training step  34 \treward  0.0\n",
      "\n",
      "Training step  35 \treward  0.0\n",
      "\n",
      "Training step  36 \treward  0.0\n",
      "\n",
      "Training step  37 \treward  0.0\n",
      "\n",
      "Training step  38 \treward  0.0\n",
      "\n",
      "Training step  39 \treward  0.0\n",
      "\n",
      "Training step  40 \treward  0.0\n",
      "\n",
      "Training step  41 \treward  0.0\n",
      "\n",
      "Training step  42 \treward  0.0\n",
      "\n",
      "Training step  43 \treward  0.0\n",
      "\n",
      "Training step  44 \treward  0.0\n",
      "\n",
      "Training step  45 \treward  0.0\n",
      "\n",
      "Training step  46 \treward  0.0\n",
      "\n",
      "Training step  47 \treward  0.0\n",
      "\n",
      "Training step  48 \treward  0.0\n",
      "\n",
      "Training step  49 \treward  0.0\n",
      "\n",
      "Training step  50 \treward  0.0\n",
      "\n",
      "Training step  51 \treward  0.0\n",
      "\n",
      "Training step  52 \treward  0.0\n",
      "\n",
      "Training step  53 \treward  0.0\n",
      "\n",
      "Training step  54 \treward  0.0\n",
      "\n",
      "Training step  55 \treward  0.0\n",
      "\n",
      "Training step  56 \treward  0.0\n",
      "\n",
      "Training step  57 \treward  0.0\n",
      "\n",
      "Training step  58 \treward  0.0\n",
      "\n",
      "Training step  59 \treward  0.0\n",
      "\n",
      "Training step  60 \treward  0.0\n",
      "\n",
      "Training step  61 \treward  0.0\n",
      "\n",
      "Training step  62 \treward  0.0\n",
      "\n",
      "Training step  63 \treward  0.0\n",
      "\n",
      "Training step  64 \treward  0.0\n",
      "\n",
      "Training step  65 \treward  0.0\n",
      "\n",
      "Training step  66 \treward  0.0\n",
      "\n",
      "Training step  67 \treward  0.0\n",
      "\n",
      "Training step  68 \treward  0.0\n",
      "\n",
      "Training step  69 \treward  0.0\n",
      "\n",
      "Training step  70 \treward  0.0\n",
      "\n",
      "Training step  71 \treward  0.0\n",
      "\n",
      "Training step  72 \treward  0.0\n",
      "\n",
      "Training step  73 \treward  0.0\n",
      "\n",
      "Training step  74 \treward  0.0\n",
      "\n",
      "Training step  75 \treward  0.0\n",
      "\n",
      "Training step  76 \treward  0.0\n",
      "\n",
      "Training step  77 \treward  0.0\n",
      "\n",
      "Training step  78 \treward  0.0\n",
      "\n",
      "Training step  79 \treward  0.0\n",
      "\n",
      "Training step  80 \treward  0.0\n",
      "\n",
      "Training step  81 \treward  0.0\n",
      "\n",
      "Training step  82 \treward  0.0\n",
      "\n",
      "Training step  83 \treward  0.0\n",
      "\n",
      "Training step  84 \treward  0.0\n",
      "\n",
      "Training step  85 \treward  0.0\n",
      "\n",
      "Training step  86 \treward  0.0\n",
      "\n",
      "Training step  87 \treward  0.0\n",
      "\n",
      "Training step  88 \treward  0.0\n",
      "\n",
      "Training step  89 \treward  0.0\n",
      "\n",
      "Training step  90 \treward  0.0\n",
      "\n",
      "Training step  91 \treward  0.0\n",
      "\n",
      "Training step  92 \treward  0.0\n",
      "\n",
      "Training step  93 \treward  0.0\n",
      "\n",
      "Training step  94 \treward  0.0\n",
      "\n",
      "Training step  95 \treward  0.0\n",
      "\n",
      "Training step  96 \treward  0.0\n",
      "\n",
      "Training step  97 \treward  0.0\n",
      "\n",
      "Training step  98 \treward  0.0\n",
      "\n",
      "Training step  99 \treward  0.0\n",
      "\n",
      "Training step  100 \treward  0.0\n",
      "\n",
      "Training step  101 \treward  0.0\n",
      "\n",
      "Training step  102 \treward  0.0\n",
      "\n",
      "Training step  103 \treward  0.0\n",
      "\n",
      "Training step  104 \treward  0.0\n",
      "\n",
      "Training step  105 \treward  0.0\n",
      "\n",
      "Training step  106 \treward  0.0\n",
      "\n",
      "Training step  107 \treward  0.0\n",
      "\n",
      "Training step  108 \treward  0.0\n",
      "\n",
      "Training step  109 \treward  0.0\n",
      "\n",
      "Training step  110 \treward  0.0\n",
      "\n",
      "Training step  111 \treward  0.0\n",
      "\n",
      "Training step  112 \treward  0.0\n",
      "\n",
      "Training step  113 \treward  0.0\n",
      "\n",
      "Training step  114 \treward  0.0\n",
      "\n",
      "Training step  115 \treward  0.0\n",
      "\n",
      "Training step  116 \treward  0.0\n",
      "\n",
      "Training step  117 \treward  0.0\n",
      "\n",
      "Training step  118 \treward  0.0\n",
      "\n",
      "Training step  119 \treward  0.0\n",
      "\n",
      "Training step  120 \treward  0.0\n",
      "\n",
      "Training step  121 \treward  0.0\n",
      "\n",
      "Training step  122 \treward  0.0\n",
      "\n",
      "Training step  123 \treward  0.0\n",
      "\n",
      "Training step  124 \treward  0.0\n",
      "\n",
      "Training step  125 \treward  0.0\n",
      "\n",
      "Training step  126 \treward  0.0\n",
      "\n",
      "Training step  127 \treward  0.0\n",
      "\n",
      "Training step  128 \treward  0.0\n",
      "\n",
      "Training step  129 \treward  0.0\n",
      "\n",
      "Training step  130 \treward  0.0\n",
      "\n",
      "Training step  131 \treward  0.0\n",
      "\n",
      "Training step  132 \treward  0.0\n",
      "\n",
      "Training step  133 \treward  0.0\n",
      "\n",
      "Training step  134 \treward  0.0\n",
      "\n",
      "Training step  135 \treward  0.0\n",
      "\n",
      "Training step  136 \treward  0.0\n",
      "\n",
      "Training step  137 \treward  0.0\n",
      "\n",
      "Training step  138 \treward  0.0\n",
      "\n",
      "Training step  139 \treward  0.0\n",
      "\n",
      "Training step  140 \treward  0.0\n",
      "\n",
      "Training step  141 \treward  0.0\n",
      "\n",
      "Training step  142 \treward  0.0\n",
      "\n",
      "Training step  143 \treward  0.0\n",
      "\n",
      "Training step  144 \treward  0.0\n",
      "\n",
      "Training step  145 \treward  0.0\n",
      "\n",
      "Training step  146 \treward  0.0\n",
      "\n",
      "Training step  147 \treward  0.0\n",
      "\n",
      "Training step  148 \treward  0.0\n",
      "\n",
      "Training step  149 \treward  0.0\n",
      "\n",
      "Training step  150 \treward  0.0\n",
      "\n",
      "Training step  151 \treward  0.0\n",
      "\n",
      "Training step  152 \treward  0.0\n",
      "\n",
      "Training step  153 \treward  0.0\n",
      "\n",
      "Training step  154 \treward  0.0\n",
      "\n",
      "Training step  155 \treward  0.0\n",
      "\n",
      "Training step  156 \treward  0.0\n",
      "\n",
      "Training step  157 \treward  0.0\n",
      "\n",
      "Training step  158 \treward  0.0\n",
      "\n",
      "Training step  159 \treward  0.0\n",
      "\n",
      "Training step  160 \treward  0.0\n",
      "\n",
      "Training step  161 \treward  0.0\n",
      "\n",
      "Training step  162 \treward  0.0\n",
      "\n",
      "Training step  163 \treward  0.0\n",
      "\n",
      "Training step  164 \treward  0.0\n",
      "\n",
      "Training step  165 \treward  0.0\n",
      "\n",
      "Training step  166 \treward  0.0\n",
      "\n",
      "Training step  167 \treward  0.0\n",
      "\n",
      "Training step  168 \treward  0.0\n",
      "\n",
      "Training step  169 \treward  0.0\n",
      "\n",
      "Training step  170 \treward  0.0\n",
      "\n",
      "Training step  171 \treward  0.0\n",
      "\n",
      "Training step  172 \treward  0.0\n",
      "\n",
      "Training step  173 \treward  0.0\n",
      "\n",
      "Training step  174 \treward  0.0\n",
      "\n",
      "Training step  175 \treward  0.0\n",
      "\n",
      "Training step  176 \treward  0.0\n",
      "\n",
      "Training step  177 \treward  0.0\n",
      "\n",
      "Training step  178 \treward  0.0\n",
      "\n",
      "Training step  179 \treward  0.0\n",
      "\n",
      "Training step  180 \treward  0.0\n",
      "\n",
      "Training step  181 \treward  0.0\n",
      "\n",
      "Training step  182 \treward  0.0\n",
      "\n",
      "Training step  183 \treward  0.0\n",
      "\n",
      "Training step  184 \treward  0.0\n",
      "\n",
      "Training step  185 \treward  0.0\n",
      "\n",
      "Training step  186 \treward  0.0\n",
      "\n",
      "Training step  187 \treward  0.0\n",
      "\n",
      "Training step  188 \treward  0.0\n",
      "\n",
      "Training step  189 \treward  0.0\n",
      "\n",
      "Training step  190 \treward  0.0\n",
      "\n",
      "Training step  191 \treward  0.0\n",
      "\n",
      "Training step  192 \treward  0.0\n",
      "\n",
      "Training step  193 \treward  0.0\n",
      "\n",
      "Training step  194 \treward  0.0\n",
      "\n",
      "Training step  195 \treward  0.0\n",
      "\n",
      "Training step  196 \treward  0.0\n",
      "\n",
      "Training step  197 \treward  0.0\n",
      "\n",
      "Training step  198 \treward  0.0\n",
      "\n",
      "Training step  199 \treward  0.0\n",
      "\n",
      "Training step  200 \treward  0.0\n",
      "\n",
      "Training step  201 \treward  0.0\n",
      "\n",
      "Training step  202 \treward  0.0\n",
      "\n",
      "Training step  203 \treward  0.0\n",
      "\n",
      "Training step  204 \treward  0.0\n",
      "\n",
      "Training step  205 \treward  0.0\n",
      "\n",
      "Training step  206 \treward  0.0\n",
      "\n",
      "Training step  207 \treward  0.0\n",
      "\n",
      "Training step  208 \treward  0.0\n",
      "\n",
      "Training step  209 \treward  0.0\n",
      "\n",
      "Training step  210 \treward  0.0\n",
      "\n",
      "Training step  211 \treward  0.0\n",
      "\n",
      "Training step  212 \treward  0.0\n",
      "\n",
      "Training step  213 \treward  0.0\n",
      "\n",
      "Training step  214 \treward  0.0\n",
      "\n",
      "Training step  215 \treward  0.0\n",
      "\n",
      "Training step  216 \treward  0.0\n",
      "\n",
      "Training step  217 \treward  0.0\n",
      "\n",
      "Training step  218 \treward  0.0\n",
      "\n",
      "Training step  219 \treward  0.0\n",
      "\n",
      "Training step  220 \treward  0.0\n",
      "\n",
      "Training step  221 \treward  0.0\n",
      "\n",
      "Training step  222 \treward  0.0\n",
      "\n",
      "Training step  223 \treward  0.0\n",
      "\n",
      "Training step  224 \treward  0.0\n",
      "\n",
      "Training step  225 \treward  0.0\n",
      "\n",
      "Training step  226 \treward  0.0\n",
      "\n",
      "Training step  227 \treward  0.0\n",
      "\n",
      "Training step  228 \treward  0.0\n",
      "\n",
      "Training step  229 \treward  0.0\n",
      "\n",
      "Training step  230 \treward  0.0\n",
      "\n",
      "Training step  231 \treward  0.0\n",
      "\n",
      "Training step  232 \treward  0.0\n",
      "\n",
      "Training step  233 \treward  0.0\n",
      "\n",
      "Training step  234 \treward  0.0\n",
      "\n",
      "Training step  235 \treward  0.0\n",
      "\n",
      "Training step  236 \treward  0.0\n",
      "\n",
      "Training step  237 \treward  0.0\n",
      "\n",
      "Training step  238 \treward  0.0\n",
      "\n",
      "Training step  239 \treward  0.0\n",
      "\n",
      "Training step  240 \treward  0.0\n",
      "\n",
      "Training step  241 \treward  0.0\n",
      "\n",
      "Training step  242 \treward  0.0\n",
      "\n",
      "Training step  243 \treward  0.0\n",
      "\n",
      "Training step  244 \treward  0.0\n",
      "\n",
      "Training step  245 \treward  0.0\n",
      "\n",
      "Training step  246 \treward  0.0\n",
      "\n",
      "Training step  247 \treward  0.0\n",
      "\n",
      "Training step  248 \treward  0.0\n",
      "\n",
      "Training step  249 \treward  0.0\n",
      "\n",
      "Training step  250 \treward  0.0\n",
      "\n",
      "Training step  251 \treward  0.0\n",
      "\n",
      "Training step  252 \treward  0.0\n",
      "\n",
      "Training step  253 \treward  0.0\n",
      "\n",
      "Training step  254 \treward  0.0\n",
      "\n",
      "Training step  255 \treward  0.0\n",
      "\n",
      "Training step  256 \treward  0.0\n",
      "\n",
      "Training step  257 \treward  0.0\n",
      "\n",
      "Training step  258 \treward  0.0\n",
      "\n",
      "Training step  259 \treward  0.0\n",
      "\n",
      "Training step  260 \treward  0.0\n",
      "\n",
      "Training step  261 \treward  0.0\n",
      "\n",
      "Training step  262 \treward  0.0\n",
      "\n",
      "Training step  263 \treward  0.0\n",
      "\n",
      "Training step  264 \treward  0.0\n",
      "\n",
      "Training step  265 \treward  0.0\n",
      "\n",
      "Training step  266 \treward  0.0\n",
      "\n",
      "Training step  267 \treward  0.0\n",
      "\n",
      "Training step  268 \treward  0.0\n",
      "\n",
      "Training step  269 \treward  0.0\n",
      "\n",
      "Training step  270 \treward  0.0\n",
      "\n",
      "Training step  271 \treward  0.0\n",
      "\n",
      "Training step  272 \treward  0.0\n",
      "\n",
      "Training step  273 \treward  0.0\n",
      "\n",
      "Training step  274 \treward  0.0\n",
      "\n",
      "Training step  275 \treward  0.0\n",
      "\n",
      "Training step  276 \treward  0.0\n",
      "\n",
      "Training step  277 \treward  0.0\n",
      "\n",
      "Training step  278 \treward  0.0\n",
      "\n",
      "Training step  279 \treward  0.0\n",
      "\n",
      "Training step  280 \treward  0.0\n",
      "\n",
      "Training step  281 \treward  0.0\n",
      "\n",
      "Training step  282 \treward  0.0\n",
      "\n",
      "Training step  283 \treward  0.0\n",
      "\n",
      "Training step  284 \treward  0.0\n",
      "\n",
      "Training step  285 \treward  0.0\n",
      "\n",
      "Training step  286 \treward  0.0\n",
      "\n",
      "Training step  287 \treward  0.0\n",
      "\n",
      "Training step  288 \treward  0.0\n",
      "\n",
      "Training step  289 \treward  0.0\n",
      "\n",
      "Training step  290 \treward  0.0\n",
      "\n",
      "Training step  291 \treward  0.0\n",
      "\n",
      "Training step  292 \treward  0.0\n",
      "\n",
      "Training step  293 \treward  0.0\n",
      "\n",
      "Training step  294 \treward  0.0\n",
      "\n",
      "Training step  295 \treward  0.0\n",
      "\n",
      "Training step  296 \treward  0.0\n",
      "\n",
      "Training step  297 \treward  0.0\n",
      "\n",
      "Training step  298 \treward  0.0\n",
      "\n",
      "Training step  299 \treward  0.0\n",
      "\n",
      "Training step  300 \treward  0.0\n",
      "\n",
      "Training step  301 \treward  0.0\n",
      "\n",
      "Training step  302 \treward  0.0\n",
      "\n",
      "Training step  303 \treward  0.0\n",
      "\n",
      "Training step  304 \treward  0.0\n",
      "\n",
      "Training step  305 \treward  0.0\n",
      "\n",
      "Training step  306 \treward  0.0\n",
      "\n",
      "Training step  307 \treward  0.0\n",
      "\n",
      "Training step  308 \treward  0.0\n",
      "\n",
      "Training step  309 \treward  0.0\n",
      "\n",
      "Training step  310 \treward  0.0\n",
      "\n",
      "Training step  311 \treward  0.0\n",
      "\n",
      "Training step  312 \treward  0.0\n",
      "\n",
      "Training step  313 \treward  0.0\n",
      "\n",
      "Training step  314 \treward  0.0\n",
      "\n",
      "Training step  315 \treward  0.0\n",
      "\n",
      "Training step  316 \treward  0.0\n",
      "\n",
      "Training step  317 \treward  0.0\n",
      "\n",
      "Training step  318 \treward  0.0\n",
      "\n",
      "Training step  319 \treward  0.0\n",
      "\n",
      "Training step  320 \treward  0.0\n",
      "\n",
      "Training step  321 \treward  0.0\n",
      "\n",
      "Training step  322 \treward  0.0\n",
      "\n",
      "Training step  323 \treward  0.0\n",
      "\n",
      "Training step  324 \treward  0.0\n",
      "\n",
      "Training step  325 \treward  0.0\n",
      "\n",
      "Training step  326 \treward  0.0\n",
      "\n",
      "Training step  327 \treward  0.0\n",
      "\n",
      "Training step  328 \treward  0.0\n",
      "\n",
      "Training step  329 \treward  0.0\n",
      "\n",
      "Training step  330 \treward  0.0\n",
      "\n",
      "Training step  331 \treward  0.0\n",
      "\n",
      "Training step  332 \treward  0.0\n",
      "\n",
      "Training step  333 \treward  0.0\n",
      "\n",
      "Training step  334 \treward  0.0\n",
      "\n",
      "Training step  335 \treward  0.0\n",
      "\n",
      "Training step  336 \treward  0.0\n",
      "\n",
      "Training step  337 \treward  0.0\n",
      "\n",
      "Training step  338 \treward  0.0\n",
      "\n",
      "Training step  339 \treward  0.0\n",
      "\n",
      "Training step  340 \treward  0.0\n",
      "\n",
      "Training step  341 \treward  0.0\n",
      "\n",
      "Training step  342 \treward  0.0\n",
      "\n",
      "Training step  343 \treward  0.0\n",
      "\n",
      "Training step  344 \treward  0.0\n",
      "\n",
      "Training step  345 \treward  0.0\n",
      "\n",
      "Training step  346 \treward  0.0\n",
      "\n",
      "Training step  347 \treward  0.0\n",
      "\n",
      "Training step  348 \treward  0.0\n",
      "\n",
      "Training step  349 \treward  0.0\n",
      "\n",
      "Training step  350 \treward  0.0\n",
      "\n",
      "Training step  351 \treward  0.0\n",
      "\n",
      "Training step  352 \treward  0.0\n",
      "\n",
      "Training step  353 \treward  0.0\n",
      "\n",
      "Training step  354 \treward  0.0\n",
      "\n",
      "Training step  355 \treward  0.0\n",
      "\n",
      "Training step  356 \treward  0.0\n",
      "\n",
      "Training step  357 \treward  0.0\n",
      "\n",
      "Training step  358 \treward  0.0\n",
      "\n",
      "Training step  359 \treward  0.0\n",
      "\n",
      "Training step  360 \treward  0.0\n",
      "\n",
      "Training step  361 \treward  0.0\n",
      "\n",
      "Training step  362 \treward  0.0\n",
      "\n",
      "Training step  363 \treward  0.0\n",
      "\n",
      "Training step  364 \treward  0.0\n",
      "\n",
      "Training step  365 \treward  0.0\n",
      "\n",
      "Training step  366 \treward  0.0\n",
      "\n",
      "Training step  367 \treward  0.0\n",
      "\n",
      "Training step  368 \treward  0.0\n",
      "\n",
      "Training step  369 \treward  0.0\n",
      "\n",
      "Training step  370 \treward  0.0\n",
      "\n",
      "Training step  371 \treward  0.0\n",
      "\n",
      "Training step  372 \treward  0.0\n",
      "\n",
      "Training step  373 \treward  0.0\n",
      "\n",
      "Training step  374 \treward  0.0\n",
      "\n",
      "Training step  375 \treward  0.0\n",
      "\n",
      "Training step  376 \treward  0.0\n",
      "\n",
      "Training step  377 \treward  0.0\n",
      "\n",
      "Training step  378 \treward  0.0\n",
      "\n",
      "Training step  379 \treward  0.0\n",
      "\n",
      "Training step  380 \treward  0.0\n",
      "\n",
      "Training step  381 \treward  0.0\n",
      "\n",
      "Training step  382 \treward  0.0\n",
      "\n",
      "Training step  383 \treward  0.0\n",
      "\n",
      "Training step  384 \treward  0.0\n",
      "\n",
      "Training step  385 \treward  0.0\n",
      "\n",
      "Training step  386 \treward  0.0\n",
      "\n",
      "Training step  387 \treward  0.0\n",
      "\n",
      "Training step  388 \treward  0.0\n",
      "\n",
      "Training step  389 \treward  0.0\n",
      "\n",
      "Training step  390 \treward  0.0\n",
      "\n",
      "Training step  391 \treward  0.0\n",
      "\n",
      "Training step  392 \treward  0.0\n",
      "\n",
      "Training step  393 \treward  0.0\n",
      "\n",
      "Training step  394 \treward  0.0\n",
      "\n",
      "Training step  395 \treward  0.0\n",
      "\n",
      "Training step  396 \treward  0.0\n",
      "\n",
      "Training step  397 \treward  0.0\n",
      "\n",
      "Training step  398 \treward  0.0\n",
      "\n",
      "Training step  399 \treward  0.0\n",
      "\n",
      "Training step  400 \treward  0.0\n",
      "\n",
      "Training step  401 \treward  0.0\n",
      "\n",
      "Training step  402 \treward  0.0\n",
      "\n",
      "Training step  403 \treward  0.0\n",
      "\n",
      "Training step  404 \treward  0.0\n",
      "\n",
      "Training step  405 \treward  0.0\n",
      "\n",
      "Training step  406 \treward  0.0\n",
      "\n",
      "Training step  407 \treward  0.0\n",
      "\n",
      "Training step  408 \treward  0.0\n",
      "\n",
      "Training step  409 \treward  0.0\n",
      "\n",
      "Training step  410 \treward  0.0\n",
      "\n",
      "Training step  411 \treward  0.0\n",
      "\n",
      "Training step  412 \treward  0.0\n",
      "\n",
      "Training step  413 \treward  0.0\n",
      "\n",
      "Training step  414 \treward  0.0\n",
      "\n",
      "Training step  415 \treward  0.0\n",
      "\n",
      "Training step  416 \treward  0.0\n",
      "\n",
      "Training step  417 \treward  0.0\n",
      "\n",
      "Training step  418 \treward  0.0\n",
      "\n",
      "Training step  419 \treward  0.0\n",
      "\n",
      "Training step  420 \treward  0.0\n",
      "\n",
      "Training step  421 \treward  0.0\n",
      "\n",
      "Training step  422 \treward  0.0\n",
      "\n",
      "Training step  423 \treward  0.0\n",
      "\n",
      "Training step  424 \treward  0.0\n",
      "\n",
      "Training step  425 \treward  0.0\n",
      "\n",
      "Training step  426 \treward  0.0\n",
      "\n",
      "Training step  427 \treward  0.0\n",
      "\n",
      "Training step  428 \treward  0.0\n",
      "\n",
      "Training step  429 \treward  0.0\n",
      "\n",
      "Training step  430 \treward  0.0\n",
      "\n",
      "Training step  431 \treward  0.0\n",
      "\n",
      "Training step  432 \treward  0.0\n",
      "\n",
      "Training step  433 \treward  0.0\n",
      "\n",
      "Training step  434 \treward  0.0\n",
      "\n",
      "Training step  435 \treward  0.0\n",
      "\n",
      "Training step  436 \treward  0.0\n",
      "\n",
      "Training step  437 \treward  0.0\n",
      "\n",
      "Training step  438 \treward  0.0\n",
      "\n",
      "Training step  439 \treward  0.0\n",
      "\n",
      "Training step  440 \treward  0.0\n",
      "\n",
      "Training step  441 \treward  0.0\n",
      "\n",
      "Training step  442 \treward  0.0\n",
      "\n",
      "Training step  443 \treward  0.0\n",
      "\n",
      "Training step  444 \treward  0.0\n",
      "\n",
      "Training step  445 \treward  0.0\n",
      "\n",
      "Training step  446 \treward  0.0\n",
      "\n",
      "Training step  447 \treward  0.0\n",
      "\n",
      "Training step  448 \treward  0.0\n",
      "\n",
      "Training step  449 \treward  0.0\n",
      "\n",
      "Training step  450 \treward  0.0\n",
      "\n",
      "Training step  451 \treward  0.0\n",
      "\n",
      "Training step  452 \treward  0.0\n",
      "\n",
      "Training step  453 \treward  0.0\n",
      "\n",
      "Training step  454 \treward  0.0\n",
      "\n",
      "Training step  455 \treward  0.0\n",
      "\n",
      "Training step  456 \treward  0.0\n",
      "\n",
      "Training step  457 \treward  0.0\n",
      "\n",
      "Training step  458 \treward  0.0\n",
      "\n",
      "Training step  459 \treward  0.0\n",
      "\n",
      "Training step  460 \treward  0.0\n",
      "\n",
      "Training step  461 \treward  0.0\n",
      "\n",
      "Training step  462 \treward  0.0\n",
      "\n",
      "Training step  463 \treward  0.0\n",
      "\n",
      "Training step  464 \treward  0.0\n",
      "\n",
      "Training step  465 \treward  0.0\n",
      "\n",
      "Training step  466 \treward  0.0\n",
      "\n",
      "Training step  467 \treward  0.0\n",
      "\n",
      "Training step  468 \treward  0.0\n",
      "\n",
      "Training step  469 \treward  0.0\n",
      "\n",
      "Training step  470 \treward  0.0\n",
      "\n",
      "Training step  471 \treward  0.0\n",
      "\n",
      "Training step  472 \treward  0.0\n",
      "\n",
      "Training step  473 \treward  0.0\n",
      "\n",
      "Training step  474 \treward  0.0\n",
      "\n",
      "Training step  475 \treward  0.0\n",
      "\n",
      "Training step  476 \treward  0.0\n",
      "\n",
      "Training step  477 \treward  0.0\n",
      "\n",
      "Training step  478 \treward  0.0\n",
      "\n",
      "Training step  479 \treward  0.0\n",
      "\n",
      "Training step  480 \treward  0.0\n",
      "\n",
      "Training step  481 \treward  0.0\n",
      "\n",
      "Training step  482 \treward  0.0\n",
      "\n",
      "Training step  483 \treward  0.0\n",
      "\n",
      "Training step  484 \treward  0.0\n",
      "\n",
      "Training step  485 \treward  0.0\n",
      "\n",
      "Training step  486 \treward  0.0\n",
      "\n",
      "Training step  487 \treward  0.0\n",
      "\n",
      "Training step  488 \treward  0.0\n",
      "\n",
      "Training step  489 \treward  0.0\n",
      "\n",
      "Training step  490 \treward  0.0\n",
      "\n",
      "Training step  491 \treward  0.0\n",
      "\n",
      "Training step  492 \treward  0.0\n",
      "\n",
      "Training step  493 \treward  0.0\n",
      "\n",
      "Training step  494 \treward  0.0\n",
      "\n",
      "Training step  495 \treward  0.0\n",
      "\n",
      "Training step  496 \treward  0.0\n",
      "\n",
      "Training step  497 \treward  0.0\n",
      "\n",
      "Training step  498 \treward  0.0\n",
      "\n",
      "Training step  499 \treward  0.0\n",
      "\n",
      "Training step  500 \treward  0.0\n",
      "\n",
      "Training step  501 \treward  0.0\n",
      "\n",
      "Training step  502 \treward  0.0\n",
      "\n",
      "Training step  503 \treward  0.0\n",
      "\n",
      "Training step  504 \treward  0.0\n",
      "\n",
      "Training step  505 \treward  0.0\n",
      "\n",
      "Training step  506 \treward  0.0\n",
      "\n",
      "Training step  507 \treward  0.0\n",
      "\n",
      "Training step  508 \treward  0.0\n",
      "\n",
      "Training step  509 \treward  0.0\n",
      "\n",
      "Training step  510 \treward  0.0\n",
      "\n",
      "Training step  511 \treward  0.0\n",
      "\n",
      "Training step  512 \treward  0.0\n",
      "\n",
      "Training step  513 \treward  0.0\n",
      "\n",
      "Training step  514 \treward  0.0\n",
      "\n",
      "Training step  515 \treward  0.0\n",
      "\n",
      "Training step  516 \treward  0.0\n",
      "\n",
      "Training step  517 \treward  0.0\n",
      "\n",
      "Training step  518 \treward  0.0\n",
      "\n",
      "Training step  519 \treward  0.0\n",
      "\n",
      "Training step  520 \treward  0.0\n",
      "\n",
      "Training step  521 \treward  0.0\n",
      "\n",
      "Training step  522 \treward  0.0\n",
      "\n",
      "Training step  523 \treward  0.0\n",
      "\n",
      "Training step  524 \treward  0.0\n",
      "\n",
      "Training step  525 \treward  0.0\n",
      "\n",
      "Training step  526 \treward  0.0\n",
      "\n",
      "Training step  527 \treward  0.0\n",
      "\n",
      "Training step  528 \treward  0.0\n",
      "\n",
      "Training step  529 \treward  0.0\n",
      "\n",
      "Training step  530 \treward  0.0\n",
      "\n",
      "Training step  531 \treward  0.0\n",
      "\n",
      "Training step  532 \treward  0.0\n",
      "\n",
      "Training step  533 \treward  0.0\n",
      "\n",
      "Training step  534 \treward  0.0\n",
      "\n",
      "Training step  535 \treward  0.0\n",
      "\n",
      "Training step  536 \treward  0.0\n",
      "\n",
      "Training step  537 \treward  0.0\n",
      "\n",
      "Training step  538 \treward  0.0\n",
      "\n",
      "Training step  539 \treward  0.0\n",
      "\n",
      "Training step  540 \treward  0.0\n",
      "\n",
      "Training step  541 \treward  0.0\n",
      "\n",
      "Training step  542 \treward  0.0\n",
      "\n",
      "Training step  543 \treward  0.0\n",
      "\n",
      "Training step  544 \treward  0.0\n",
      "\n",
      "Training step  545 \treward  0.0\n",
      "\n",
      "Training step  546 \treward  0.0\n",
      "\n",
      "Training step  547 \treward  0.0\n",
      "\n",
      "Training step  548 \treward  0.0\n",
      "\n",
      "Training step  549 \treward  0.0\n",
      "\n",
      "Training step  550 \treward  0.0\n",
      "\n",
      "Training step  551 \treward  0.0\n",
      "\n",
      "Training step  552 \treward  0.0\n",
      "\n",
      "Training step  553 \treward  0.0\n",
      "\n",
      "Training step  554 \treward  0.0\n",
      "\n",
      "Training step  555 \treward  0.0\n",
      "\n",
      "Training step  556 \treward  0.0\n",
      "\n",
      "Training step  557 \treward  0.0\n",
      "\n",
      "Training step  558 \treward  0.0\n",
      "\n",
      "Training step  559 \treward  0.0\n",
      "\n",
      "Training step  560 \treward  0.0\n",
      "\n",
      "Training step  561 \treward  0.0\n",
      "\n",
      "Training step  562 \treward  0.0\n",
      "\n",
      "Training step  563 \treward  0.0\n",
      "\n",
      "Training step  564 \treward  0.0\n",
      "\n",
      "Training step  565 \treward  0.0\n",
      "\n",
      "Training step  566 \treward  0.0\n",
      "\n",
      "Training step  567 \treward  0.0\n",
      "\n",
      "Training step  568 \treward  0.0\n",
      "\n",
      "Training step  569 \treward  0.0\n",
      "\n",
      "Training step  570 \treward  0.0\n",
      "\n",
      "Training step  571 \treward  0.0\n",
      "\n",
      "Training step  572 \treward  0.0\n",
      "\n",
      "Training step  573 \treward  0.0\n",
      "\n",
      "Training step  574 \treward  0.0\n",
      "\n",
      "Training step  575 \treward  0.0\n",
      "\n",
      "Training step  576 \treward  0.0\n",
      "\n",
      "Training step  577 \treward  0.0\n",
      "\n",
      "Training step  578 \treward  0.0\n",
      "\n",
      "Training step  579 \treward  0.0\n",
      "\n",
      "Training step  580 \treward  0.0\n",
      "\n",
      "Training step  581 \treward  0.0\n",
      "\n",
      "Training step  582 \treward  0.0\n",
      "\n",
      "Training step  583 \treward  0.0\n",
      "\n",
      "Training step  584 \treward  0.0\n",
      "\n",
      "Training step  585 \treward  0.0\n",
      "\n",
      "Training step  586 \treward  0.0\n",
      "\n",
      "Training step  587 \treward  0.0\n",
      "\n",
      "Training step  588 \treward  0.0\n",
      "\n",
      "Training step  589 \treward  0.0\n",
      "\n",
      "Training step  590 \treward  0.0\n",
      "\n",
      "Training step  591 \treward  0.0\n",
      "\n",
      "Training step  592 \treward  0.0\n",
      "\n",
      "Training step  593 \treward  0.0\n",
      "\n",
      "Training step  594 \treward  0.0\n",
      "\n",
      "Training step  595 \treward  0.0\n",
      "\n",
      "Training step  596 \treward  0.0\n",
      "\n",
      "Training step  597 \treward  0.0\n",
      "\n",
      "Training step  598 \treward  0.0\n",
      "\n",
      "Training step  599 \treward  0.0\n",
      "\n",
      "Training step  600 \treward  0.0\n",
      "\n",
      "Training step  601 \treward  0.0\n",
      "\n",
      "Training step  602 \treward  0.0\n",
      "\n",
      "Training step  603 \treward  0.0\n",
      "\n",
      "Training step  604 \treward  0.0\n",
      "\n",
      "Training step  605 \treward  0.0\n",
      "\n",
      "Training step  606 \treward  0.0\n",
      "\n",
      "Training step  607 \treward  0.0\n",
      "\n",
      "Training step  608 \treward  0.0\n",
      "\n",
      "Training step  609 \treward  0.0\n",
      "\n",
      "Training step  610 \treward  0.0\n",
      "\n",
      "Training step  611 \treward  0.0\n",
      "\n",
      "Training step  612 \treward  0.0\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-31cd66a3d67d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0mexperiences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0mTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_q_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m   \u001b[0mcumulative_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training step \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\treward \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-dee542e9134d>\u001b[0m in \u001b[0;36mgenerate_trajectories\u001b[0;34m(env, q_net, buffer_size, epsilon)\u001b[0m\n\u001b[1;32m    106\u001b[0m       \u001b[0;31m# Generate an action for all the Agents that requested a decision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0;31m# Compute the values for each action given the observation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0mact1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecision_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecision_steps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-f62e8f4c85f9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, visual_obs)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "# This code is used to close an env that might not have been closed before\n",
    "try:\n",
    "  env.close()\n",
    "except:\n",
    "  pass\n",
    "# -----------------\n",
    "\n",
    "from mlagents_envs.registry import default_registry\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create the GridWorld Environment from the registry\n",
    "env = UE(file_name='run32_training', seed=1, worker_id=1, side_channels=[])\n",
    "# env = default_registry[\"GridWorld\"].make()\n",
    "print(\"GridWorld environment created.\")\n",
    "\n",
    "# moved Qnet outside to reuse it\n",
    "\n",
    "# Create a new Q-Network. \n",
    "qnet = VisualQNetwork((44, 1), 126, 3)\n",
    "\n",
    "experiences: Buffer = []\n",
    "optim = torch.optim.Adam(qnet.parameters(), lr= 0.001)\n",
    "\n",
    "cumulative_rewards: List[float] = []\n",
    "\n",
    "# The number of training steps that will be performed\n",
    "NUM_TRAINING_STEPS = 10000000 #70\n",
    "# The number of experiences to collect per training step\n",
    "NUM_NEW_EXP = 1000\n",
    "# The maximum size of the Buffer\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "for n in range(NUM_TRAINING_STEPS):\n",
    "  new_exp,_ = Trainer.generate_trajectories(env, qnet, NUM_NEW_EXP, epsilon=0.1)\n",
    "  random.shuffle(experiences)\n",
    "  if len(experiences) > BUFFER_SIZE:\n",
    "    experiences = experiences[:BUFFER_SIZE]\n",
    "  experiences.extend(new_exp)\n",
    "  Trainer.update_q_net(qnet, optim, experiences, 3)\n",
    "  _, rewards = Trainer.generate_trajectories(env, qnet, 100, epsilon=0)\n",
    "  cumulative_rewards.append(rewards)\n",
    "  print(\"Training step \", n+1, \"\\treward \", rewards)\n",
    "  print()\n",
    "\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Show the training graph\n",
    "plt.plot(range(NUM_TRAINING_STEPS), cumulative_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f548f2ca040>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(612), cumulative_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed environment\n"
     ]
    }
   ],
   "source": [
    "env.close()\n",
    "print(\"Closed environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.4.2-cp37-cp37m-macosx_10_9_x86_64.whl (7.2 MB)\n",
      "\u001b[K     || 7.2 MB 743 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp37-cp37m-macosx_10_9_x86_64.whl (61 kB)\n",
      "\u001b[K     || 61 kB 441 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/ribr/.virtualenvs/ultron/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.16 in /Users/ribr/.virtualenvs/ultron/lib/python3.7/site-packages (from matplotlib) (1.20.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/ribr/.virtualenvs/ultron/lib/python3.7/site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/ribr/.virtualenvs/ultron/lib/python3.7/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six in /Users/ribr/.virtualenvs/ultron/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n",
      "Installing collected packages: kiwisolver, cycler, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.3.1 matplotlib-3.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
