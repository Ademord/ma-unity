{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml-agents already installed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import mlagents\n",
    "    from mlagents_envs.environment import UnityEnvironment as UE\n",
    "    from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "    print(\"ml-agents already installed\")\n",
    "except ImportError:\n",
    "    !pip install mlagents==0.26.0\n",
    "    print(\"Installed ml-agents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "from math import floor\n",
    "\n",
    "\n",
    "class VisualQNetwork(torch.nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    input_shape: Tuple[int], \n",
    "    encoding_size: int, \n",
    "    output_size: int\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Creates a neural network that takes as input a batch of images (3\n",
    "    dimensional tensors) and outputs a batch of outputs (1 dimensional\n",
    "    tensors)\n",
    "    \"\"\"\n",
    "    super(VisualQNetwork, self).__init__()\n",
    "#     height = input_shape[0]\n",
    "#     width = input_shape[1]\n",
    "#     initial_channels = input_shape[2]\n",
    "#     conv_1_hw = self.conv_output_shape((height, width), 8, 4)\n",
    "#     conv_2_hw = self.conv_output_shape(conv_1_hw, 4, 2)\n",
    "#     self.final_flat = conv_2_hw[0] * conv_2_hw[1] * 32\n",
    "    \n",
    "    \n",
    "#     self.conv1 = torch.nn.Conv2d(initial_channels, 16, [8, 8], [4, 4])\n",
    "#     self.conv2 = torch.nn.Conv2d(16, 32, [4, 4], [2, 2])\n",
    "#     self.dense1 = torch.nn.Linear(self.final_flat, encoding_size)\n",
    "\n",
    "    \n",
    "    self.dense1 = torch.nn.Linear(input_shape[0], encoding_size)\n",
    "    \n",
    "    self.dense2_x1 = torch.nn.Linear(encoding_size, output_size)\n",
    "    self.dense2_x2 = torch.nn.Linear(encoding_size, output_size)\n",
    "    self.dense2_x3 = torch.nn.Linear(encoding_size, output_size)\n",
    "\n",
    "    \n",
    "    \n",
    "  def forward(self, visual_obs: torch.tensor):\n",
    "#     print(\"torch input size:\", visual_obs.size())\n",
    "#     visual_obs = visual_obs.permute(0, 3, 1, 2)\n",
    "#     conv_1 = torch.relu(self.conv1(visual_obs))\n",
    "#     conv_2 = torch.relu(self.conv2(conv_1))\n",
    "#     hidden = self.dense1(conv_2.reshape([-1, self.final_flat]))\n",
    "\n",
    "    hidden = self.dense1(visual_obs)\n",
    "    hidden = torch.relu(hidden)\n",
    "    \n",
    "    x1 = self.dense2_x1(hidden)\n",
    "    x2 = self.dense2_x2(hidden)\n",
    "    x3 = self.dense2_x3(hidden)\n",
    "\n",
    "    return x1, x2, x3\n",
    "\n",
    "  @staticmethod\n",
    "  def conv_output_shape(\n",
    "    h_w: Tuple[int, int],\n",
    "    kernel_size: int = 1,\n",
    "    stride: int = 1,\n",
    "    pad: int = 0,\n",
    "    dilation: int = 1,\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Computes the height and width of the output of a convolution layer.\n",
    "    \"\"\"\n",
    "    h = floor(\n",
    "      ((h_w[0] + (2 * pad) - (dilation * (kernel_size - 1)) - 1) / stride) + 1\n",
    "    )\n",
    "    w = floor(\n",
    "      ((h_w[1] + (2 * pad) - (dilation * (kernel_size - 1)) - 1) / stride) + 1\n",
    "    )\n",
    "    return h, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import NamedTuple, List\n",
    "\n",
    "\n",
    "class Experience(NamedTuple):\n",
    "  \"\"\"\n",
    "  An experience contains the data of one Agent transition.\n",
    "  - Observation\n",
    "  - Action\n",
    "  - Reward\n",
    "  - Done flag\n",
    "  - Next Observation\n",
    "  \"\"\"\n",
    "\n",
    "  obs: np.ndarray\n",
    "  action: np.ndarray\n",
    "  reward: float\n",
    "  done: bool\n",
    "  next_obs: np.ndarray\n",
    "\n",
    "# A Trajectory is an ordered sequence of Experiences\n",
    "Trajectory = List[Experience]\n",
    "\n",
    "# A Buffer is an unordered list of Experiences from multiple Trajectories\n",
    "Buffer = List[Experience]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import ActionTuple, BaseEnv\n",
    "from typing import Dict\n",
    "import random\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "  @staticmethod\n",
    "  def generate_trajectories(\n",
    "    env: BaseEnv, q_net: VisualQNetwork, buffer_size: int, epsilon: float\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Given a Unity Environment and a Q-Network, this method will generate a\n",
    "    buffer of Experiences obtained by running the Environment with the Policy\n",
    "    derived from the Q-Network.\n",
    "    :param BaseEnv: The UnityEnvironment used.\n",
    "    :param q_net: The Q-Network used to collect the data.\n",
    "    :param buffer_size: The minimum size of the buffer this method will return.\n",
    "    :param epsilon: Will add a random normal variable with standard deviation.\n",
    "    epsilon to the value heads of the Q-Network to encourage exploration.\n",
    "    :returns: a Tuple containing the created buffer and the average cumulative\n",
    "    the Agents obtained.\n",
    "    \"\"\"\n",
    "    # Create an empty Buffer\n",
    "    buffer: Buffer = []\n",
    "\n",
    "    # Reset the environment\n",
    "    env.reset()\n",
    "    # Read and store the Behavior Name of the Environment\n",
    "    behavior_name = list(env.behavior_specs)[0]\n",
    "    # Read and store the Behavior Specs of the Environment\n",
    "    spec = env.behavior_specs[behavior_name]\n",
    "\n",
    "    # Create a Mapping from AgentId to Trajectories. This will help us create\n",
    "    # trajectories for each Agents\n",
    "    dict_trajectories_from_agent: Dict[int, Trajectory] = {}\n",
    "    # Create a Mapping from AgentId to the last observation of the Agent\n",
    "    dict_last_obs_from_agent: Dict[int, np.ndarray] = {}\n",
    "    # Create a Mapping from AgentId to the last observation of the Agent\n",
    "    dict_last_action_from_agent: Dict[int, np.ndarray] = {}\n",
    "    # Create a Mapping from AgentId to cumulative reward (Only for reporting)\n",
    "    dict_cumulative_reward_from_agent: Dict[int, float] = {}\n",
    "    # Create a list to store the cumulative rewards obtained so far\n",
    "    cumulative_rewards: List[float] = []\n",
    "    \n",
    "    \n",
    "    entered_terminal = False\n",
    "    while len(buffer) < buffer_size:  # While not enough data in the buffer\n",
    "      # Get the Decision Steps and Terminal Steps of the Agents\n",
    "      decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    \n",
    "        # For all Agents with a Terminal Step:\n",
    "      for agent_id_terminated in terminal_steps:\n",
    "#         print(\"entered agent with terminal step\")\n",
    "#         print(agent_id_terminated)\n",
    "\n",
    "        # Create its last experience (is last because the Agent terminated)\n",
    "        last_experience = Experience(\n",
    "          obs=dict_last_obs_from_agent[agent_id_terminated].copy(),\n",
    "          reward=terminal_steps[agent_id_terminated].reward,\n",
    "          done=not terminal_steps[agent_id_terminated].interrupted,\n",
    "          action=dict_last_action_from_agent[agent_id_terminated].copy(),\n",
    "          next_obs=terminal_steps[agent_id_terminated].obs[0],\n",
    "        )\n",
    "        # Clear its last observation and action (Since the trajectory is over)\n",
    "        dict_last_obs_from_agent.pop(agent_id_terminated)\n",
    "        dict_last_action_from_agent.pop(agent_id_terminated)\n",
    "        # Report the cumulative reward\n",
    "        cumulative_reward = (\n",
    "          dict_cumulative_reward_from_agent.pop(agent_id_terminated)\n",
    "          + terminal_steps[agent_id_terminated].reward\n",
    "        )\n",
    "        print(\"cumulative reward: \", cumulative_reward)\n",
    "        cumulative_rewards.append(cumulative_reward)\n",
    "        # Add the Trajectory and the last experience to the buffer\n",
    "        buffer.extend(dict_trajectories_from_agent.pop(agent_id_terminated))\n",
    "        buffer.append(last_experience)\n",
    "        entered_terminal = True\n",
    "\n",
    "      # For all Agents with a Decision Step:\n",
    "      for agent_id_decisions in decision_steps:\n",
    "        # If the Agent does not have a Trajectory, create an empty one\n",
    "        if agent_id_decisions not in dict_trajectories_from_agent:\n",
    "          dict_trajectories_from_agent[agent_id_decisions] = []\n",
    "          dict_cumulative_reward_from_agent[agent_id_decisions] = 0\n",
    "\n",
    "        # If the Agent requesting a decision has a \"last observation\"\n",
    "        if agent_id_decisions in dict_last_obs_from_agent:\n",
    "          # Create an Experience from the last observation and the Decision Step\n",
    "          exp = Experience(\n",
    "            obs=dict_last_obs_from_agent[agent_id_decisions].copy(),\n",
    "            reward=decision_steps[agent_id_decisions].reward,\n",
    "            done=False,\n",
    "            action=dict_last_action_from_agent[agent_id_decisions].copy(),\n",
    "            next_obs=decision_steps[agent_id_decisions].obs[0],\n",
    "          )\n",
    "          # Update the Trajectory of the Agent and its cumulative reward\n",
    "          dict_trajectories_from_agent[agent_id_decisions].append(exp)\n",
    "          dict_cumulative_reward_from_agent[agent_id_decisions] += (\n",
    "            decision_steps[agent_id_decisions].reward\n",
    "          )\n",
    "        # Store the observation as the new \"last observation\"\n",
    "        dict_last_obs_from_agent[agent_id_decisions] = (\n",
    "          decision_steps[agent_id_decisions].obs[0]\n",
    "        )\n",
    "\n",
    "      # Generate an action for all the Agents that requested a decision\n",
    "      # Compute the values for each action given the observation\n",
    "     \n",
    "        \n",
    "      act1, act2, act3 = q_net(torch.from_numpy(decision_steps.obs[0]))\n",
    "      act1 = act1.detach().numpy()\n",
    "      act2 = act2.detach().numpy()\n",
    "      act3 = act3.detach().numpy()\n",
    "\n",
    "      try:\n",
    "        actions_values = np.array([act1, act2, act3]).reshape(3,3)\n",
    "      except:\n",
    "        actions_values = np.zeros((3,3))\n",
    "        print(\"error: network received an input of size 0 and i caught the error :/\")\n",
    "\n",
    "        #      0-8nt(type(actions_values))\n",
    "      # Add some noise with epsilon to the values\n",
    "      actions_values += epsilon * np.random.randn(actions_values.shape[0], actions_values.shape[1]).astype(np.float32)\n",
    "      # Pick the best action using argmax\n",
    "      actions = np.argmax(actions_values, axis=1)\n",
    "      actions.resize((len(decision_steps), 3))\n",
    "    \n",
    "      # Store the action that was picked, it will be put in the trajectory later\n",
    "      for agent_index, agent_id in enumerate(decision_steps.agent_id):\n",
    "        dict_last_action_from_agent[agent_id] = actions[agent_index]\n",
    "\n",
    "      # Set the actions in the environment\n",
    "      # Unity Environments expect ActionTuple instances.\n",
    "      action_tuple = ActionTuple()\n",
    "      action_tuple.add_discrete(actions)\n",
    "#       print(\"action received from QNetwork: \", action_tuple.discrete)\n",
    "      env.set_actions(behavior_name, action_tuple)\n",
    "      # Perform a step in the simulation\n",
    "      env.step()\n",
    "    return buffer, np.mean(cumulative_rewards)\n",
    "\n",
    "  @staticmethod\n",
    "  def update_q_net(\n",
    "    q_net: VisualQNetwork, \n",
    "    optimizer: torch.optim, \n",
    "    buffer: Buffer, \n",
    "    action_size: int\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Performs an update of the Q-Network using the provided optimizer and buffer\n",
    "    \"\"\"\n",
    "    def calculate_bellman_loss(next_pred_action, pred_action, reward, done, GAMMA, batch, action_size, action):\n",
    "        # Use the Bellman equation to update the Q-Network\n",
    "        target = (\n",
    "          reward\n",
    "          + (1.0 - done)\n",
    "          * GAMMA\n",
    "          * torch.max(next_pred_action.detach(), dim=1, keepdim=True).values\n",
    "        )\n",
    "        mask = torch.zeros((len(batch), action_size))\n",
    "        mask.scatter_(1, action, 1)\n",
    "        prediction = torch.sum(pred_action * mask, dim=1, keepdim=True)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        loss = criterion(prediction, target)\n",
    "        return loss\n",
    "\n",
    "    BATCH_SIZE = 1000\n",
    "    NUM_EPOCH = 3\n",
    "    GAMMA = 0.9\n",
    "    batch_size = min(len(buffer), BATCH_SIZE)\n",
    "    random.shuffle(buffer)\n",
    "    # Split the buffer into batches\n",
    "    batches = [\n",
    "      buffer[batch_size * start : batch_size * (start + 1)]\n",
    "      for start in range(int(len(buffer) / batch_size))\n",
    "    ]\n",
    "    for _ in range(NUM_EPOCH):\n",
    "      for batch in batches:\n",
    "        # Create the Tensors that will be fed in the network\n",
    "        obs = torch.from_numpy(np.stack([ex.obs for ex in batch]))\n",
    "        reward = torch.from_numpy(\n",
    "          np.array([ex.reward for ex in batch], dtype=np.float32).reshape(-1, 1)\n",
    "        )\n",
    "        done = torch.from_numpy(\n",
    "          np.array([ex.done for ex in batch], dtype=np.float32).reshape(-1, 1)\n",
    "        )\n",
    "        action = torch.from_numpy(np.stack([ex.action for ex in batch]))\n",
    "        next_obs = torch.from_numpy(np.stack([ex.next_obs for ex in batch]))\n",
    "        \n",
    "        # Prerequisite: collect outputs\n",
    "        pnext_a1, pnext_a2, pnext_a3 = q_net(next_obs)\n",
    "        p_a1, p_a2, p_a3 = q_net(obs)\n",
    "        \n",
    "        # bellman equation for each loss\n",
    "        loss1 = calculate_bellman_loss(pnext_a1, p_a1, reward, done, GAMMA, batch, action_size, action)\n",
    "        loss2 = calculate_bellman_loss(pnext_a2, p_a2, reward, done, GAMMA, batch, action_size, action)\n",
    "        loss3 = calculate_bellman_loss(pnext_a3, p_a3, reward, done, GAMMA, batch, action_size, action)\n",
    "        loss = loss1 + loss2 + loss3\n",
    "        \n",
    "        # Perform the backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridWorld environment created.\n",
      "Starting training\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  680.2788114398718\n",
      "error: network received an input of size 0 and i caught the error :/\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  50.91722024977207\n",
      "error: network received an input of size 0 and i caught the error :/\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  107.1366455256939\n",
      "error: network received an input of size 0 and i caught the error :/\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  3.169999733567238\n",
      "error: network received an input of size 0 and i caught the error :/\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  3.449999749660492\n",
      "Training step  1 \treward  3.309999741613865\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  158.72953082621098\n",
      "error: network received an input of size 0 and i caught the error :/\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  37.043522983789444\n",
      "error: network received an input of size 0 and i caught the error :/\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  32.43999803066254\n",
      "error: network received an input of size 0 and i caught the error :/\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  2 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  31.369998052716255\n",
      "error: network received an input of size 0 and i caught the error :/\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  36.77999772131443\n",
      "error: network received an input of size 0 and i caught the error :/\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  3 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  30.679998084902763\n",
      "error: network received an input of size 0 and i caught the error :/\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  177.34977035224438\n",
      "error: network received an input of size 0 and i caught the error :/\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  230.18847300112247\n",
      "error: network received an input of size 0 and i caught the error :/\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  4 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  505.2008621543646\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  5 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  6 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  7 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  8 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  9 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  10 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  11 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  12 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  13 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  14 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.899997025728226\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  15 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.94999702274799\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  16 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  17 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.899997025728226\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  18 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  275.93664894998074\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  19 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  20 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  21 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  22 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.94999702274799\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  23 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  24 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.84999702870846\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  25 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.499997049570084\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.84999702870846\n",
      "Training step  26 \treward  49.84999702870846\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  37.599997758865356\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.2499999850988388\n",
      "Training step  27 \treward  0.2499999850988388\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  877.5955633223057\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  28 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "Training step  29 \treward  49.99999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  49.99999701976776\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.9499999433755875\n",
      "Training step  30 \treward  0.9499999433755875\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  2.1499998718500137\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.14999999105930328\n",
      "Training step  31 \treward  0.14999999105930328\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.14999999105930328\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.9499999433755875\n",
      "Training step  32 \treward  0.9499999433755875\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.0\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  1.049999937415123\n",
      "Training step  33 \treward  1.049999937415123\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  3.999999761581421\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  1.3999999165534973\n",
      "Training step  34 \treward  1.3999999165534973\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.14999999105930328\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.0\n",
      "Training step  35 \treward  0.0\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  2.5999998450279236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.7999999523162842\n",
      "Training step  36 \treward  0.7999999523162842\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  1.8499998897314072\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.09999999403953552\n",
      "Training step  37 \treward  0.09999999403953552\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.4999999701976776\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.0\n",
      "Training step  38 \treward  0.0\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.5499999672174454\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  1.4999999105930328\n",
      "Training step  39 \treward  1.4999999105930328\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.5499999672174454\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.0\n",
      "Training step  40 \treward  0.0\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  1.2999999225139618\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.4999999701976776\n",
      "Training step  41 \treward  0.4999999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.0\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.5999999642372131\n",
      "Training step  42 \treward  0.5999999642372131\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.7499999552965164\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.0\n",
      "Training step  43 \treward  0.0\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  1.1499999314546585\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  1.0999999344348907\n",
      "Training step  44 \treward  1.0999999344348907\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  4.049999758601189\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  1.1999999284744263\n",
      "Training step  45 \treward  1.1999999284744263\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  1377.615269228816\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.09999999403953552\n",
      "Training step  46 \treward  0.09999999403953552\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.7999999523162842\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.6499999612569809\n",
      "Training step  47 \treward  0.6499999612569809\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  1.0999999344348907\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.4999999701976776\n",
      "Training step  48 \treward  0.4999999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  1.4999999105930328\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.9499999433755875\n",
      "Training step  49 \treward  0.9499999433755875\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.9999999403953552\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.04999999701976776\n",
      "Training step  50 \treward  0.04999999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  1.1999999284744263\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.2499999850988388\n",
      "Training step  51 \treward  0.2499999850988388\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.3999999761581421\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.04999999701976776\n",
      "Training step  52 \treward  0.04999999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.19999998807907104\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.7999999523162842\n",
      "Training step  53 \treward  0.7999999523162842\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.09999999403953552\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.4999999701976776\n",
      "Training step  54 \treward  0.4999999701976776\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.0\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.7999999523162842\n",
      "Training step  55 \treward  0.7999999523162842\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.5499999672174454\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.34999997913837433\n",
      "Training step  56 \treward  0.34999997913837433\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.2499999850988388\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  1.2999999225139618\n",
      "Training step  57 \treward  1.2999999225139618\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  3.0499998182058334\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  1.3999999165534973\n",
      "Training step  58 \treward  1.3999999165534973\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  4.199999749660492\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.7499999552965164\n",
      "Training step  59 \treward  0.7499999552965164\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  10.199999392032623\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.9999999403953552\n",
      "Training step  60 \treward  0.9999999403953552\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.3999999761581421\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.09999999403953552\n",
      "Training step  61 \treward  0.09999999403953552\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  10.399999380111694\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.19999998807907104\n",
      "Training step  62 \treward  0.19999998807907104\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  1.049999937415123\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.7999999523162842\n",
      "Training step  63 \treward  0.7999999523162842\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  4.049999758601189\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.0\n",
      "Training step  64 \treward  0.0\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  1.1999999284744263\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.5499999672174454\n",
      "Training step  65 \treward  0.5499999672174454\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  11.799999296665192\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.44999997317790985\n",
      "Training step  66 \treward  0.44999997317790985\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  1.6499999016523361\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.09999999403953552\n",
      "Training step  67 \treward  0.09999999403953552\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  7.099999576807022\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  1.4999999105930328\n",
      "Training step  68 \treward  1.4999999105930328\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  3.6999997794628143\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.0\n",
      "Training step  69 \treward  0.0\n",
      "\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  4.949999704957008\n",
      "entered agent with terminal step\n",
      "0\n",
      "cumulative reward:  0.7499999552965164\n",
      "Training step  70 \treward  0.7499999552965164\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f20d03210a0>]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAl1UlEQVR4nO3deZxcZbkn8N9TW7ZuknS6aUKSzkI2gkDAJmwBAQHjhgyjCCoTEY0zggPKjIp3Ue+9ozjjFXG7yhUk48IuEnKRLaIgSqCzkZ0sZOnQne7snU66aznP/eOcU11ddc6p01v6vMnv+/nk07V2vemq+tVbz3kXUVUQEZF5YoPdACIi6h0GOBGRoRjgRESGYoATERmKAU5EZKjEsXyw6upqnTRp0rF8SCIi4y1btmyPqtYUX35MA3zSpEloaGg4lg9JRGQ8EdnudTlLKEREhmKAExEZigFORGQoBjgRkaEY4EREhgo1CkVEtgFoA5ADkFXVehGpAvAIgEkAtgG4XlX3D0wziYioWE964Jer6mxVrXfOfw3AElWdBmCJc56IiI6RvowD/wiAy5zTCwH8CcBX+9ieUI6ks3jwr9vQkc4di4ejHhg5PIWJVcNRN2Y46qqGY2gyHnj7Tbvb8PSqd7pdNjQVx6cvmoThqWM6TYHIOGHfIQrgeRFRAD9X1fsA1Kpqk3N9M4BarzuKyAIACwCgrq6uj821/W3LXvzfZzc6v79ffiX1A6+l5WeeUomnvzgXybj3l71fvPI2HmnYmX8e3d8x/eRKXDnL8yVFRI6wAT5XVXeJyMkAXhCRDYVXqqo64V7CCfv7AKC+vr5fdo/ozFoAgOfuuBQzTqnsj19J/UBVsf9IBtv3tmPHviN4ZnUTnlu7GwePZlBdMcTzPp3ZHOqqhuPlr1wOANjY3Ib3/eDl/HNMRP5CBbiq7nJ+tojIkwDmANgtImNVtUlExgJoGcB2dpPJ2W/uZJzd7ygREVSNSKFqRArn1I3GkXQOz63djWzO/3M7k9Nuz6N72n2Oichf2YOYIjJCRCrd0wCuBrAGwCIA852bzQfw1EA1spjbO/P7Wk7R4D4/QWGcyVndnkf3dJoBTlRWmB54LYAnxS5SJgD8VlWfFZE3ADwqIrcA2A7g+oFrZnduIKQSDPAoc3vTQWGcyVndnkf3dFCvnYhsZQNcVbcCONvj8r0A3jsQjSonwx64EcL1wBWJWGEJpfx9iMhmZAJmnN4Za+DR5oZxUG86XVRCSbAGThSakQGezrEHboIwJZRscQmFNXCi0IxMwHwNnAEeae7zkwkYEuhbQsmyBk5UjpEJmMlZSMQEsRhLKFGWyNezg4YRdi+hxGOCmLCEQhSGoQGuLJ8YID+m2/IP43TOQrJoNFEyHgu8DxHZjEzBdNbiAUwDJEOUULI5LSmFpeIxllCIQjAzwIsOfFE0uc9RuRJKoqgUlogLSyhEIRiZgpmsxRKKAdxgLjsT06uEwgAnKsvIFCw+8EXRFGZSTjprlZRQkvEYhxEShWBkChYvgETRFKaEkrVKn8tUIhZ4HyKyGRngxbP3KJrCLmaVKOmBC7LsgROVZWQKFi+ARNFUblq8qnoOCWUNnCgcI1Mwkyutm1L0pMpM5Mla6tyueBRKDGmWUIjKMjIFM1lO5DFBuRJKxmdNm1RcAseOE5HNyBT0mr1H0VNuWrw7Wae0Bs4SClEYRqagPfSMo1BMEDQkMJ1flKz7c2lPpWcJhagcIwOc48DNkYzHfNcDz1reJZRkPMYSClEIRqYgA9wcyYBp8W4JpTTAOZWeKAwjU5CrEZojqJ7tllASXiUUBjhRWUamoL2YFWvgJkjGY0j7rCzotzGHHeCsgROVY2SAcxy4OYLKIdmcdwkllRCuhUIUgpEpyNUIzZGMx/IHK4vl9zb1WI2QU+mJyjMyBTM55ThwQ4QpoSRjXjVwllCIyjEuBVWVi1kZJJnwPyCZ8emBJ+IsoRCFYVwKuj0zTuQxQzLWixq4MwpFlb1woiAGBrj35A+KpqCJPPkauMcwQlUgx9mYRIGMS0EGuFmSCf+p9H7PZdciWAxwoiDGpaDfyAWKplTQTEzfAHfWEfcZvUJENuNS0O2VDWEP3AiJWNBBTLcGXrqlGgCuh0JUhnEp6L6pk5yJaYRkwr8G7tcDT8RYQiEKw7wAZw3cKMmAIYH5D2O/EgqHEhIFCp2CIhIXkRUistg5P1lElorIZhF5RERSA9fMLmkGuFFSAQtTlSuhcCw4UbCepODtANYXnP8ugHtUdSqA/QBu6c+G+UlnvRdAomhKxMW3FJIJWA8cgG/phYhsoVJQRMYD+CCAXzjnBcAVAB53brIQwLUD0L4SGZ/JHxRNQUvD+q8HHryXJhHZwqbgDwB8BYD7jhoD4ICqZp3zjQDGed1RRBaISIOINLS2tvalrQAKa+A8iGmC4BKKhZjYe2cWctcHZwmFKFjZABeRDwFoUdVlvXkAVb1PVetVtb6mpqY3v6IbjgM3S9DCVBnLe00btzzGYYREwRIhbnMxgGtE5AMAhgI4CcC9AEaJSMLphY8HsGvgmtklwxq4URJxQc5SWJYiVtTTzmTV83nkTEyicMqmoKreparjVXUSgBsA/FFVPwngJQAfdW42H8BTA9bKAvnFrNgDN0I+jD1mVWZyVsl2avZ9OIyQKIy+pOBXAXxZRDbDronf3z9NCsZx4GZJBfSm/Tan5kFMonDClFDyVPVPAP7knN4KYE7/NymY3wp2FE353nTWAoZ0v85vc+r8VHqWUIgCGdeN9dsIl6IpEdCbzuQsz1JYIsYSClEYxqVg2mf6NUVTvoTisbZ3Jmflw7qQ+9xyGCFRMONS0G8bLoomd9ExryGBfjXwrhIKA5woiHEp6Ld+BkVT0AFJv82pkxwHThSKcQGeL6HEjGv6CcldGtarHJLJWZ57m7ofzlluqUYUyLgUtL92S8mkEIqmlFNC8VqYyq6B+/fAWQMnCmZogBvX7BNWUAklXbaEwh44URDjktBv7DBFU1BvOutTQonHBDHhQUyicoxLwjR74EbpmhYffiamfT//VQyJyGZcEmay3r02iqagESWZnOYn+hRLBaxiSEQ24wI8nbM4Btwg+d11PBazSmct3+GgyQR74ETlGJeEPIhplq4aeGlvOmtZvksiJGLCACcqw7gkTGd5ENMk3RazKhJ0QDoZj3EYIVEZxiWh3wJIFE2BMzGz3uuBA/Z0etbAiYIZl4R+s/compJBi1kFlFCSceFUeqIyjAxwllDMEbS/ZbkSiteBTyLqYlwSpjmRxyj51QiLSig5S5Gz1LeEYtfAWUIhCmJcEmay7IGbxF3rpDjAy22NxxIKUXnGJaF9EJM1cFP4zcR0Vxr0r4FzHDhROcYFOKfSm0VE7N50cQ88v7OSfwmFAU4UzLgkZAnFPF5h7J73m0qf5FR6orKMS8J0TjkO3DD2rMruYexO0vEroaQSnIlJVI5xSWiPAzeu2Se0lMe6Ju4GD0mf4xksoRCVZ1wSujvykDmCSih+5bBEjCUUonIMDXDjmn1C86pnuyUUry3VALuEwrVQiIIZlYSqyh15DJSIl4axG+h+Q0JZQiEqz6gk7HrTG9XsE14qHkO2pAZebiJPzHMjZCLqYlQSdtVNWQM3SVAJhcvJEvWeUQGezga/6SmaPCfyuKNQfCfy2PdRZS+cyI9RSeiGAEsoZknEY/kPX1emzIdxMh6Dqr3oFRF5K5uEIjJURF4XkVUislZEvuVcPllElorIZhF5RERSA93Ycl+7KZpS8Vh+7ROXu1RsUIAD3rvZE5EtTBJ2ArhCVc8GMBvAPBG5AMB3AdyjqlMB7Adwy4C10pE/iMkAN4pXCSWdL6H4r0Zo3451cCI/ZZNQbYeds0nnnwK4AsDjzuULAVw7EA0sVG7yB0VTMrCE4r+lGoCS0StE1CVUEopIXERWAmgB8AKALQAOqGrWuUkjgHE+910gIg0i0tDa2tqnxqbLvOkpmrzGdLOEQtR3oQJcVXOqOhvAeABzAMwM+wCqep+q1qtqfU1NTe9a6cj3wHkQ0yjJuJTUwMuVUBIx7518iKhLj5JQVQ8AeAnAhQBGiUjCuWo8gF3927RSrIGbKRmPleyu4573X43Qvpw1cCJ/YUah1IjIKOf0MABXAVgPO8g/6txsPoCnBqiNeRwHbqZkonR/y671wP2n0hfejohKJcrfBGMBLBSROOzAf1RVF4vIOgAPi8i/AFgB4P4BbCcAzsQ0VTJWOgrFLamUq4FzOj2Rv7IBrqpvAjjH4/KtsOvhx0yaE3mM5HUQs9wBaQ4jJCrPqCTMz8RkCcUoyUTpwlTuuu4iPsMI3RIKd6Yn8mVUEnIcuJnchakK1zXJ5CzftcCBrr0yOYyQyJ9RSZjJuttwGdXsE17SGRJYOJTQXtfd/1iGex0PYhL5MyoJ0zyIaST3A7cwjDM5K/BYhvstizVwIn9GBThr4GbymlVZbmu8rqn0LKEQ+TEqCVkDN1PKoxySyanvGHCA48CJwjAqCTmRx0wJjzAu1wN3p9KzhELkz6gkTJfZxYWiKd+bznYvoQSVwlIedXMi6s6oAHff9H5jhyma8iNKrF6UUDgOnMiXWQGetdj7NlCqFyWUrmGEPIhJ5MesAM9ZHANuoIRPCSU4wJ37WOyBE/kxKg3TOeUBTAN5rWuSyWlgDdyrbk5E3RmVhuUOfFE0pfIrC3YvoQTVwOMxQUx4EJMoiFFp6C6ARGbpmonZ1ZtOZ4NLKID3KoZE1MXAADeqyQTv7dGyVnAJBbB77hwHTuTPqDRMZ4PXz6Bo8lrXJMy3Ka9laImoi1FpyIOYZvJa1ySTtfKjU/wk46U7+RBRF6PSMJPlQUwTea1rkrHKfxgnWUIhCmRUGtrjwHkQ0zTewwit/CJX/veLcSIPUQDzApw9cON49sBDjUIRTqUnCmBUGrIGbiavHebttVA4jJCoL4xKQ07kMVPx9miqiowVsoRisYRC5MeoNOREHjMVDyPMWQrV8uu6p+IxllCIApgV4BwHbqTidU3cA5PlFiZLcBghUSCj0jDNg5hGctc1yTorC7o9cXeGph/WwImCGZWGYdbPoGgqHNPtLmpV7tuUfR/WwIn8GJWGmZyyhGIou55dVEIpVwNPsIRCFMSoNORBTHMV1rPdn2FmYmYZ4ES+jAlwy1JkQ0y/pmgqrGen8wHOmZhEfWFMGrpbazHAzVQYxtmQJZRkXLgWClEAY9LQffNzIo+ZUolYr0oorIET+SubhiIyQUReEpF1IrJWRG53Lq8SkRdEZJPzc/RANtSd0MEauJkSMeldCYUTeYh8henOZgHcqaqzAFwA4FYRmQXgawCWqOo0AEuc8wMmkx96Fh/Ih6EBUtib7vowDtED51R6Il9lA1xVm1R1uXO6DcB6AOMAfATAQudmCwFcO0BtBAB0sgdutGSioAZuhRxG6IxcUWWIE3npUUFZRCYBOAfAUgC1qtrkXNUMoNbnPgtEpEFEGlpbW3vd0EzIyR8UTal470ooqvbaKURUKnQaikgFgCcA3KGqhwqvU7uL5PkuU9X7VLVeVetramp63dCwkz8omhKxnpdQ3OVmOZSQyFuoNBSRJOzw/o2q/s65eLeIjHWuHwugZWCaaAs7coGiKZnomhYftoTitZMPEXUJMwpFANwPYL2qfr/gqkUA5jun5wN4qv+b1yXs126KplRc8rMqMyGfS7dcxqGERN4SIW5zMYCbAKwWkZXOZV8HcDeAR0XkFgDbAVw/IC10uF+7OQ7cTN1mYvZgFArQfScfIupSNsBV9S8A/LpK7+3f5vgLu4Y0RVOiYCZm2OMZXntpElEXY9IwPwqFPXAjJeOS73lnrbCjUFgDJwpiTBqmeRDTaKl4rGtDB7eEEmI9cIA9cCI/xqSh+6ZPJXgQ00RJjxJKuW9TxVuxEVF3xgQ4hxGaLRGX/IFo97ksv6UaSyhEQYxJQwa42VJFW6qJ2HtllruPe3siKmVMGqY5E9NoyXgsP4EnnbM35rCnGATcJ8GZmERBjElDjgM3WzIeQ85S5CxFJmeFeh7dEgsPYhJ5MyYN8yUUHsQ0UiLeFcaZnJU/H8T9tsUaOJE34wKcPXAzpQqGBGZy4fY25VR6omDGpGE6p6EOfFE0uSNKsrnwJRROpScKZk6AZ61QB74ompKJwh64FWpRMg4jJApmTICH7bVRNCVjXfVsuwYeooTCmZhEgYxJxLC9Noom9+BzJqeha+D5DR24sTGRJ8MC3JjmUpFkwaQc+9tU+BIKx4ETeTMmEdPZcL02iqbCIYFhP4w5jJAomDGJmMlZ3NDYYIW96UxWezQOnKNQiLwZk4g8iGm2wqVhM1a4Hng8JojHhAcxiXwYk4iZnMVZmAZLFhyQ7MmHcYIBTuTLmABPhxy5QNGUD3DLLqGEfS4LVzEkou6MScR0NscAN1iqsAduhVsLBbAnALEHTuTNmETM5JQ1cIMVL2YV9rlMxoU78hD5MCYROZHHbN2GEfaghJKMx5Cx2AMn8mJMgLtroZCZUgVDAsMuJ+vejxN5iLwZk4j2KBRjmktFuqbSh5/IA3TfS5OIujMmEVkDN1si1n098LCTsuzd7BngRF6MSURO5DFbKl8D1x4dz0hyGCGRL2MSkRN5zFZYQslamu+Rl5OKxziVnsiHMQHOg5hmc5+7I+kcAIQvoSQ4E5PIjzGJmGYJxWjuDvNHOrMA0KMSCgOcyJsxiRh2EwCKJhFBMi5od3rgoUehxGJIs4RC5Knsu0hEHhCRFhFZU3BZlYi8ICKbnJ+jB7KROUuRsxjgpkvGYziatnvgYbZUA4AUSyhEvsK8ix4EMK/osq8BWKKq0wAscc4PGPcNzIOYZkvGY101cJZQiPqsbICr6ssA9hVd/BEAC53TCwFc27/N6s59A7MGbrZkXPIB3pOp9ByFQuStt4lYq6pNzulmALV+NxSRBSLSICINra2tvXowdyo1d+QxWzIeQ3s6mz8d9j4cB07krc+JqKoKwLeLpKr3qWq9qtbX1NT06jHyJRT2wI1m18DdHnjYEgpr4ER+epuIu0VkLAA4P1v6r0ml0lkG+PGgtyUUroVC5K23ibgIwHzn9HwAT/VPc7x19cB5ENNk9kHMnpdQuBohkbcwwwgfAvA3ADNEpFFEbgFwN4CrRGQTgCud8wMmzYOYx4XCUSjht1QTpHMW7EodERVKlLuBqt7oc9V7+7ktvtwdWVhCMVv3Ekr4YYSAPRcg7BriRCcKIxIxnR8HbkRzyUfhB3DoEkrCXYaWPXCiYkYkImvgx4feBLi7hgqHEhKVMirAh7AHbrTCD+BUyFm1qXwPnAFOVMyIROQ48OND4fMXdj1w9z4McKJSRiRimgcxjwuFxzDCHs9wn3NOpycqZUQisgd+fEjGusomPZmJCbAGTuTFiETkYlbHh8IP4LDPZYolFCJfRiRifio9l5M1WmHZJOx64O7t3LkARNTFiABnCeX4kOo2jJAlFKK+MiIR3S21GOBmSxTWwHuwKz3AEgqRFyMSkePAjw9uCSURE8RiIXvgCY5CIfJjRCJmuJzsccF9/nqypgnHgRP5MyIRMzkLMQHiIXttFE3uPpg9+SDmVHoif0YEeDrHHemPB+6Ikp4MB+VUeiJ/RqRiJmdxDPhxgCUUov5lRCpmchaXkj0O9KaE4g4j5HKyRKWMSMV01uJSsseBZG9KKOyBE/kyI8BzFmvgxwG3Bt6zHrg7E5MBTlTMiFTM5JQ18OOA+y2qJzXwBEsoRL6MSMVM1sqPRiBzpfrQA+cwQqJSRqRihiWU40JvauAchULkr+yu9FFw2cyT0d6ZHexmUB+55ZCerCoZjwniMeFUeiIPRgT4TRdMHOwmUD9we95ht1NzJePCHjiRB9Yl6Jhxx/L3tByWjMdYAyfywACnYyZfA+/hxhzJeIw9cCIPDHA6ZpK9mInp3o878hCVYoDTMZPsdQ2cPXAiLwxwOmZ6W0JJxWPIWH3rge9rT+OFdbvRkcn16fcUa+vI4M5HV2HZ9v39+ntPJOmshaaDR7G68SBe2tCCZ9c09/vzNFBa2jpwx8MrsKH50KA8vhGjUOj40PsSSqxXU+kb9x/B82t34/l1zXj97X2wFPjgWWPx4xvPgUjf19ZRVdz1u9VY/GYTXtnUimduvwTVFUP6/HtdzQc7sLX1MFoPd2LP4TT2HO7E5DEj8LH68f3S/sHW1pHBlx5ZhRfX7y65rq5qOP7+g6fjqlm1kf2/prMWvvDr5WjYvh+rGg/i6S/ORcWQYxupDHA6ZnozExOwx48f6shAVT3fzB2ZHO7/y9tY13QIe9o6sbfdDrsDRzIAgOm1Fbj18qlIZy38/OWtOGvcSHz+Paf1+f/z0Os7sfjNJny8fgKeXLkLX350FR789Hmht4sL8vzaZtz62+XdlhCIxwQ5S7F1Tzu+Om9Gr4Lt7T3t+MOaJlwwZQzOrRvd53b2VuP+I7jlwQZsaT2MBZdOwcQxw1FdMQTVFUNw8Ggad/9hAxb8ahkumVaNb3x4FiZXV2BjcxuW79iP5Tv2ozNj4esfPB3jRg0btP/DPy9eh4bt+7Hg0in4xStb8fXfrca9N8z2fF5ylg7IhjQMcDpmEr1YDxwAzqkbhV+/tgO3PbQC3772TIwcnsxft2l3G7740ApsaG7D5OoRqK5IYdrJFbhwyhhMHDMc7z29FpOrRwCwe8yN+4/iu89uwKxTT8Il02ryvyebs/DDJZvw66U78LlLpuCzl0wO/KBZ33QI33p6LS6ZVo3vXHcmzpowEn/35Br87OUt+MJlU33vl7MU+9rT2NeexrjRwzx7bM+uacJtv12Bd40bia/Om4mayhSqK4agcmgS31i0Bj/78xZYqrjr/TO7hYWq4vW396GlrRN1VcMxccxwjByWRCaneH5dM367dAf+umUvAHuno29ccwY+dX5djz8IOjI53PPiW3isoRGnj63E3Kk1uGRaNWaNPQkd2RyWbt2HVzbtwaub92BvexrXzj4Vnzi/DlNqKgAAK3bsx+f+fwM6sxYWfmYOLp5aXfIYl06rwa9f247vv/AW3veDVzAsGcdhZzJfdUUKR9M5vLZ1L376yXNx/pQxge3d357GT/+0GY8va8RnL5mC//6e03zD1K+TUOzRN3biV69tx+cvnYK7PnA6Kock8K8vvIWLThuDG+bUdft9i1a9g3tf3IRHPn8hair77xsaAIhq72uLIjIPwL0A4gB+oap3B92+vr5eGxoaev14ZLa2jgzO/Obz+OIVU3Hn1TNC3y9nKe57eSv+9fmNOLlyCH5wwzk4b9Jo/HrpDvzL4nWoGJLA//vYWbhiZm3Z39XemcV1P/0rdrd14Onb5mJC1XDs3HcEtz+8Ast3HMCM2kps3N2GmadU4tvXnenZS23vzOLDP/4LDndk82UTVcVtD63As2ua8ciCC1A/qSr/f37kjZ34/cpdaD7YgX3tabjl/FHDk7j1sqm46cKJGJqMAwD+480m/M+HV2D2hFF48ObzUDk02e2xVRXfXLQWC/+2HZ+5eDL+4UOnAwD+/FYrfrhkE5bvONDt9pVDE4iJ4ODRDMaNGoYb50zAvHeNxbefWY8/bmjBDedNwLc+cgaGJOKhnotVOw/gzsdWYXPLYVx5+slo3H8UG5rb8v+f9s6svfhcIoY5k6owYkgcS9a3IGspLpwyBheeNgY/eWkzak8aigc+XY+pJ1cGPt7ew534+ctbcTSdw7snjsa5daMxoWoYtrS2Y8GvGrBj7xH844dn4aYLJpYE79F0Dg+8+jZ+9qctaE9nccapI7F610GcP7kK93x8Nk4t6L2vfecgfrRkM5Zs2I2Rw5L5bwPVFSmcUzcaV59Ri7Ej7duv3HkA1//sb5gzuQoP3nweEvEYcpZi/gOv441t+7DotrmYcUol9hzuxN8/uQbPrm3G7Amj8MMbzkHdmOGh/s7FRGSZqtaXXN7bABeROIC3AFwFoBHAGwBuVNV1fvdhgJ/YOjI5zPyHZ/GlK6fj9iun9fj+q3YewO0Pr8COfUdw5riRWNV4EJdOr8H3PnYWTq4cGvr3bNvTjmt+/BeMGz0cN180Cf+0eB1EgP/zX87ENWefiufWNuMbT63F7rYOfOr8ifjE+XWoqRyC0cNTiMcEX350JX6/Yhd+89kLcOFpXb2/to4MPvSjvyCdtfDv/60ev1+xCw+/sROHO7M4t24UZpxSmQ+GkcOSeGJ5I17ZtAenjhyKL101Hcl4DHc+tgrn1o3CL2+e41tPVVX80+J1+OWr2/CR2adi2552rGo8iHGjhuELl5+Gc+tGY8e+I9i57wh27DuCw51ZfPisU3Hp9Jp8zzNnKe554S38+KXNOLduFL593ZnYfagTG5oOYUNzG7buacepI4diximVmHnKSZheW4EnljfiZ3/eipMrh+Du/3oW3jPd/gbT0taBVzfvwWtb9mHUiCQumVqD+kmj8x9KLW0deKyhEQ+9vgON+4+ifuJo/Pymd2NMH48XHOrI4EsPr8SSDS24vn48rjy9Nn+sYM/hTjy7phktbZ248vST8b/fNxPTayvw+LJGfGPRWiTjMXznujMxfvQw/HDJZry4fjcqhyZw7exxyFqa/x3NBzvQdLADAHDW+JG46vRa/GbpDiTigqdvm4vRI1L59rS2deL9975ifzBffhr+efF6HO7I4stXT8dn507OfwPtjYEI8AsBfFNV3+ecvwsAVPU7fvdhgNNPXtqM951RW7bn5edwZxbfXLQWT696B1+ZNxM3XzSpVzXnlza24DMPvgFVoH7iaNzz8dmYUNXVOzrcmcX3ntuIhX/bBvctEhNg9PAU9rancceV03DHldNLfu/qxoO47t9eRSZn1zw/cOZYfO6SyThr/CjPdry6eQ/u/sMGrN51EAAwZ3IVfvnp8zCizMEwVcW3n1mPf3/lbUyoGoZbL5uK684d3+NVO59Z3YT/9dgqHEl3jfoYO3IoJlePQNPBDmzb247CiLi+fjz+/kOzcFLRN4MwLEuxetdBzBxbGbrHH+Z33vPiW/jRHzd3u/ykoQmccepIfPnq6TjP+TbkentPO25/eAXebLT/5iOHJXHL3MmYf9EkjBxW+v/a3HIYz69rxvNrd2PlzgMYkojhif9xEd41bmTJbV/dvAefun8pVO3A/97Hzsb02t691gsNRIB/FMA8Vf2sc/4mAOer6m1Ft1sAYAEA1NXVvXv79u29ejyiQv2xQuUTyxqxrz2Nmy+e5Ns72txyGBuaD2FvQc+upmIIbr9yum8d9T/ebMLadw7ikxdMDHWQzbIUz6xpwoodB3Dn1dMxPBXu0JSqYu07hzDjlMo+/S22tB7Ga1v3YmpNBWaeclK3YwxH0lm8tfswNjYfQl3ViG7fOKLkrd1t6MjkUF0xBGMqUmU/IDI5Cw++ug2WKj5xfl1JqcpP88EOdGZzmDhmhO9tHm3YiUNHM/j0Rf6vq54atAAvxB44EVHP+QV4Xz4edgGYUHB+vHMZEREdA30J8DcATBORySKSAnADgEX90ywiIiqn1+PAVTUrIrcBeA72MMIHVHVtv7WMiIgC9Wkij6o+A+CZfmoLERH1ABezIiIyFAOciMhQDHAiIkMxwImIDNWnxax6/GAirQB6OxWzGsCefmzOQGN7BxbbO/BMa/Px3N6JqlpTfOExDfC+EJEGr5lIUcX2Diy2d+CZ1uYTsb0soRARGYoBTkRkKJMC/L7BbkAPsb0Di+0deKa1+YRrrzE1cCIi6s6kHjgRERVggBMRGcqIABeReSKyUUQ2i8jXBrs9xUTkARFpEZE1BZdVicgLIrLJ+Vm6O+4gEZEJIvKSiKwTkbUicrtzeSTbLCJDReR1EVnltPdbzuWTRWSp87p4xFnWODJEJC4iK0RksXM+su0VkW0islpEVopIg3NZJF8PACAio0TkcRHZICLrReTCqLZXRGY4f1f33yERuaM/2hv5AHc2T/4JgPcDmAXgRhGZNbitKvEggHlFl30NwBJVnQZgiXM+KrIA7lTVWQAuAHCr8zeNaps7AVyhqmcDmA1gnohcAOC7AO5R1akA9gO4ZfCa6Ol2AOsLzke9vZer6uyCsclRfT0AwL0AnlXVmQDOhv13jmR7VXWj83edDeDdAI4AeBL90V5VjfQ/ABcCeK7g/F0A7hrsdnm0cxKANQXnNwIY65weC2DjYLcxoO1PAbjKhDYDGA5gOYDzYc9iS3i9Tgb7H+wdqpYAuALAYgAS8fZuA1BddFkkXw8ARgJ4G84gjKi3t6iNVwN4tb/aG/keOIBxAHYWnG90Lou6WlVtck43A6gdzMb4EZFJAM4BsBQRbrNTjlgJoAXACwC2ADigqlnnJlF7XfwAwFcAWM75MYh2exXA8yKyzNmIHIju62EygFYAv3RKVL8QkRGIbnsL3QDgIed0n9trQoAbT+2P2MiN1xSRCgBPALhDVQ8VXhe1NqtqTu2voOMBzAEwc3Bb5E9EPgSgRVWXDXZbemCuqp4Lu1R5q4hcWnhlxF4PCQDnAvg3VT0HQDuKyg8Ray8AwDnmcQ2Ax4qv6217TQhwUzdP3i0iYwHA+dkyyO3pRkSSsMP7N6r6O+fiSLcZAFT1AICXYJcgRomIu6tUlF4XFwO4RkS2AXgYdhnlXkS3vVDVXc7PFtj12TmI7uuhEUCjqi51zj8OO9Cj2l7X+wEsV9Xdzvk+t9eEADd18+RFAOY7p+fDrjNHgogIgPsBrFfV7xdcFck2i0iNiIxyTg+DXa9fDzvIP+rcLDLtVdW7VHW8qk6C/Xr9o6p+EhFtr4iMEJFK9zTsOu0aRPT1oKrNAHaKyAznovcCWIeItrfAjegqnwD90d7BLuqHLPx/AMBbsOuefzfY7fFo30MAmgBkYPcOboFd81wCYBOAFwFUDXY7C9o7F/bXtTcBrHT+fSCqbQZwFoAVTnvXAPhH5/IpAF4HsBn219Ihg91Wj7ZfBmBxlNvrtGuV82+t+x6L6uvBadtsAA3Oa+L3AEZHvL0jAOwFMLLgsj63l1PpiYgMZUIJhYiIPDDAiYgMxQAnIjIUA5yIyFAMcCIiQzHAiYgMxQAnIjLUfwKWyGJWd9a+UgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -----------------\n",
    "# This code is used to close an env that might not have been closed before\n",
    "try:\n",
    "  env.close()\n",
    "except:\n",
    "  pass\n",
    "# -----------------\n",
    "\n",
    "from mlagents_envs.registry import default_registry\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create the GridWorld Environment from the registry\n",
    "env = UE(file_name='run31', seed=1, side_channels=[])\n",
    "# env = default_registry[\"GridWorld\"].make()\n",
    "print(\"GridWorld environment created.\")\n",
    "\n",
    "# Create a new Q-Network. \n",
    "qnet = VisualQNetwork((44, 1), 126, 3)\n",
    "\n",
    "experiences: Buffer = []\n",
    "optim = torch.optim.Adam(qnet.parameters(), lr= 0.001)\n",
    "\n",
    "cumulative_rewards: List[float] = []\n",
    "\n",
    "# The number of training steps that will be performed\n",
    "NUM_TRAINING_STEPS = 70\n",
    "# The number of experiences to collect per training step\n",
    "NUM_NEW_EXP = 1000\n",
    "# The maximum size of the Buffer\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "for n in range(NUM_TRAINING_STEPS):\n",
    "  new_exp,_ = Trainer.generate_trajectories(env, qnet, NUM_NEW_EXP, epsilon=0.1)\n",
    "  random.shuffle(experiences)\n",
    "  if len(experiences) > BUFFER_SIZE:\n",
    "    experiences = experiences[:BUFFER_SIZE]\n",
    "  experiences.extend(new_exp)\n",
    "  Trainer.update_q_net(qnet, optim, experiences, 3)\n",
    "  _, rewards = Trainer.generate_trajectories(env, qnet, 100, epsilon=0)\n",
    "  cumulative_rewards.append(rewards)\n",
    "  print(\"Training step \", n+1, \"\\treward \", rewards)\n",
    "  print()\n",
    "\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Show the training graph\n",
    "plt.plot(range(NUM_TRAINING_STEPS), cumulative_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed environment\n"
     ]
    }
   ],
   "source": [
    "env.close()\n",
    "print(\"Closed environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
