{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading pip-21.1.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.0.2\n",
      "    Not uninstalling pip at /usr/lib/python3/dist-packages, outside environment /usr\n",
      "    Can't uninstall 'pip'. No files were found to uninstall.\n",
      "Successfully installed pip-21.1.2\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlagents\n",
      "  Using cached mlagents-0.26.0-py3-none-any.whl (160 kB)\n",
      "Collecting mlagents-envs==0.26.0\n",
      "  Using cached mlagents_envs-0.26.0-py3-none-any.whl (75 kB)\n",
      "Collecting numpy<2.0,>=1.13.3\n",
      "  Using cached numpy-1.20.3-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.4 MB)\n",
      "Collecting cattrs>=1.1.0\n",
      "  Using cached cattrs-1.7.1-py3-none-any.whl (22 kB)\n",
      "Collecting protobuf>=3.6\n",
      "  Using cached protobuf-3.17.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "Collecting attrs>=19.3.0\n",
      "  Using cached attrs-21.2.0-py2.py3-none-any.whl (53 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Using cached h5py-3.2.1-cp38-cp38-manylinux1_x86_64.whl (4.5 MB)\n",
      "Collecting grpcio>=1.11.0\n",
      "  Using cached grpcio-1.38.0-cp38-cp38-manylinux2014_x86_64.whl (4.2 MB)\n",
      "Collecting Pillow>=4.2.1\n",
      "  Using cached Pillow-8.2.0-cp38-cp38-manylinux1_x86_64.whl (3.0 MB)\n",
      "Collecting pyyaml>=3.1.0\n",
      "  Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n",
      "Collecting torch<1.9.0,>=1.6.0\n",
      "  Using cached torch-1.8.1-cp38-cp38-manylinux1_x86_64.whl (804.1 MB)\n",
      "Collecting tensorboard>=1.15\n",
      "  Using cached tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "Collecting cloudpickle\n",
      "  Using cached cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Collecting six>=1.5.2\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Using cached google_auth-1.30.1-py2.py3-none-any.whl (146 kB)\n",
      "Collecting setuptools>=41.0.0\n",
      "  Using cached setuptools-57.0.0-py3-none-any.whl (821 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
      "Collecting absl-py>=0.4\n",
      "  Using cached absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Collecting wheel>=0.26\n",
      "  Using cached wheel-0.36.2-py2.py3-none-any.whl (35 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting chardet<5,>=3.0.2\n",
      "  Using cached chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.5-py2.py3-none-any.whl (138 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting typing-extensions\n",
      "  Using cached typing_extensions-3.10.0.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: urllib3, pyasn1, idna, chardet, certifi, six, setuptools, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, wheel, werkzeug, typing-extensions, tensorboard-plugin-wit, tensorboard-data-server, pyyaml, protobuf, Pillow, numpy, markdown, grpcio, google-auth-oauthlib, cloudpickle, attrs, absl-py, torch, tensorboard, mlagents-envs, h5py, cattrs, mlagents\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.5\n",
      "    Uninstalling urllib3-1.26.5:\n",
      "      Successfully uninstalled urllib3-1.26.5\n",
      "  Attempting uninstall: pyasn1\n",
      "    Found existing installation: pyasn1 0.4.8\n",
      "    Uninstalling pyasn1-0.4.8:\n",
      "      Successfully uninstalled pyasn1-0.4.8\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 2.10\n",
      "    Uninstalling idna-2.10:\n",
      "      Successfully uninstalled idna-2.10\n",
      "  Attempting uninstall: chardet\n",
      "    Found existing installation: chardet 4.0.0\n",
      "    Uninstalling chardet-4.0.0:\n",
      "      Successfully uninstalled chardet-4.0.0\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2021.5.30\n",
      "    Uninstalling certifi-2021.5.30:\n",
      "      Successfully uninstalled certifi-2021.5.30\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 57.0.0\n",
      "    Uninstalling setuptools-57.0.0:\n",
      "      Successfully uninstalled setuptools-57.0.0\n",
      "  Attempting uninstall: rsa\n",
      "    Found existing installation: rsa 4.7.2\n",
      "    Uninstalling rsa-4.7.2:\n",
      "      Successfully uninstalled rsa-4.7.2\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.25.1\n",
      "    Uninstalling requests-2.25.1:\n",
      "      Successfully uninstalled requests-2.25.1\n",
      "  Attempting uninstall: pyasn1-modules\n",
      "    Found existing installation: pyasn1-modules 0.2.1\n",
      "\u001b[31mERROR: Cannot uninstall 'pyasn1-modules'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall mlagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml-agents already installed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import mlagents\n",
    "    from mlagents_envs.environment import UnityEnvironment as UE\n",
    "    from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "    print(\"ml-agents already installed\")\n",
    "except ImportError:\n",
    "#     !pip install mlagents==0.26.0\n",
    "    print(\"Installed ml-agents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "from math import floor\n",
    "\n",
    "\n",
    "class VisualQNetwork(torch.nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    input_shape: Tuple[int], \n",
    "    encoding_size: int, \n",
    "    output_size: int\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Creates a neural network that takes as input a batch of images (3\n",
    "    dimensional tensors) and outputs a batch of outputs (1 dimensional\n",
    "    tensors)\n",
    "    \"\"\"\n",
    "    super(VisualQNetwork, self).__init__()\n",
    "#     height = input_shape[0]\n",
    "#     width = input_shape[1]\n",
    "#     initial_channels = input_shape[2]\n",
    "#     conv_1_hw = self.conv_output_shape((height, width), 8, 4)\n",
    "#     conv_2_hw = self.conv_output_shape(conv_1_hw, 4, 2)\n",
    "#     self.final_flat = conv_2_hw[0] * conv_2_hw[1] * 32\n",
    "    \n",
    "    \n",
    "#     self.conv1 = torch.nn.Conv2d(initial_channels, 16, [8, 8], [4, 4])\n",
    "#     self.conv2 = torch.nn.Conv2d(16, 32, [4, 4], [2, 2])\n",
    "#     self.dense1 = torch.nn.Linear(self.final_flat, encoding_size)\n",
    "\n",
    "    \n",
    "    self.dense1 = torch.nn.Linear(input_shape[0], encoding_size)\n",
    "    self.dense2 = torch.nn.Linear(encoding_size, encoding_size)\n",
    "    \n",
    "    self.dense2_x1 = torch.nn.Linear(encoding_size, output_size)\n",
    "    self.dense2_x2 = torch.nn.Linear(encoding_size, output_size)\n",
    "    self.dense2_x3 = torch.nn.Linear(encoding_size, output_size)\n",
    "\n",
    "    \n",
    "    \n",
    "  def forward(self, visual_obs: torch.tensor):\n",
    "#     print(\"torch input size:\", visual_obs.size())\n",
    "#     visual_obs = visual_obs.permute(0, 3, 1, 2)\n",
    "#     conv_1 = torch.relu(self.conv1(visual_obs))\n",
    "#     conv_2 = torch.relu(self.conv2(conv_1))\n",
    "#     hidden = self.dense1(conv_2.reshape([-1, self.final_flat]))\n",
    "\n",
    "    hidden = self.dense1(visual_obs)\n",
    "    hidden = torch.relu(hidden)\n",
    "\n",
    "    hidden = self.dense2(hidden)\n",
    "    hidden = torch.relu(hidden)\n",
    "\n",
    "    x1 = self.dense2_x1(hidden)\n",
    "    x2 = self.dense2_x2(hidden)\n",
    "    x3 = self.dense2_x3(hidden)\n",
    "\n",
    "    return x1, x2, x3\n",
    "\n",
    "  @staticmethod\n",
    "  def conv_output_shape(\n",
    "    h_w: Tuple[int, int],\n",
    "    kernel_size: int = 1,\n",
    "    stride: int = 1,\n",
    "    pad: int = 0,\n",
    "    dilation: int = 1,\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Computes the height and width of the output of a convolution layer.\n",
    "    \"\"\"\n",
    "    h = floor(\n",
    "      ((h_w[0] + (2 * pad) - (dilation * (kernel_size - 1)) - 1) / stride) + 1\n",
    "    )\n",
    "    w = floor(\n",
    "      ((h_w[1] + (2 * pad) - (dilation * (kernel_size - 1)) - 1) / stride) + 1\n",
    "    )\n",
    "    return h, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import NamedTuple, List\n",
    "\n",
    "\n",
    "class Experience(NamedTuple):\n",
    "  \"\"\"\n",
    "  An experience contains the data of one Agent transition.\n",
    "  - Observation\n",
    "  - Action\n",
    "  - Reward\n",
    "  - Done flag\n",
    "  - Next Observation\n",
    "  \"\"\"\n",
    "\n",
    "  obs: np.ndarray\n",
    "  action: np.ndarray\n",
    "  reward: float\n",
    "  done: bool\n",
    "  next_obs: np.ndarray\n",
    "\n",
    "# A Trajectory is an ordered sequence of Experiences\n",
    "Trajectory = List[Experience]\n",
    "\n",
    "# A Buffer is an unordered list of Experiences from multiple Trajectories\n",
    "Buffer = List[Experience]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import ActionTuple, BaseEnv\n",
    "from typing import Dict\n",
    "import random\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "  @staticmethod\n",
    "  def generate_trajectories(\n",
    "    env: BaseEnv, q_net: VisualQNetwork, buffer_size: int, epsilon: float\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Given a Unity Environment and a Q-Network, this method will generate a\n",
    "    buffer of Experiences obtained by running the Environment with the Policy\n",
    "    derived from the Q-Network.\n",
    "    :param BaseEnv: The UnityEnvironment used.\n",
    "    :param q_net: The Q-Network used to collect the data.\n",
    "    :param buffer_size: The minimum size of the buffer this method will return.\n",
    "    :param epsilon: Will add a random normal variable with standard deviation.\n",
    "    epsilon to the value heads of the Q-Network to encourage exploration.\n",
    "    :returns: a Tuple containing the created buffer and the average cumulative\n",
    "    the Agents obtained.\n",
    "    \"\"\"\n",
    "    # Create an empty Buffer\n",
    "    buffer: Buffer = []\n",
    "\n",
    "    # Reset the environment\n",
    "    env.reset()\n",
    "    # Read and store the Behavior Name of the Environment\n",
    "    behavior_name = list(env.behavior_specs)[0]\n",
    "    # Read and store the Behavior Specs of the Environment\n",
    "    spec = env.behavior_specs[behavior_name]\n",
    "\n",
    "    # Create a Mapping from AgentId to Trajectories. This will help us create\n",
    "    # trajectories for each Agents\n",
    "    dict_trajectories_from_agent: Dict[int, Trajectory] = {}\n",
    "    # Create a Mapping from AgentId to the last observation of the Agent\n",
    "    dict_last_obs_from_agent: Dict[int, np.ndarray] = {}\n",
    "    # Create a Mapping from AgentId to the last observation of the Agent\n",
    "    dict_last_action_from_agent: Dict[int, np.ndarray] = {}\n",
    "    # Create a Mapping from AgentId to cumulative reward (Only for reporting)\n",
    "    dict_cumulative_reward_from_agent: Dict[int, float] = {}\n",
    "    # Create a list to store the cumulative rewards obtained so far\n",
    "    cumulative_rewards: List[float] = []\n",
    "    \n",
    "    \n",
    "    entered_terminal = False\n",
    "    while len(buffer) < buffer_size:  # While not enough data in the buffer\n",
    "      # Get the Decision Steps and Terminal Steps of the Agents\n",
    "      decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    \n",
    "        # For all Agents with a Terminal Step:\n",
    "      for agent_id_terminated in terminal_steps:\n",
    "#         print(\"entered agent with terminal step\")\n",
    "#         print(agent_id_terminated)\n",
    "\n",
    "        # Create its last experience (is last because the Agent terminated)\n",
    "        last_experience = Experience(\n",
    "          obs=dict_last_obs_from_agent[agent_id_terminated].copy(),\n",
    "          reward=terminal_steps[agent_id_terminated].reward,\n",
    "          done=not terminal_steps[agent_id_terminated].interrupted,\n",
    "          action=dict_last_action_from_agent[agent_id_terminated].copy(),\n",
    "          next_obs=terminal_steps[agent_id_terminated].obs[0],\n",
    "        )\n",
    "        # Clear its last observation and action (Since the trajectory is over)\n",
    "        dict_last_obs_from_agent.pop(agent_id_terminated)\n",
    "        dict_last_action_from_agent.pop(agent_id_terminated)\n",
    "        # Report the cumulative reward\n",
    "        cumulative_reward = (\n",
    "          dict_cumulative_reward_from_agent.pop(agent_id_terminated)\n",
    "          + terminal_steps[agent_id_terminated].reward\n",
    "        )\n",
    "        print(\"cumulative reward: \", cumulative_reward)\n",
    "        print(\"reward at the terminal step: \", terminal_steps[agent_id_terminated].reward)\n",
    "        \n",
    "        cumulative_rewards.append(cumulative_reward)\n",
    "        # Add the Trajectory and the last experience to the buffer\n",
    "        buffer.extend(dict_trajectories_from_agent.pop(agent_id_terminated))\n",
    "        buffer.append(last_experience)\n",
    "        entered_terminal = True\n",
    "\n",
    "      # For all Agents with a Decision Step:\n",
    "      for agent_id_decisions in decision_steps:\n",
    "        # If the Agent does not have a Trajectory, create an empty one\n",
    "        if agent_id_decisions not in dict_trajectories_from_agent:\n",
    "          dict_trajectories_from_agent[agent_id_decisions] = []\n",
    "          dict_cumulative_reward_from_agent[agent_id_decisions] = 0\n",
    "\n",
    "        # If the Agent requesting a decision has a \"last observation\"\n",
    "        if agent_id_decisions in dict_last_obs_from_agent:\n",
    "          # Create an Experience from the last observation and the Decision Step\n",
    "        \n",
    "          exp = Experience(\n",
    "            obs=dict_last_obs_from_agent[agent_id_decisions].copy(),\n",
    "            reward=decision_steps[agent_id_decisions].reward - 0.05,\n",
    "            done=False,\n",
    "            action=dict_last_action_from_agent[agent_id_decisions].copy(),\n",
    "            next_obs=decision_steps[agent_id_decisions].obs[0],\n",
    "          )\n",
    "#           print(\"reward at decision step: \", decision_steps[agent_id_decisions].reward - 0.05)\n",
    "          \n",
    "          # Update the Trajectory of the Agent and its cumulative reward\n",
    "          dict_trajectories_from_agent[agent_id_decisions].append(exp)\n",
    "          dict_cumulative_reward_from_agent[agent_id_decisions] += (\n",
    "            decision_steps[agent_id_decisions].reward\n",
    "          )\n",
    "        # Store the observation as the new \"last observation\"\n",
    "        dict_last_obs_from_agent[agent_id_decisions] = (\n",
    "          decision_steps[agent_id_decisions].obs[0]\n",
    "        )\n",
    "\n",
    "      # Generate an action for all the Agents that requested a decision\n",
    "      # Compute the values for each action given the observation\n",
    "     \n",
    "        \n",
    "      act1, act2, act3 = q_net(torch.from_numpy(decision_steps.obs[0]))\n",
    "      act1 = act1.detach().numpy()\n",
    "      act2 = act2.detach().numpy()\n",
    "      act3 = act3.detach().numpy()\n",
    "\n",
    "      try:\n",
    "        actions_values = np.array([act1, act2, act3]).reshape(3,3)\n",
    "      except:\n",
    "#         pass\n",
    "        actions_values = np.zeros((3,3))\n",
    "#         print(\"error: network received an input of size 0 and i caught the error :/\")\n",
    "\n",
    "        #      0-8nt(type(actions_values))\n",
    "      # Add some noise with epsilon to the values\n",
    "#       actions_values += epsilon * np.random.randn(actions_values.shape[0], actions_values.shape[1]).astype(np.float32)\n",
    "      # Pick the best action using argmax\n",
    "      actions = np.argmax(actions_values, axis=1)\n",
    "      actions.resize((len(decision_steps), 3))\n",
    "    \n",
    "      # Store the action that was picked, it will be put in the trajectory later\n",
    "      for agent_index, agent_id in enumerate(decision_steps.agent_id):\n",
    "        dict_last_action_from_agent[agent_id] = actions[agent_index]\n",
    "\n",
    "      # Set the actions in the environment\n",
    "      # Unity Environments expect ActionTuple instances.\n",
    "      action_tuple = ActionTuple()\n",
    "      action_tuple.add_discrete(actions)\n",
    "#       print(\"action received from QNetwork: \", action_tuple.discrete)\n",
    "      env.set_actions(behavior_name, action_tuple)\n",
    "      # Perform a step in the simulation\n",
    "      env.step()\n",
    "    return buffer, np.mean(cumulative_rewards)\n",
    "\n",
    "  @staticmethod\n",
    "  def update_q_net(\n",
    "    q_net: VisualQNetwork, \n",
    "    optimizer: torch.optim, \n",
    "    buffer: Buffer, \n",
    "    action_size: int\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Performs an update of the Q-Network using the provided optimizer and buffer\n",
    "    \"\"\"\n",
    "    def calculate_bellman_loss(next_pred_action, pred_action, reward, done, GAMMA, batch, action_size, action):\n",
    "        # Use the Bellman equation to update the Q-Network\n",
    "        target = (\n",
    "          reward\n",
    "          + (1.0 - done)\n",
    "          * GAMMA\n",
    "          * torch.max(next_pred_action.detach(), dim=1, keepdim=True).values\n",
    "        )\n",
    "        mask = torch.zeros((len(batch), action_size))\n",
    "        mask.scatter_(1, action, 1)\n",
    "        prediction = torch.sum(pred_action * mask, dim=1, keepdim=True)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        loss = criterion(prediction, target)\n",
    "        return loss\n",
    "\n",
    "    BATCH_SIZE = 1000\n",
    "    NUM_EPOCH = 3\n",
    "    GAMMA = 0.9\n",
    "    batch_size = min(len(buffer), BATCH_SIZE)\n",
    "    random.shuffle(buffer)\n",
    "    # Split the buffer into batches\n",
    "    batches = [\n",
    "      buffer[batch_size * start : batch_size * (start + 1)]\n",
    "      for start in range(int(len(buffer) / batch_size))\n",
    "    ]\n",
    "    for _ in range(NUM_EPOCH):\n",
    "      for batch in batches:\n",
    "        # Create the Tensors that will be fed in the network\n",
    "        obs = torch.from_numpy(np.stack([ex.obs for ex in batch]))\n",
    "        reward = torch.from_numpy(\n",
    "          np.array([ex.reward for ex in batch], dtype=np.float32).reshape(-1, 1)\n",
    "        )\n",
    "        done = torch.from_numpy(\n",
    "          np.array([ex.done for ex in batch], dtype=np.float32).reshape(-1, 1)\n",
    "        )\n",
    "        action = torch.from_numpy(np.stack([ex.action for ex in batch]))\n",
    "        next_obs = torch.from_numpy(np.stack([ex.next_obs for ex in batch]))\n",
    "        \n",
    "        # Prerequisite: collect outputs\n",
    "        pnext_a1, pnext_a2, pnext_a3 = q_net(next_obs)\n",
    "        p_a1, p_a2, p_a3 = q_net(obs)\n",
    "        \n",
    "        # bellman equation for each loss\n",
    "        loss1 = calculate_bellman_loss(pnext_a1, p_a1, reward, done, GAMMA, batch, action_size, action)\n",
    "        loss2 = calculate_bellman_loss(pnext_a2, p_a2, reward, done, GAMMA, batch, action_size, action)\n",
    "        loss3 = calculate_bellman_loss(pnext_a3, p_a3, reward, done, GAMMA, batch, action_size, action)\n",
    "        loss = loss1 + loss2 + loss3\n",
    "        \n",
    "        # Perform the backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridWorld environment created.\n",
      "cumulative reward:  0.0\n",
      "reward at the terminal step:  0.0\n",
      "cumulative reward:  9.099999397993088\n",
      "reward at the terminal step:  -1.0\n",
      "Training step  1 \treward  9.099999397993088\n",
      "\n",
      "cumulative reward:  10.829999268054962\n",
      "reward at the terminal step:  -0.97\n",
      "cumulative reward:  10.249999329447746\n",
      "reward at the terminal step:  -1.0\n",
      "cumulative reward:  2716.8445627987385\n",
      "reward at the terminal step:  3.7837474\n",
      "Training step  2 \treward  2716.8445627987385\n",
      "\n",
      "cumulative reward:  13.74999913573265\n",
      "reward at the terminal step:  -0.95\n",
      "cumulative reward:  7.899999529123306\n",
      "reward at the terminal step:  0.0\n",
      "cumulative reward:  1.2999999225139618\n",
      "reward at the terminal step:  0.0\n",
      "Training step  3 \treward  1.2999999225139618\n",
      "\n",
      "cumulative reward:  0.0\n",
      "reward at the terminal step:  0.0\n",
      "cumulative reward:  42.43999743461609\n",
      "reward at the terminal step:  -0.96\n",
      "Training step  4 \treward  42.43999743461609\n",
      "\n",
      "cumulative reward:  35.78999783098698\n",
      "reward at the terminal step:  -0.96\n",
      "cumulative reward:  49.739997036755085\n",
      "reward at the terminal step:  0.04\n",
      "cumulative reward:  6.139999598264694\n",
      "reward at the terminal step:  -0.96\n",
      "Training step  5 \treward  6.139999598264694\n",
      "\n",
      "cumulative reward:  8.03999948501587\n",
      "reward at the terminal step:  -0.96\n",
      "cumulative reward:  10.909999281167984\n",
      "reward at the terminal step:  -0.99\n",
      "cumulative reward:  7.259999498724937\n",
      "reward at the terminal step:  -0.99\n",
      "cumulative reward:  5.029999613761902\n",
      "reward at the terminal step:  -0.97\n",
      "cumulative reward:  4.979999616742134\n",
      "reward at the terminal step:  -0.97\n",
      "cumulative reward:  6.0899996012449265\n",
      "reward at the terminal step:  -0.96\n",
      "cumulative reward:  10.929999262094498\n",
      "reward at the terminal step:  -0.97\n",
      "cumulative reward:  13.679999098181725\n",
      "reward at the terminal step:  -0.97\n",
      "Training step  6 \treward  13.679999098181725\n",
      "\n",
      "cumulative reward:  36.53999778628349\n",
      "reward at the terminal step:  -0.96\n",
      "cumulative reward:  28.939998239278793\n",
      "reward at the terminal step:  -0.96\n",
      "cumulative reward:  54.162413001060486\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  7 \treward  54.162413001060486\n",
      "\n",
      "cumulative reward:  52.27455452084541\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  54.84599259495735\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  8 \treward  54.84599259495735\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  42.84999740123749\n",
      "reward at the terminal step:  -0.95\n",
      "Training step  9 \treward  42.84999740123749\n",
      "\n",
      "cumulative reward:  2044.2229522764683\n",
      "reward at the terminal step:  -0.97\n",
      "cumulative reward:  49.97999702207744\n",
      "reward at the terminal step:  0.03\n",
      "cumulative reward:  4509.3486084342\n",
      "reward at the terminal step:  4.825876\n",
      "Training step  10 \treward  4509.3486084342\n",
      "\n",
      "cumulative reward:  2425.7656964957714\n",
      "reward at the terminal step:  3.8516822\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  11 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  12 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.899997025728226\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  13 \treward  49.899997025728226\n",
      "\n",
      "cumulative reward:  49.899997025728226\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  14 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  15 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.39999705553055\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  16 \treward  49.39999705553055\n",
      "\n",
      "cumulative reward:  3.0499998182058334\n",
      "reward at the terminal step:  0.0\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  17 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  22.34999866783619\n",
      "reward at the terminal step:  0.0\n",
      "Training step  18 \treward  22.34999866783619\n",
      "\n",
      "cumulative reward:  17.499998956918716\n",
      "reward at the terminal step:  0.0\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  19 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  20 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  21 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  22 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  36.299997836351395\n",
      "reward at the terminal step:  0.0\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  23 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  24 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.79999703168869\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  25 \treward  49.79999703168869\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  26 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  27 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  41.29999753832817\n",
      "reward at the terminal step:  0.0\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  28 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  29 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  30 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  31 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  32 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  33 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  13.449999198317528\n",
      "reward at the terminal step:  0.0\n",
      "Training step  34 \treward  13.449999198317528\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  48.64999710023403\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  35 \treward  48.64999710023403\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  5.299999684095383\n",
      "reward at the terminal step:  0.0\n",
      "Training step  36 \treward  5.299999684095383\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.14999707043171\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  37 \treward  49.14999707043171\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  48.04999713599682\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  38 \treward  48.04999713599682\n",
      "\n",
      "cumulative reward:  0.6999999582767487\n",
      "reward at the terminal step:  0.0\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  39 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  40 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  41 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  21.84999869763851\n",
      "reward at the terminal step:  0.0\n",
      "Training step  42 \treward  21.84999869763851\n",
      "\n",
      "cumulative reward:  38.299997717142105\n",
      "reward at the terminal step:  0.0\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  43 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  44 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  45 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.94999702274799\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.94999702274799\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  46 \treward  49.94999702274799\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  47 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  48 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  10.299999386072159\n",
      "reward at the terminal step:  0.0\n",
      "cumulative reward:  49.64999704062939\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  49 \treward  49.64999704062939\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  42.89999744296074\n",
      "reward at the terminal step:  0.0\n",
      "Training step  50 \treward  42.89999744296074\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  35.94999785721302\n",
      "reward at the terminal step:  0.0\n",
      "Training step  51 \treward  35.94999785721302\n",
      "\n",
      "cumulative reward:  23.149998620152473\n",
      "reward at the terminal step:  0.0\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  52 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  53 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.94999702274799\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  54 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  55 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  37.79999774694443\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  56 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  48.04999713599682\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.94999702274799\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  57 \treward  49.94999702274799\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  58 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.54999704658985\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  59 \treward  49.54999704658985\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  60 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  61 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  62 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  63 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  64 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  65 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  66 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  67 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  68 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n",
      "Training step  69 \treward  49.99999701976776\n",
      "\n",
      "cumulative reward:  49.99999701976776\n",
      "reward at the terminal step:  0.049999997\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a7bd7b6f701f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0mexperiences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_q_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m   \u001b[0mcumulative_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training step \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\treward \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-774dc65a4b7d>\u001b[0m in \u001b[0;36mgenerate_trajectories\u001b[0;34m(env, q_net, buffer_size, epsilon)\u001b[0m\n\u001b[1;32m    143\u001b[0m       \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbehavior_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m       \u001b[0;31m# Perform a step in the simulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m       \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcumulative_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/mlagents_envs/timers.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/mlagents_envs/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mstep_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_step_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"communicator.exchange\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexchange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll_process\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUnityCommunicatorStoppedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Communicator has exited.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/mlagents_envs/rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[0;34m(self, inputs, poll_callback)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll_for_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/mlagents_envs/rpc_communicator.py\u001b[0m in \u001b[0;36mpoll_for_timeout\u001b[0;34m(self, poll_callback)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mcallback_timeout_wait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout_wait\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback_timeout_wait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0;31m# Got an acknowledgment from the connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "# This code is used to close an env that might not have been closed before\n",
    "try:\n",
    "  env.close()\n",
    "except:\n",
    "  pass\n",
    "# -----------------\n",
    "\n",
    "from mlagents_envs.registry import default_registry\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create the GridWorld Environment from the registry\n",
    "env = UE(file_name='run31', seed=1, side_channels=[])\n",
    "# env = default_registry[\"GridWorld\"].make()\n",
    "print(\"GridWorld environment created.\")\n",
    "\n",
    "# Create a new Q-Network. \n",
    "qnet = VisualQNetwork((44, 1), 126, 3)\n",
    "\n",
    "experiences: Buffer = []\n",
    "optim = torch.optim.Adam(qnet.parameters(), lr= 0.001)\n",
    "\n",
    "cumulative_rewards: List[float] = []\n",
    "\n",
    "# The number of training steps that will be performed\n",
    "NUM_TRAINING_STEPS = 70\n",
    "# The number of experiences to collect per training step\n",
    "NUM_NEW_EXP = 1000\n",
    "# The maximum size of the Buffer\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "for n in range(NUM_TRAINING_STEPS):\n",
    "  new_exp,_ = Trainer.generate_trajectories(env, qnet, NUM_NEW_EXP, epsilon=0.1)\n",
    "  random.shuffle(experiences)\n",
    "  if len(experiences) > BUFFER_SIZE:\n",
    "    experiences = experiences[:BUFFER_SIZE]\n",
    "  experiences.extend(new_exp)\n",
    "  Trainer.update_q_net(qnet, optim, experiences, 3)\n",
    "  _, rewards = Trainer.generate_trajectories(env, qnet, 100, epsilon=0)\n",
    "  cumulative_rewards.append(rewards)\n",
    "  print(\"Training step \", n+1, \"\\treward \", rewards)\n",
    "  print()\n",
    "\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Show the training graph\n",
    "plt.plot(range(NUM_TRAINING_STEPS), cumulative_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed environment\n"
     ]
    }
   ],
   "source": [
    "env.close()\n",
    "print(\"Closed environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
