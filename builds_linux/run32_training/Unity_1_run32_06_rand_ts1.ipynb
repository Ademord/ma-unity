{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f9f89eb-2d35-426d-ae3e-bba27489dd3d",
   "metadata": {},
   "source": [
    "# üöÄ Install, Import, and Log In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e55fe183-657a-4323-8641-7655c8a8f7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import random\n",
    "from functools import wraps\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d1e49908-fed1-47a2-89b0-829c1e527d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(func):\n",
    "    @wraps(func)\n",
    "    def _time_it(*args, **kwargs):\n",
    "        start = int(round(time() * 1000))\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        finally:\n",
    "            end_ = int(round(time() * 1000)) - start\n",
    "            print(f\"Total execution time: {end_/(1000*60) if end_ > 0 else 0} minutes\")\n",
    "    return _time_it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfbd7d2-c9c7-41f8-b272-178742073096",
   "metadata": {},
   "source": [
    "### 0Ô∏è‚É£ Step 0: Install W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8143d7fa-840c-4f68-94b9-f39d8f0775f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec61dda-606d-45c0-bbd6-6193665559b1",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Step 1: Import W&B and Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef585afd-2998-4b9c-b4e9-65454d72e3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72848fd9-bcdb-486b-83a5-13abf7e1bfb7",
   "metadata": {},
   "source": [
    "# üë©‚Äçüî¨ Define the Experiment and Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5314474d-5562-4e65-bf6b-7cad3756473e",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Step 2: Track metadata and hyperparameters with `wandb.init`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "fa146a4e-9987-4728-9e73-afc9d8e6eec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    epochs=5,\n",
    "    classes=10,\n",
    "    kernels=[16, 32],\n",
    "    batch_size=128,\n",
    "    learning_rate=0.005,\n",
    "    dataset=\"MNIST\",\n",
    "    architecture=\"CNN\",\n",
    "    NUM_TRAINING_STEPS = 100000, #10000000\n",
    "    NUM_TEST_STEPS = 10,\n",
    "    NUM_NEW_EXP = 1000,\n",
    "    BUFFER_SIZE = 10000,\n",
    "    worker_id=3,\n",
    "    time_scale=1\n",
    "    )\n",
    "env = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d2308ec7-05f9-4f7d-8ea9-f6c1170d4c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters):\n",
    "    global env\n",
    "    \n",
    "    try:\n",
    "        env.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"pytorch-demo1\", config=hyperparameters):\n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "\n",
    "        # make the model, data, and optimization problem\n",
    "        model, env, criterion, optimizer = make(config)\n",
    "        print(model)\n",
    "\n",
    "        # and use them to train the model\n",
    "        train(model, env, criterion, optimizer, config)\n",
    "\n",
    "        # and test its final performance\n",
    "        test(model, env, config)\n",
    "\n",
    "        env.close()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9bb60f26-0808-4f56-b6ef-0cc41c3033f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make(config):\n",
    "    # Make the data\n",
    "    env, spec = make_env(config)\n",
    "    \n",
    "    # Make the model\n",
    "    # model = Agent(config.kernels, config.classes).to(device)\n",
    "    model = Agent(spec)\n",
    "\n",
    "    # Make the loss and optimizer\n",
    "    criterion = None # nn.CrossEntropyLoss()\n",
    "    optimizer = None # torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    \n",
    "    return model, env, criterion, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a15c7c2-720c-4c1e-b005-0707e9b16a9c",
   "metadata": {},
   "source": [
    "# üì° Define the Env Loading and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "eb6b9020-dcf3-4ffc-80a5-5217d0fe5937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(config):\n",
    "    channel = EngineConfigurationChannel()\n",
    "    env = UE(\"run32_training\", seed=1, worker_id=config.worker_id, side_channels=[channel])\n",
    "    channel.set_configuration_parameters(time_scale = config.time_scale)\n",
    "    print(\"Environment created.\")\n",
    "    \n",
    "    env.reset()\n",
    "    behavior_name = list(env.behavior_specs)[0]\n",
    "    print(f\"Name of the behavior : {behavior_name}\")\n",
    "    \n",
    "    spec = env.behavior_specs[behavior_name]\n",
    "    print(f\"Type of the spec : {spec}\")\n",
    "    \n",
    "    return env, spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3dc6a727-901a-4175-b712-a1d6364ec1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conventional and convolutional neural network\n",
    "class Agent():\n",
    "    def __init__(self, spec):\n",
    "        self.spec = spec\n",
    "    def get_action(self, decision_steps):\n",
    "        return self.spec.action_spec.random_action(len(decision_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c1fcbe6d-7616-4035-9893-bcb65c6f8494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "from math import floor\n",
    "\n",
    "\n",
    "class VisualQNetwork(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: Tuple[int], \n",
    "        encoding_size: int, \n",
    "        output_size: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Creates a neural network that takes as input a batch of images (3\n",
    "        dimensional tensors) and outputs a batch of outputs (1 dimensional\n",
    "        tensors)\n",
    "        \"\"\"\n",
    "        super(VisualQNetwork, self).__init__()\n",
    "        self.dense1 = torch.nn.Linear(input_shape[0], encoding_size)\n",
    "        self.dense2 = torch.nn.Linear(encoding_size, encoding_size)\n",
    "\n",
    "        self.dense2_x1 = torch.nn.Linear(encoding_size, output_size)\n",
    "        self.dense2_x2 = torch.nn.Linear(encoding_size, output_size)\n",
    "        self.dense2_x3 = torch.nn.Linear(encoding_size, output_size)\n",
    "\n",
    "        self.act = torch.nn.Sigmoid() # ReLU\n",
    "\n",
    "    def forward(self, visual_obs: torch.tensor):\n",
    "        hidden = self.dense1(visual_obs)\n",
    "        hidden = self.act(hidden)\n",
    "        hidden = self.dense2(hidden)\n",
    "        hidden = self.act(hidden)\n",
    "        x1 = self.dense2_x1(hidden)\n",
    "        x2 = self.dense2_x2(hidden)\n",
    "        x3 = self.dense2_x3(hidden)\n",
    "\n",
    "        #     x1 = self.act(x1)\n",
    "        #     x2 = self.act(x2)\n",
    "        #     x3 = self.act(x3)\n",
    "\n",
    "        return x1, x2, x3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1edc74-99d3-4177-8d78-1db24cb7fa77",
   "metadata": {},
   "source": [
    "# üëü Define Training Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e807cbd-e6f9-4c6e-aceb-811495f82bdc",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Step 3. Track gradients with `wandb.watch` and everything else with `wandb.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6bdd68cb-68dd-44d6-9d7c-03f022136d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure\n",
    "def train(model, env, criterion, optimizer, config):\n",
    "#     wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "    behavior_name = list(env.behavior_specs)[0]\n",
    "        \n",
    "    for episode in range(config.NUM_TRAINING_STEPS):\n",
    "        env.reset()\n",
    "        decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "        tracked_agent = -1 # -1 indicates not yet tracking\n",
    "        done = False # For the tracked_agent\n",
    "        episode_rewards = 0 # For the tracked_agent\n",
    "        \n",
    "        while not done:\n",
    "            # Track the first agent we see if not tracking \n",
    "            # Note : len(decision_steps) = [numberb of agents that requested a decision]\n",
    "            if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                tracked_agent = decision_steps.agent_id[0] \n",
    "\n",
    "            # Generate an action for all agents\n",
    "            # these are the observations\n",
    "#             print(decision_steps[0])\n",
    "            action = model.get_action(decision_steps)\n",
    "    #         print(action.discrete)\n",
    "            # Set the actions\n",
    "            env.set_actions(behavior_name, action)\n",
    "\n",
    "            # Move the simulation forward\n",
    "            env.step()\n",
    "\n",
    "            # Get the new simulation results\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            if tracked_agent in decision_steps: # The agent requested a decision\n",
    "                episode_rewards += decision_steps[tracked_agent].reward\n",
    "            if tracked_agent in terminal_steps: # The agent terminated its episode\n",
    "                print(\"reward on terminal step:\", terminal_steps[tracked_agent].reward)\n",
    "                episode_rewards += terminal_steps[tracked_agent].reward\n",
    "                done = True\n",
    "        # print(\"Training step \", episode + 1, \"\\treward \", episode_rewards)\n",
    "        train_log(episode_rewards, episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "af41c215-fdab-4a17-a306-dee1b85f5a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_log(reward, episode):\n",
    "#     loss = float(loss)\n",
    "    reward = float(reward)\n",
    "\n",
    "    # where the magic happens\n",
    "    wandb.log({\"episode\": episode, \"reward\": reward}) #, step=example_ct\n",
    "    print(f\"Reward after \" + str(episode).zfill(5) + f\" episodes: {reward:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13b8c33-5d2d-457c-9abb-9361e273f50f",
   "metadata": {},
   "source": [
    "# üß™ Define Testing Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b01425d-94c6-4573-9f9b-7f502b615a97",
   "metadata": {},
   "source": [
    "#### 4Ô∏è‚É£ Optional Step 4: Call `wandb.save`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "08a9a3d0-7baf-4a78-900c-64b47e0b3d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, env, config):\n",
    "#     model.eval()\n",
    "\n",
    "    # Run the model on some test examples\n",
    "    with torch.no_grad():\n",
    "        behavior_name = list(env.behavior_specs)[0]\n",
    "        cumulative_rewards: List[float] = []\n",
    "\n",
    "        for episode in range(config.NUM_TEST_STEPS):\n",
    "            env.reset()\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            tracked_agent = -1 # -1 indicates not yet tracking\n",
    "            done = False # For the tracked_agent\n",
    "            episode_rewards = 0 # For the tracked_agent\n",
    "\n",
    "            while not done:\n",
    "                # Track the first agent we see if not tracking \n",
    "                # Note : len(decision_steps) = [number of agents that requested a decision]\n",
    "                if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                    tracked_agent = decision_steps.agent_id[0] \n",
    "\n",
    "                # Generate an action for all agents\n",
    "                action = model.get_action(decision_steps)\n",
    "\n",
    "                # Set the actions\n",
    "                env.set_actions(behavior_name, action)\n",
    "\n",
    "                # Move the simulation forward\n",
    "                env.step()\n",
    "\n",
    "                # Get the new simulation results\n",
    "                decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "                if tracked_agent in decision_steps: # The agent requested a decision\n",
    "                    episode_rewards += decision_steps[tracked_agent].reward\n",
    "                if tracked_agent in terminal_steps: # The agent terminated its episode\n",
    "                    print(\"reward on terminal step:\", terminal_steps[tracked_agent].reward)\n",
    "                    episode_rewards += terminal_steps[tracked_agent].reward\n",
    "                    done = True\n",
    "            cumulative_rewards.append(episode_rewards)\n",
    "\n",
    "        print(f\"Average reward of the model after {config.NUM_TEST_STEPS} \" +\n",
    "              f\"test episodes: {numpy.average(cumulative_rewards)}%\")\n",
    "\n",
    "        wandb.log({\"test_average_reward\": numpy.average(cumulative_rewards)})\n",
    "\n",
    "    # Save the model in the exchangeable ONNX format\n",
    "    torch.onnx.export(model, [], \"model.onnx\")\n",
    "    wandb.save(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e3150d-5d2b-48dd-a229-b9d9f8eb8b7f",
   "metadata": {},
   "source": [
    "# üèÉ‚Äç‚ôÄÔ∏è Run training and watch your metrics live on [wandb.ai](https://wandb.ai)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "41cb9620-5a48-4fc2-9f2c-422623703720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">fanciful-morning-24</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ademord/pytorch-demo1\" target=\"_blank\">https://wandb.ai/ademord/pytorch-demo1</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ademord/pytorch-demo1/runs/28ab0ac6\" target=\"_blank\">https://wandb.ai/ademord/pytorch-demo1/runs/28ab0ac6</a><br/>\n",
       "                Run data is saved locally in <code>/host/unity_builds/run32_training/wandb/run-20210604_162636-28ab0ac6</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment created.\n",
      "Name of the behavior : Hummingbird?team=0\n",
      "Type of the spec : BehaviorSpec(observation_specs=[ObservationSpec(shape=(44,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='RayPerceptionSensor'), ObservationSpec(shape=(3,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='StackingSensor_size3_VectorSensor_size1')], action_spec=ActionSpec(continuous_size=0, discrete_branches=(3, 3, 3)))\n",
      "<__main__.Agent object at 0x7ff034696670>\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00000 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00001 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00002 episodes: 185.935\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00003 episodes: 100.940\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00004 episodes: 130.720\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00005 episodes: -1.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00006 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00007 episodes: 57.887\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00008 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00009 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00010 episodes: 6.887\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00011 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00012 episodes: 46.824\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00013 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00014 episodes: 23.661\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00015 episodes: 195.389\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00016 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00017 episodes: 13.896\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00018 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00019 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00020 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00021 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00022 episodes: 262.123\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00023 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00024 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00025 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00026 episodes: 46.483\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00027 episodes: 315.920\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00028 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00029 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00030 episodes: 194.097\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00031 episodes: 38.106\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00032 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00033 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00034 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00035 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00036 episodes: 131.054\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00037 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00038 episodes: 191.129\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00039 episodes: 159.846\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00040 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00041 episodes: 100.766\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00042 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00043 episodes: 11.208\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00044 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00045 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00046 episodes: 336.440\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00047 episodes: 36.067\n",
      "Total execution time: 145.3019 minutes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 13217<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/host/unity_builds/run32_training/wandb/run-20210604_162636-28ab0ac6/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/host/unity_builds/run32_training/wandb/run-20210604_162636-28ab0ac6/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>episode</td><td>47</td></tr><tr><td>reward</td><td>36.06682</td></tr><tr><td>_runtime</td><td>8576</td></tr><tr><td>_timestamp</td><td>1622832572</td></tr><tr><td>_step</td><td>47</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>episode</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>reward</td><td>‚ñÅ‚ñÅ‚ñÖ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñÇ‚ñà‚ñÅ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÖ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ</td></tr><tr><td>_runtime</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>_timestamp</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">fanciful-morning-24</strong>: <a href=\"https://wandb.ai/ademord/pytorch-demo1/runs/28ab0ac6\" target=\"_blank\">https://wandb.ai/ademord/pytorch-demo1/runs/28ab0ac6</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-182-318ae2ededa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build, train and analyze the model with the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-174-e11be425bb88>\u001b[0m in \u001b[0;36mmodel_pipeline\u001b[0;34m(hyperparameters)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# and use them to train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# and test its final performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-153-2872183460f3>\u001b[0m in \u001b[0;36m_time_it\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mend_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-179-e194e3bd004a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, env, criterion, optimizer, config)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# Move the simulation forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# Get the new simulation results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/mlagents_envs/timers.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/mlagents_envs/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mstep_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_step_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"communicator.exchange\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexchange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll_process\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUnityCommunicatorStoppedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Communicator has exited.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/mlagents_envs/rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[0;34m(self, inputs, poll_callback)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll_for_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/mlagents_envs/rpc_communicator.py\u001b[0m in \u001b[0;36mpoll_for_timeout\u001b[0;34m(self, poll_callback)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mcallback_timeout_wait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout_wait\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback_timeout_wait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0;31m# Got an acknowledgment from the connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Build, train and analyze the model with the pipeline\n",
    "model, env = model_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0a637147-249e-43f9-b50d-ad3c662b0afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ce5513-e1eb-453b-a0b4-689330842816",
   "metadata": {},
   "source": [
    "# üßπ Test Hyperparameters with Sweeps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13526307-58d0-4263-8b59-714525a51117",
   "metadata": {},
   "source": [
    "## [Check out Hyperparameter Optimization in PyTorch using W&B Sweep $\\rightarrow$](https://colab.research.google.com/drive/1QTIK23LBuAkdejbrvdP5hwBGyYlyEJpT?usp=sharing)\n",
    "\n",
    "Running a hyperparameter sweep with Weights & Biases is very easy. There are just 3 simple steps:\n",
    "\n",
    "1. **Define the sweep:** We do this by creating a dictionary or a [YAML file](https://docs.wandb.com/library/sweeps/configuration) that specifies the parameters to search through, the search strategy, the optimization metric et all.\n",
    "\n",
    "2. **Initialize the sweep:** \n",
    "`sweep_id = wandb.sweep(sweep_config)`\n",
    "\n",
    "3. **Run the sweep agent:** \n",
    "`wandb.agent(sweep_id, function=train)`\n",
    "\n",
    "And voila! That's all there is to running a hyperparameter sweep!\n",
    "<img src=\"https://imgur.com/UiQKg0L.png\" alt=\"Weights & Biases\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215b498a-9711-46b3-afc7-4cd3742df753",
   "metadata": {},
   "source": [
    "# ü§ì Advanced Setup\n",
    "1. [Environment variables](https://docs.wandb.com/library/environment-variables): Set API keys in environment variables so you can run training on a managed cluster.\n",
    "2. [Offline mode](https://docs.wandb.com/library/technical-faq#can-i-run-wandb-offline): Use `dryrun` mode to train offline and sync results later.\n",
    "3. [On-prem](https://docs.wandb.com/self-hosted): Install W&B in a private cloud or air-gapped servers in your own infrastructure. We have local installations for everyone from academics to enterprise teams.\n",
    "4. [Sweeps](https://docs.wandb.com/sweeps): Set up hyperparameter search quickly with our lightweight tool for tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9360baa-2b59-4cb6-86f9-8dca4deb31f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n in range(NUM_TRAINING_STEPS):\n",
    "#   new_exp,_ = Trainer.generate_trajectories(env, qnet, NUM_NEW_EXP, epsilon=0.1)\n",
    "#   random.shuffle(experiences)\n",
    "#   if len(experiences) > BUFFER_SIZE:\n",
    "#     experiences = experiences[:BUFFER_SIZE]\n",
    "#   experiences.extend(new_exp)\n",
    "#   Trainer.update_q_net(qnet, optim, experiences, 3)\n",
    "#   _, rewards = Trainer.generate_trajectories(env, qnet, 100, epsilon=0)\n",
    "#   cumulative_rewards.append(rewards)\n",
    "#   print(\"Training step \", n+1, \"\\treward \", rewards)\n",
    "#   print()\n",
    "\n",
    "\n",
    "# env.close()\n",
    "\n",
    "# # Show the training graph\n",
    "# plt.plot(range(NUM_TRAINING_STEPS), cumulative_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f84bef9-4e0b-4829-98f6-9d5548a5b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  class UnityEnv(gym.Env):\n",
    "#     \"\"\"\n",
    "#     Provides Gym wrapper for Unity Learning Environments.\n",
    "#     Multi-agent environments use lists for object types, as done here:\n",
    "#     https://github.com/openai/multiagent-particle-envs\n",
    "#     \"\"\"\n",
    " \n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         environment_filename: str,\n",
    "#         dimensions: int = [],   #Added\n",
    "#         timescale: int = 1,     #Added\n",
    "#         worker_id: int = 0,\n",
    "#         use_visual: bool = False,\n",
    "#         uint8_visual: bool = False,\n",
    "#         multiagent: bool = False,\n",
    "#         flatten_branched: bool = False,\n",
    "#         no_graphics: bool = False,\n",
    "#         allow_multiple_visual_obs: bool = False,\n",
    "#         set_config: bool = True,    #Added\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Environment initialization\n",
    "#         :param environment_filename: The UnityEnvironment path or file to be wrapped in the gym.\n",
    "#         :param worker_id: Worker number for environment.\n",
    "#         :param use_visual: Whether to use visual observation or vector observation.\n",
    "#         :param uint8_visual: Return visual observations as uint8 (0-255) matrices instead of float (0.0-1.0).\n",
    "#         :param multiagent: Whether to run in multi-agent mode (lists of obs, reward, done).\n",
    "#         :param flatten_branched: If True, turn branched discrete action spaces into a Discrete space rather than\n",
    "#             MultiDiscrete.\n",
    "#         :param no_graphics: Whether to run the Unity simulator in no-graphics mode\n",
    "#         :param allow_multiple_visual_obs: If True, return a list of visual observations instead of only one.\n",
    "#         \"\"\"\n",
    "#         base_port = 5005\n",
    "#         if environment_filename is None:\n",
    "#             base_port = UnityEnvironment.DEFAULT_EDITOR_PORT\n",
    " \n",
    "#         channel = EngineConfigurationChannel()        # Added\n",
    " \n",
    " \n",
    "#         #Added\n",
    "#         if set_config == True:\n",
    "#             channel.set_configuration_parameters(time_scale=timescale, width=dimensions[0], height=dimensions[1])\n",
    "#         #Added\n",
    " \n",
    "#         self._env = UnityEnvironment(\n",
    "#             environment_filename,\n",
    "#             worker_id,\n",
    "#             base_port=base_port,\n",
    "#             no_graphics=no_graphics,\n",
    "#             side_channels=[channel],        # Added\n",
    "#         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
