{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f9f89eb-2d35-426d-ae3e-bba27489dd3d",
   "metadata": {},
   "source": [
    "# ğŸš€ Install, Import, and Log In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e55fe183-657a-4323-8641-7655c8a8f7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import random\n",
    "from functools import wraps\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1e49908-fed1-47a2-89b0-829c1e527d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(func):\n",
    "    @wraps(func)\n",
    "    def _time_it(*args, **kwargs):\n",
    "        start = int(round(time() * 1000))\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        finally:\n",
    "            end_ = int(round(time() * 1000)) - start\n",
    "            print(f\"Total execution time: {end_/(1000*60) if end_ > 0 else 0} minutes\")\n",
    "    return _time_it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfbd7d2-c9c7-41f8-b272-178742073096",
   "metadata": {},
   "source": [
    "### 0ï¸âƒ£ Step 0: Install W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8143d7fa-840c-4f68-94b9-f39d8f0775f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec61dda-606d-45c0-bbd6-6193665559b1",
   "metadata": {},
   "source": [
    "### 1ï¸âƒ£ Step 1: Import W&B and Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef585afd-2998-4b9c-b4e9-65454d72e3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mademord\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72848fd9-bcdb-486b-83a5-13abf7e1bfb7",
   "metadata": {},
   "source": [
    "# ğŸ‘©â€ğŸ”¬ Define the Experiment and Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5314474d-5562-4e65-bf6b-7cad3756473e",
   "metadata": {},
   "source": [
    "### 2ï¸âƒ£ Step 2: Track metadata and hyperparameters with `wandb.init`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa146a4e-9987-4728-9e73-afc9d8e6eec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    epochs=5,\n",
    "    classes=10,\n",
    "    kernels=[16, 32],\n",
    "    batch_size=128,\n",
    "    learning_rate=0.005,\n",
    "    dataset=\"MNIST\",\n",
    "    architecture=\"CNN\",\n",
    "    NUM_TRAINING_STEPS = 100000, #10000000\n",
    "    NUM_TEST_STEPS = 10,\n",
    "    NUM_NEW_EXP = 1000,\n",
    "    BUFFER_SIZE = 10000,\n",
    "    worker_id=4,\n",
    "    time_scale=20,\n",
    "    no_graphics = True\n",
    "    )\n",
    "env = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2308ec7-05f9-4f7d-8ea9-f6c1170d4c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters):\n",
    "    global env\n",
    "    \n",
    "    try:\n",
    "        env.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"pytorch-demo1\", config=hyperparameters):\n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "\n",
    "        # make the model, data, and optimization problem\n",
    "        model, env, criterion, optimizer = make(config)\n",
    "        print(model)\n",
    "\n",
    "        # and use them to train the model\n",
    "        train(model, env, criterion, optimizer, config)\n",
    "\n",
    "        # and test its final performance\n",
    "        test(model, env, config)\n",
    "\n",
    "        env.close()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bb60f26-0808-4f56-b6ef-0cc41c3033f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make(config):\n",
    "    # Make the data\n",
    "    env, spec = make_env(config)\n",
    "    \n",
    "    # Make the model\n",
    "    # model = Agent(config.kernels, config.classes).to(device)\n",
    "    model = Agent(spec)\n",
    "\n",
    "    # Make the loss and optimizer\n",
    "    criterion = None # nn.CrossEntropyLoss()\n",
    "    optimizer = None # torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    \n",
    "    return model, env, criterion, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a15c7c2-720c-4c1e-b005-0707e9b16a9c",
   "metadata": {},
   "source": [
    "# ğŸ“¡ Define the Env Loading and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb6b9020-dcf3-4ffc-80a5-5217d0fe5937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(config):\n",
    "    channel = EngineConfigurationChannel()\n",
    "    env = UE(\"run32_training\", seed=1, worker_id=config.worker_id, no_graphics=config.no_graphics, side_channels=[channel])\n",
    "    channel.set_configuration_parameters(time_scale = config.time_scale)\n",
    "    print(\"Environment created.\")\n",
    "    \n",
    "    env.reset()\n",
    "    behavior_name = list(env.behavior_specs)[0]\n",
    "    print(f\"Name of the behavior : {behavior_name}\")\n",
    "    \n",
    "    spec = env.behavior_specs[behavior_name]\n",
    "    print(f\"Type of the spec : {spec}\")\n",
    "    \n",
    "    return env, spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dc6a727-901a-4175-b712-a1d6364ec1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conventional and convolutional neural network\n",
    "class Agent():\n",
    "    def __init__(self, spec):\n",
    "        self.spec = spec\n",
    "    def get_action(self, decision_steps):\n",
    "        return self.spec.action_spec.random_action(len(decision_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf7a5bde-932f-4d80-a530-18fbd5346952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "from math import floor\n",
    "\n",
    "\n",
    "class VisualQNetwork(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: Tuple[int], \n",
    "        encoding_size: int, \n",
    "        output_size: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Creates a neural network that takes as input a batch of images (3\n",
    "        dimensional tensors) and outputs a batch of outputs (1 dimensional\n",
    "        tensors)\n",
    "        \"\"\"\n",
    "        super(VisualQNetwork, self).__init__()\n",
    "        self.dense1 = torch.nn.Linear(input_shape[0], encoding_size)\n",
    "        self.dense2 = torch.nn.Linear(encoding_size, encoding_size)\n",
    "\n",
    "        self.dense2_x1 = torch.nn.Linear(encoding_size, output_size)\n",
    "        self.dense2_x2 = torch.nn.Linear(encoding_size, output_size)\n",
    "        self.dense2_x3 = torch.nn.Linear(encoding_size, output_size)\n",
    "\n",
    "        self.act = torch.nn.Sigmoid() # ReLU\n",
    "\n",
    "    def forward(self, visual_obs: torch.tensor):\n",
    "        hidden = self.dense1(visual_obs)\n",
    "        hidden = self.act(hidden)\n",
    "        hidden = self.dense2(hidden)\n",
    "        hidden = self.act(hidden)\n",
    "        x1 = self.dense2_x1(hidden)\n",
    "        x2 = self.dense2_x2(hidden)\n",
    "        x3 = self.dense2_x3(hidden)\n",
    "\n",
    "        #     x1 = self.act(x1)\n",
    "        #     x2 = self.act(x2)\n",
    "        #     x3 = self.act(x3)\n",
    "\n",
    "        return x1, x2, x3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1edc74-99d3-4177-8d78-1db24cb7fa77",
   "metadata": {},
   "source": [
    "# ğŸ‘Ÿ Define Training Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e807cbd-e6f9-4c6e-aceb-811495f82bdc",
   "metadata": {},
   "source": [
    "### 3ï¸âƒ£ Step 3. Track gradients with `wandb.watch` and everything else with `wandb.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bdd68cb-68dd-44d6-9d7c-03f022136d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure\n",
    "def train(model, env, criterion, optimizer, config):\n",
    "#     wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "    behavior_name = list(env.behavior_specs)[0]\n",
    "        \n",
    "    for episode in range(config.NUM_TRAINING_STEPS):\n",
    "        env.reset()\n",
    "        decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "        tracked_agent = -1 # -1 indicates not yet tracking\n",
    "        done = False # For the tracked_agent\n",
    "        episode_rewards = 0 # For the tracked_agent\n",
    "        \n",
    "        while not done:\n",
    "            # Track the first agent we see if not tracking \n",
    "            # Note : len(decision_steps) = [numberb of agents that requested a decision]\n",
    "            if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                tracked_agent = decision_steps.agent_id[0] \n",
    "\n",
    "            # Generate an action for all agents\n",
    "            # these are the observations\n",
    "#             print(decision_steps[0])\n",
    "            action = model.get_action(decision_steps)\n",
    "    #         print(action.discrete)\n",
    "            # Set the actions\n",
    "            env.set_actions(behavior_name, action)\n",
    "\n",
    "            # Move the simulation forward\n",
    "            env.step()\n",
    "\n",
    "            # Get the new simulation results\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            if tracked_agent in decision_steps: # The agent requested a decision\n",
    "                episode_rewards += decision_steps[tracked_agent].reward\n",
    "            if tracked_agent in terminal_steps: # The agent terminated its episode\n",
    "                print(\"reward on terminal step:\", terminal_steps[tracked_agent].reward)\n",
    "                episode_rewards += terminal_steps[tracked_agent].reward\n",
    "                done = True\n",
    "        # print(\"Training step \", episode + 1, \"\\treward \", episode_rewards)\n",
    "        train_log(episode_rewards, episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af41c215-fdab-4a17-a306-dee1b85f5a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_log(reward, episode):\n",
    "#     loss = float(loss)\n",
    "    reward = float(reward)\n",
    "    episode +=1\n",
    "    \n",
    "    # where the magic happens\n",
    "    wandb.log({\"episode\": episode, \"reward\": reward}) #, step=example_ct\n",
    "    print(f\"Reward after \" + str(episode).zfill(5) + f\" episodes: {reward:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13b8c33-5d2d-457c-9abb-9361e273f50f",
   "metadata": {},
   "source": [
    "# ğŸ§ª Define Testing Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b01425d-94c6-4573-9f9b-7f502b615a97",
   "metadata": {},
   "source": [
    "#### 4ï¸âƒ£ Optional Step 4: Call `wandb.save`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08a9a3d0-7baf-4a78-900c-64b47e0b3d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, env, config):\n",
    "#     model.eval()\n",
    "\n",
    "    # Run the model on some test examples\n",
    "    with torch.no_grad():\n",
    "        behavior_name = list(env.behavior_specs)[0]\n",
    "        cumulative_rewards: List[float] = []\n",
    "\n",
    "        for episode in range(config.NUM_TEST_STEPS):\n",
    "            env.reset()\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            tracked_agent = -1 # -1 indicates not yet tracking\n",
    "            done = False # For the tracked_agent\n",
    "            episode_rewards = 0 # For the tracked_agent\n",
    "\n",
    "            while not done:\n",
    "                # Track the first agent we see if not tracking \n",
    "                # Note : len(decision_steps) = [number of agents that requested a decision]\n",
    "                if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                    tracked_agent = decision_steps.agent_id[0] \n",
    "\n",
    "                # Generate an action for all agents\n",
    "                action = model.get_action(decision_steps)\n",
    "\n",
    "                # Set the actions\n",
    "                env.set_actions(behavior_name, action)\n",
    "\n",
    "                # Move the simulation forward\n",
    "                env.step()\n",
    "\n",
    "                # Get the new simulation results\n",
    "                decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "                if tracked_agent in decision_steps: # The agent requested a decision\n",
    "                    episode_rewards += decision_steps[tracked_agent].reward\n",
    "                if tracked_agent in terminal_steps: # The agent terminated its episode\n",
    "                    print(\"reward on terminal step:\", terminal_steps[tracked_agent].reward)\n",
    "                    episode_rewards += terminal_steps[tracked_agent].reward\n",
    "                    done = True\n",
    "            cumulative_rewards.append(episode_rewards)\n",
    "\n",
    "        print(f\"Average reward of the model after {config.NUM_TEST_STEPS} \" +\n",
    "              f\"test episodes: {numpy.average(cumulative_rewards)}%\")\n",
    "\n",
    "        wandb.log({\"test_average_reward\": numpy.average(cumulative_rewards)})\n",
    "\n",
    "    # Save the model in the exchangeable ONNX format\n",
    "    torch.onnx.export(model, [], \"model.onnx\")\n",
    "    wandb.save(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e3150d-5d2b-48dd-a229-b9d9f8eb8b7f",
   "metadata": {},
   "source": [
    "# ğŸƒâ€â™€ï¸ Run training and watch your metrics live on [wandb.ai](https://wandb.ai)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41cb9620-5a48-4fc2-9f2c-422623703720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">good-cherry-25</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ademord/pytorch-demo1\" target=\"_blank\">https://wandb.ai/ademord/pytorch-demo1</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ademord/pytorch-demo1/runs/1bf9a6oj\" target=\"_blank\">https://wandb.ai/ademord/pytorch-demo1/runs/1bf9a6oj</a><br/>\n",
       "                Run data is saved locally in <code>/host/unity_builds/run32_training/wandb/run-20210604_185334-1bf9a6oj</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment created.\n",
      "Name of the behavior : Hummingbird?team=0\n",
      "Type of the spec : BehaviorSpec(observation_specs=[ObservationSpec(shape=(44,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='RayPerceptionSensor'), ObservationSpec(shape=(3,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='StackingSensor_size3_VectorSensor_size1')], action_spec=ActionSpec(continuous_size=0, discrete_branches=(3, 3, 3)))\n",
      "<__main__.Agent object at 0x7f26a05b19a0>\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00001 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00002 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00003 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00004 episodes: -1.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00005 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00006 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00007 episodes: 126.473\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00008 episodes: 192.373\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00009 episodes: 109.199\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00010 episodes: 131.809\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00011 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00012 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00013 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00014 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00015 episodes: 189.375\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00016 episodes: 282.061\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00017 episodes: 358.273\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00018 episodes: 233.284\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00019 episodes: 356.032\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00020 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00021 episodes: 180.322\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00022 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00023 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00024 episodes: 200.533\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00025 episodes: 308.437\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00026 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00027 episodes: 181.599\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00028 episodes: 23.785\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00029 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00030 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00031 episodes: 143.207\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00032 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00033 episodes: 178.710\n",
      "reward on terminal step: 0.9579929\n",
      "Reward after 00034 episodes: 198.230\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00035 episodes: 196.724\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00036 episodes: 113.054\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00037 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00038 episodes: 185.156\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00039 episodes: 124.559\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00040 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00041 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00042 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00043 episodes: 36.442\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00044 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00045 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00046 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00047 episodes: 376.800\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00048 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00049 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00050 episodes: 311.007\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00051 episodes: 87.161\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00052 episodes: 150.966\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00053 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00054 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00055 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00056 episodes: 57.943\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00057 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00058 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00059 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00060 episodes: 109.350\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00061 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00062 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00063 episodes: 4.989\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00064 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00065 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00066 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00067 episodes: 27.411\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00068 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00069 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00070 episodes: -1.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00071 episodes: 228.783\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00072 episodes: 166.938\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00073 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00074 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00075 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00076 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00077 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00078 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00079 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00080 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00081 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00082 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00083 episodes: 63.753\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00084 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00085 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00086 episodes: 192.649\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00087 episodes: 9.421\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00088 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00089 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00090 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00091 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00092 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00093 episodes: 219.628\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00094 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00095 episodes: 18.472\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00096 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00097 episodes: 185.501\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00098 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00099 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00100 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00101 episodes: 12.491\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00102 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00103 episodes: 171.523\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00104 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00105 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00106 episodes: 18.895\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00107 episodes: 164.617\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00108 episodes: 120.699\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00109 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00110 episodes: 209.040\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00111 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00112 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00113 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00114 episodes: 194.494\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00115 episodes: 225.752\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00116 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00117 episodes: 182.454\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00118 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00119 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00120 episodes: 193.189\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00121 episodes: 170.176\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00122 episodes: 172.017\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00123 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00124 episodes: 17.998\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00125 episodes: 25.695\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00126 episodes: 83.874\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00127 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00128 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00129 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00130 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00131 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00132 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00133 episodes: 332.846\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00134 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00135 episodes: 0.000\n",
      "reward on terminal step: 2.8547566\n",
      "Reward after 00136 episodes: 3.776\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00137 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00138 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00139 episodes: 6.053\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00140 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00141 episodes: -1.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00142 episodes: 0.000\n",
      "reward on terminal step: 1.719106\n",
      "Reward after 00143 episodes: 6.858\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00144 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00145 episodes: 10.938\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00146 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00147 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00148 episodes: 171.243\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00149 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00150 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00151 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00152 episodes: 53.857\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00153 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00154 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00155 episodes: 189.784\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00156 episodes: 221.377\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00157 episodes: 38.952\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00158 episodes: 149.844\n",
      "reward on terminal step: -1.0\n",
      "Reward after 00159 episodes: -1.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00160 episodes: 144.986\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00161 episodes: 101.414\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00162 episodes: 4.926\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00163 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00164 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00165 episodes: 85.619\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00166 episodes: 45.963\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00167 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00168 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00169 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00170 episodes: 193.582\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00171 episodes: 7.299\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00172 episodes: 152.173\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00173 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00174 episodes: 41.173\n",
      "reward on terminal step: 0.0\n",
      "Reward after 00175 episodes: 0.000\n",
      "Total execution time: 86.61113333333333 minutes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 14184<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/host/unity_builds/run32_training/wandb/run-20210604_185334-1bf9a6oj/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/host/unity_builds/run32_training/wandb/run-20210604_185334-1bf9a6oj/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>episode</td><td>175</td></tr><tr><td>reward</td><td>0.0</td></tr><tr><td>_runtime</td><td>5183</td></tr><tr><td>_timestamp</td><td>1622837997</td></tr><tr><td>_step</td><td>174</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>episode</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>reward</td><td>â–â–â–„â–â–†â–†â–‚â–â–„â–â–â–ˆâ–â–â–â–â–…â–â–â–â–â–â–â–â–â–â–â–…â–â–â–â–â–â–â–â–†â–„â–â–â–‚</td></tr><tr><td>_runtime</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>_timestamp</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>_step</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">good-cherry-25</strong>: <a href=\"https://wandb.ai/ademord/pytorch-demo1/runs/1bf9a6oj\" target=\"_blank\">https://wandb.ai/ademord/pytorch-demo1/runs/1bf9a6oj</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-318ae2ededa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build, train and analyze the model with the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-e11be425bb88>\u001b[0m in \u001b[0;36mmodel_pipeline\u001b[0;34m(hyperparameters)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# and use them to train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# and test its final performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-2872183460f3>\u001b[0m in \u001b[0;36m_time_it\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mend_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-43ae36343050>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, env, criterion, optimizer, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# Move the simulation forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# Get the new simulation results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/mlagents_envs/timers.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/mlagents_envs/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mgroup_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 ].action_spec.empty_action(n_agents)\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0mstep_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_step_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"communicator.exchange\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexchange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll_process\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/mlagents_envs/timers.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/mlagents_envs/environment.py\u001b[0m in \u001b[0;36m_generate_step_input\u001b[0;34m(self, vector_action)\u001b[0m\n\u001b[1;32m    454\u001b[0m                         \u001b[0mvector_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscrete\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                     )\n\u001b[0;32m--> 456\u001b[0;31m                     \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscrete_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscrete\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m                 \u001b[0mrl_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                 \u001b[0mrl_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSTEP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/mlagents_envs/base_env.py\u001b[0m in \u001b[0;36mdiscrete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_continuous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdiscrete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_discrete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Build, train and analyze the model with the pipeline\n",
    "model, env = model_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13d8b4c2-9262-490f-b150-a956d2d946d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ce5513-e1eb-453b-a0b4-689330842816",
   "metadata": {},
   "source": [
    "# ğŸ§¹ Test Hyperparameters with Sweeps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13526307-58d0-4263-8b59-714525a51117",
   "metadata": {},
   "source": [
    "## [Check out Hyperparameter Optimization in PyTorch using W&B Sweep $\\rightarrow$](https://colab.research.google.com/drive/1QTIK23LBuAkdejbrvdP5hwBGyYlyEJpT?usp=sharing)\n",
    "\n",
    "Running a hyperparameter sweep with Weights & Biases is very easy. There are just 3 simple steps:\n",
    "\n",
    "1. **Define the sweep:** We do this by creating a dictionary or a [YAML file](https://docs.wandb.com/library/sweeps/configuration) that specifies the parameters to search through, the search strategy, the optimization metric et all.\n",
    "\n",
    "2. **Initialize the sweep:** \n",
    "`sweep_id = wandb.sweep(sweep_config)`\n",
    "\n",
    "3. **Run the sweep agent:** \n",
    "`wandb.agent(sweep_id, function=train)`\n",
    "\n",
    "And voila! That's all there is to running a hyperparameter sweep!\n",
    "<img src=\"https://imgur.com/UiQKg0L.png\" alt=\"Weights & Biases\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215b498a-9711-46b3-afc7-4cd3742df753",
   "metadata": {},
   "source": [
    "# ğŸ¤“ Advanced Setup\n",
    "1. [Environment variables](https://docs.wandb.com/library/environment-variables): Set API keys in environment variables so you can run training on a managed cluster.\n",
    "2. [Offline mode](https://docs.wandb.com/library/technical-faq#can-i-run-wandb-offline): Use `dryrun` mode to train offline and sync results later.\n",
    "3. [On-prem](https://docs.wandb.com/self-hosted): Install W&B in a private cloud or air-gapped servers in your own infrastructure. We have local installations for everyone from academics to enterprise teams.\n",
    "4. [Sweeps](https://docs.wandb.com/sweeps): Set up hyperparameter search quickly with our lightweight tool for tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9360baa-2b59-4cb6-86f9-8dca4deb31f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n in range(NUM_TRAINING_STEPS):\n",
    "#   new_exp,_ = Trainer.generate_trajectories(env, qnet, NUM_NEW_EXP, epsilon=0.1)\n",
    "#   random.shuffle(experiences)\n",
    "#   if len(experiences) > BUFFER_SIZE:\n",
    "#     experiences = experiences[:BUFFER_SIZE]\n",
    "#   experiences.extend(new_exp)\n",
    "#   Trainer.update_q_net(qnet, optim, experiences, 3)\n",
    "#   _, rewards = Trainer.generate_trajectories(env, qnet, 100, epsilon=0)\n",
    "#   cumulative_rewards.append(rewards)\n",
    "#   print(\"Training step \", n+1, \"\\treward \", rewards)\n",
    "#   print()\n",
    "\n",
    "\n",
    "# env.close()\n",
    "\n",
    "# # Show the training graph\n",
    "# plt.plot(range(NUM_TRAINING_STEPS), cumulative_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f84bef9-4e0b-4829-98f6-9d5548a5b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  class UnityEnv(gym.Env):\n",
    "#     \"\"\"\n",
    "#     Provides Gym wrapper for Unity Learning Environments.\n",
    "#     Multi-agent environments use lists for object types, as done here:\n",
    "#     https://github.com/openai/multiagent-particle-envs\n",
    "#     \"\"\"\n",
    " \n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         environment_filename: str,\n",
    "#         dimensions: int = [],   #Added\n",
    "#         timescale: int = 1,     #Added\n",
    "#         worker_id: int = 0,\n",
    "#         use_visual: bool = False,\n",
    "#         uint8_visual: bool = False,\n",
    "#         multiagent: bool = False,\n",
    "#         flatten_branched: bool = False,\n",
    "#         no_graphics: bool = False,\n",
    "#         allow_multiple_visual_obs: bool = False,\n",
    "#         set_config: bool = True,    #Added\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Environment initialization\n",
    "#         :param environment_filename: The UnityEnvironment path or file to be wrapped in the gym.\n",
    "#         :param worker_id: Worker number for environment.\n",
    "#         :param use_visual: Whether to use visual observation or vector observation.\n",
    "#         :param uint8_visual: Return visual observations as uint8 (0-255) matrices instead of float (0.0-1.0).\n",
    "#         :param multiagent: Whether to run in multi-agent mode (lists of obs, reward, done).\n",
    "#         :param flatten_branched: If True, turn branched discrete action spaces into a Discrete space rather than\n",
    "#             MultiDiscrete.\n",
    "#         :param no_graphics: Whether to run the Unity simulator in no-graphics mode\n",
    "#         :param allow_multiple_visual_obs: If True, return a list of visual observations instead of only one.\n",
    "#         \"\"\"\n",
    "#         base_port = 5005\n",
    "#         if environment_filename is None:\n",
    "#             base_port = UnityEnvironment.DEFAULT_EDITOR_PORT\n",
    " \n",
    "#         channel = EngineConfigurationChannel()        # Added\n",
    " \n",
    " \n",
    "#         #Added\n",
    "#         if set_config == True:\n",
    "#             channel.set_configuration_parameters(time_scale=timescale, width=dimensions[0], height=dimensions[1])\n",
    "#         #Added\n",
    " \n",
    "#         self._env = UnityEnvironment(\n",
    "#             environment_filename,\n",
    "#             worker_id,\n",
    "#             base_port=base_port,\n",
    "#             no_graphics=no_graphics,\n",
    "#             side_channels=[channel],        # Added\n",
    "#         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
