{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f9f89eb-2d35-426d-ae3e-bba27489dd3d",
   "metadata": {},
   "source": [
    "# üöÄ Install, Import, and Log In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e55fe183-657a-4323-8641-7655c8a8f7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import random\n",
    "from functools import wraps\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1e49908-fed1-47a2-89b0-829c1e527d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(func):\n",
    "    @wraps(func)\n",
    "    def _time_it(*args, **kwargs):\n",
    "        start = int(round(time() * 1000))\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        finally:\n",
    "            end_ = int(round(time() * 1000)) - start\n",
    "            print(f\"Total execution time: {end_/(1000*60) if end_ > 0 else 0} minutes\")\n",
    "    return _time_it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfbd7d2-c9c7-41f8-b272-178742073096",
   "metadata": {},
   "source": [
    "### 0Ô∏è‚É£ Step 0: Install W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8143d7fa-840c-4f68-94b9-f39d8f0775f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec61dda-606d-45c0-bbd6-6193665559b1",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Step 1: Import W&B and Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef585afd-2998-4b9c-b4e9-65454d72e3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mademord\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72848fd9-bcdb-486b-83a5-13abf7e1bfb7",
   "metadata": {},
   "source": [
    "# üë©‚Äçüî¨ Define the Experiment and Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5314474d-5562-4e65-bf6b-7cad3756473e",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Step 2: Track metadata and hyperparameters with `wandb.init`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa146a4e-9987-4728-9e73-afc9d8e6eec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    epochs=5,\n",
    "    classes=10,\n",
    "    kernels=[16, 32],\n",
    "    batch_size=128,\n",
    "    learning_rate=0.005,\n",
    "    dataset=\"MNIST\",\n",
    "    architecture=\"CNN\",\n",
    "    NUM_TRAINING_STEPS = 100000, #10000000\n",
    "    NUM_TEST_STEPS = 10,\n",
    "    NUM_NEW_EXP = 1000,\n",
    "    BUFFER_SIZE = 10000,\n",
    "    worker_id=4,\n",
    "    time_scale=20\n",
    "    )\n",
    "env = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2308ec7-05f9-4f7d-8ea9-f6c1170d4c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters):\n",
    "    global env\n",
    "    \n",
    "    try:\n",
    "        env.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"pytorch-demo1\", config=hyperparameters):\n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "\n",
    "        # make the model, data, and optimization problem\n",
    "        model, env, criterion, optimizer = make(config)\n",
    "        print(model)\n",
    "\n",
    "        # and use them to train the model\n",
    "        train(model, env, criterion, optimizer, config)\n",
    "\n",
    "        # and test its final performance\n",
    "        test(model, env, config)\n",
    "\n",
    "        env.close()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9bb60f26-0808-4f56-b6ef-0cc41c3033f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make(config):\n",
    "    # Make the data\n",
    "    env, spec = make_env(config)\n",
    "    \n",
    "    # Make the model\n",
    "    # model = Agent(config.kernels, config.classes).to(device)\n",
    "    model = Agent(spec)\n",
    "\n",
    "    # Make the loss and optimizer\n",
    "    criterion = None # nn.CrossEntropyLoss()\n",
    "    optimizer = None # torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    \n",
    "    return model, env, criterion, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a15c7c2-720c-4c1e-b005-0707e9b16a9c",
   "metadata": {},
   "source": [
    "# üì° Define the Env Loading and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb6b9020-dcf3-4ffc-80a5-5217d0fe5937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(config):\n",
    "    channel = EngineConfigurationChannel()\n",
    "    env = UE(\"run32_training\", seed=1, worker_id=config.worker_id, side_channels=[channel])\n",
    "    channel.set_configuration_parameters(time_scale = config.time_scale)\n",
    "    print(\"Environment created.\")\n",
    "    \n",
    "    env.reset()\n",
    "    behavior_name = list(env.behavior_specs)[0]\n",
    "    print(f\"Name of the behavior : {behavior_name}\")\n",
    "    \n",
    "    spec = env.behavior_specs[behavior_name]\n",
    "    print(f\"Type of the spec : {spec}\")\n",
    "    \n",
    "    return env, spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3dc6a727-901a-4175-b712-a1d6364ec1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conventional and convolutional neural network\n",
    "class Agent():\n",
    "    def __init__(self, spec):\n",
    "        self.spec = spec\n",
    "    def get_action(self, decision_steps):\n",
    "        return self.spec.action_spec.random_action(len(decision_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a5bde-932f-4d80-a530-18fbd5346952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "from math import floor\n",
    "\n",
    "\n",
    "class VisualQNetwork(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: Tuple[int], \n",
    "        encoding_size: int, \n",
    "        output_size: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Creates a neural network that takes as input a batch of images (3\n",
    "        dimensional tensors) and outputs a batch of outputs (1 dimensional\n",
    "        tensors)\n",
    "        \"\"\"\n",
    "        super(VisualQNetwork, self).__init__()\n",
    "        self.dense1 = torch.nn.Linear(input_shape[0], encoding_size)\n",
    "        self.dense2 = torch.nn.Linear(encoding_size, encoding_size)\n",
    "\n",
    "        self.dense2_x1 = torch.nn.Linear(encoding_size, output_size)\n",
    "        self.dense2_x2 = torch.nn.Linear(encoding_size, output_size)\n",
    "        self.dense2_x3 = torch.nn.Linear(encoding_size, output_size)\n",
    "\n",
    "        self.act = torch.nn.Sigmoid() # ReLU\n",
    "\n",
    "    def forward(self, visual_obs: torch.tensor):\n",
    "        hidden = self.dense1(visual_obs)\n",
    "        hidden = self.act(hidden)\n",
    "        hidden = self.dense2(hidden)\n",
    "        hidden = self.act(hidden)\n",
    "        x1 = self.dense2_x1(hidden)\n",
    "        x2 = self.dense2_x2(hidden)\n",
    "        x3 = self.dense2_x3(hidden)\n",
    "\n",
    "        #     x1 = self.act(x1)\n",
    "        #     x2 = self.act(x2)\n",
    "        #     x3 = self.act(x3)\n",
    "\n",
    "        return x1, x2, x3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1edc74-99d3-4177-8d78-1db24cb7fa77",
   "metadata": {},
   "source": [
    "# üëü Define Training Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e807cbd-e6f9-4c6e-aceb-811495f82bdc",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Step 3. Track gradients with `wandb.watch` and everything else with `wandb.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bdd68cb-68dd-44d6-9d7c-03f022136d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure\n",
    "def train(model, env, criterion, optimizer, config):\n",
    "#     wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "    behavior_name = list(env.behavior_specs)[0]\n",
    "        \n",
    "    for episode in range(config.NUM_TRAINING_STEPS):\n",
    "        env.reset()\n",
    "        decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "        tracked_agent = -1 # -1 indicates not yet tracking\n",
    "        done = False # For the tracked_agent\n",
    "        episode_rewards = 0 # For the tracked_agent\n",
    "        \n",
    "        while not done:\n",
    "            # Track the first agent we see if not tracking \n",
    "            # Note : len(decision_steps) = [numberb of agents that requested a decision]\n",
    "            if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                tracked_agent = decision_steps.agent_id[0] \n",
    "\n",
    "            # Generate an action for all agents\n",
    "            # these are the observations\n",
    "#             print(decision_steps[0])\n",
    "            action = model.get_action(decision_steps)\n",
    "    #         print(action.discrete)\n",
    "            # Set the actions\n",
    "            env.set_actions(behavior_name, action)\n",
    "\n",
    "            # Move the simulation forward\n",
    "            env.step()\n",
    "\n",
    "            # Get the new simulation results\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            if tracked_agent in decision_steps: # The agent requested a decision\n",
    "                episode_rewards += decision_steps[tracked_agent].reward\n",
    "            if tracked_agent in terminal_steps: # The agent terminated its episode\n",
    "                print(\"reward on terminal step:\", terminal_steps[tracked_agent].reward)\n",
    "                episode_rewards += terminal_steps[tracked_agent].reward\n",
    "                done = True\n",
    "        # print(\"Training step \", episode + 1, \"\\treward \", episode_rewards)\n",
    "        train_log(episode_rewards, episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af41c215-fdab-4a17-a306-dee1b85f5a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_log(reward, episode):\n",
    "#     loss = float(loss)\n",
    "    reward = float(reward)\n",
    "    episode +=1\n",
    "    \n",
    "    # where the magic happens\n",
    "    wandb.log({\"episode\": episode, \"reward\": reward}) #, step=example_ct\n",
    "    print(f\"Reward after \" + str(episode).zfill(5) + f\" episodes: {reward:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13b8c33-5d2d-457c-9abb-9361e273f50f",
   "metadata": {},
   "source": [
    "# üß™ Define Testing Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b01425d-94c6-4573-9f9b-7f502b615a97",
   "metadata": {},
   "source": [
    "#### 4Ô∏è‚É£ Optional Step 4: Call `wandb.save`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08a9a3d0-7baf-4a78-900c-64b47e0b3d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, env, config):\n",
    "#     model.eval()\n",
    "\n",
    "    # Run the model on some test examples\n",
    "    with torch.no_grad():\n",
    "        behavior_name = list(env.behavior_specs)[0]\n",
    "        cumulative_rewards: List[float] = []\n",
    "\n",
    "        for episode in range(config.NUM_TEST_STEPS):\n",
    "            env.reset()\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            tracked_agent = -1 # -1 indicates not yet tracking\n",
    "            done = False # For the tracked_agent\n",
    "            episode_rewards = 0 # For the tracked_agent\n",
    "\n",
    "            while not done:\n",
    "                # Track the first agent we see if not tracking \n",
    "                # Note : len(decision_steps) = [number of agents that requested a decision]\n",
    "                if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                    tracked_agent = decision_steps.agent_id[0] \n",
    "\n",
    "                # Generate an action for all agents\n",
    "                action = model.get_action(decision_steps)\n",
    "\n",
    "                # Set the actions\n",
    "                env.set_actions(behavior_name, action)\n",
    "\n",
    "                # Move the simulation forward\n",
    "                env.step()\n",
    "\n",
    "                # Get the new simulation results\n",
    "                decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "                if tracked_agent in decision_steps: # The agent requested a decision\n",
    "                    episode_rewards += decision_steps[tracked_agent].reward\n",
    "                if tracked_agent in terminal_steps: # The agent terminated its episode\n",
    "                    print(\"reward on terminal step:\", terminal_steps[tracked_agent].reward)\n",
    "                    episode_rewards += terminal_steps[tracked_agent].reward\n",
    "                    done = True\n",
    "            cumulative_rewards.append(episode_rewards)\n",
    "\n",
    "        print(f\"Average reward of the model after {config.NUM_TEST_STEPS} \" +\n",
    "              f\"test episodes: {numpy.average(cumulative_rewards)}%\")\n",
    "\n",
    "        wandb.log({\"test_average_reward\": numpy.average(cumulative_rewards)})\n",
    "\n",
    "    # Save the model in the exchangeable ONNX format\n",
    "    torch.onnx.export(model, [], \"model.onnx\")\n",
    "    wandb.save(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e3150d-5d2b-48dd-a229-b9d9f8eb8b7f",
   "metadata": {},
   "source": [
    "# üèÉ‚Äç‚ôÄÔ∏è Run training and watch your metrics live on [wandb.ai](https://wandb.ai)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "41cb9620-5a48-4fc2-9f2c-422623703720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">earthy-water-23</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ademord/pytorch-demo1\" target=\"_blank\">https://wandb.ai/ademord/pytorch-demo1</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ademord/pytorch-demo1/runs/nrbf9n2z\" target=\"_blank\">https://wandb.ai/ademord/pytorch-demo1/runs/nrbf9n2z</a><br/>\n",
       "                Run data is saved locally in <code>/host/unity_builds/run32_training/wandb/run-20210604_160744-nrbf9n2z</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment created.\n",
      "Name of the behavior : Hummingbird?team=0\n",
      "Type of the spec : BehaviorSpec(observation_specs=[ObservationSpec(shape=(44,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='RayPerceptionSensor'), ObservationSpec(shape=(3,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='StackingSensor_size3_VectorSensor_size1')], action_spec=ActionSpec(continuous_size=0, discrete_branches=(3, 3, 3)))\n",
      "<__main__.Agent object at 0x7f035f67b5e0>\n",
      "reward on terminal step: 0.0\n",
      "Training step  1 \treward  0.0\n",
      "Reward after 00001 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  2 \treward  268.1500812768936\n",
      "Reward after 00002 episodes: 268.150\n",
      "reward on terminal step: -1.0\n",
      "Training step  3 \treward  167.53348442912102\n",
      "Reward after 00003 episodes: 167.533\n",
      "reward on terminal step: 0.0\n",
      "Training step  4 \treward  0.0\n",
      "Reward after 00004 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  5 \treward  306.4784227013588\n",
      "Reward after 00005 episodes: 306.478\n",
      "reward on terminal step: 0.0\n",
      "Training step  6 \treward  0.0\n",
      "Reward after 00006 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  7 \treward  0.0\n",
      "Reward after 00007 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  8 \treward  122.35498017072678\n",
      "Reward after 00008 episodes: 122.355\n",
      "reward on terminal step: -1.0\n",
      "Training step  9 \treward  65.30450585484505\n",
      "Reward after 00009 episodes: 65.305\n",
      "reward on terminal step: 0.0\n",
      "Training step  10 \treward  73.12713271379471\n",
      "Reward after 00010 episodes: 73.127\n",
      "reward on terminal step: 0.0\n",
      "Training step  11 \treward  0.0\n",
      "Reward after 00011 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  12 \treward  60.92063972353935\n",
      "Reward after 00012 episodes: 60.921\n",
      "reward on terminal step: 0.0\n",
      "Training step  13 \treward  0.0\n",
      "Reward after 00013 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  14 \treward  0.0\n",
      "Reward after 00014 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  15 \treward  120.60789829492569\n",
      "Reward after 00015 episodes: 120.608\n",
      "reward on terminal step: 0.0\n",
      "Training step  16 \treward  0.0\n",
      "Reward after 00016 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  17 \treward  0.0\n",
      "Reward after 00017 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  18 \treward  138.69811564683914\n",
      "Reward after 00018 episodes: 138.698\n",
      "reward on terminal step: 0.0\n",
      "Training step  19 \treward  264.3755224943161\n",
      "Reward after 00019 episodes: 264.376\n",
      "reward on terminal step: 0.0\n",
      "Training step  20 \treward  0.0\n",
      "Reward after 00020 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  21 \treward  353.07028046250343\n",
      "Reward after 00021 episodes: 353.070\n",
      "reward on terminal step: 0.0\n",
      "Training step  22 \treward  0.0\n",
      "Reward after 00022 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  23 \treward  0.6099094450473785\n",
      "Reward after 00023 episodes: 0.610\n",
      "reward on terminal step: 0.0\n",
      "Training step  24 \treward  218.40457957983017\n",
      "Reward after 00024 episodes: 218.405\n",
      "reward on terminal step: -1.0\n",
      "Training step  25 \treward  146.2689794898033\n",
      "Reward after 00025 episodes: 146.269\n",
      "reward on terminal step: 0.0\n",
      "Training step  26 \treward  0.0\n",
      "Reward after 00026 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  27 \treward  -1.0\n",
      "Reward after 00027 episodes: -1.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  28 \treward  170.81782174110413\n",
      "Reward after 00028 episodes: 170.818\n",
      "reward on terminal step: 0.0\n",
      "Training step  29 \treward  0.0\n",
      "Reward after 00029 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  30 \treward  226.39850145578384\n",
      "Reward after 00030 episodes: 226.399\n",
      "reward on terminal step: 0.0\n",
      "Training step  31 \treward  0.0\n",
      "Reward after 00031 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  32 \treward  0.0\n",
      "Reward after 00032 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  33 \treward  28.276114225387573\n",
      "Reward after 00033 episodes: 28.276\n",
      "reward on terminal step: -1.0\n",
      "Training step  34 \treward  186.05386033654213\n",
      "Reward after 00034 episodes: 186.054\n",
      "reward on terminal step: 0.0\n",
      "Training step  35 \treward  0.0\n",
      "Reward after 00035 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  36 \treward  0.0\n",
      "Reward after 00036 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  37 \treward  0.0\n",
      "Reward after 00037 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  38 \treward  0.0\n",
      "Reward after 00038 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  39 \treward  0.0\n",
      "Reward after 00039 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  40 \treward  0.0\n",
      "Reward after 00040 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  41 \treward  0.0\n",
      "Reward after 00041 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  42 \treward  0.18422365188598633\n",
      "Reward after 00042 episodes: 0.184\n",
      "reward on terminal step: 0.0\n",
      "Training step  43 \treward  0.0\n",
      "Reward after 00043 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  44 \treward  144.95900297164917\n",
      "Reward after 00044 episodes: 144.959\n",
      "reward on terminal step: 0.0\n",
      "Training step  45 \treward  0.0\n",
      "Reward after 00045 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  46 \treward  0.0\n",
      "Reward after 00046 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  47 \treward  0.0\n",
      "Reward after 00047 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  48 \treward  0.0\n",
      "Reward after 00048 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  49 \treward  0.0\n",
      "Reward after 00049 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  50 \treward  0.0\n",
      "Reward after 00050 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  51 \treward  235.48145031929016\n",
      "Reward after 00051 episodes: 235.481\n",
      "reward on terminal step: 0.0\n",
      "Training step  52 \treward  48.452516943216324\n",
      "Reward after 00052 episodes: 48.453\n",
      "reward on terminal step: 0.0\n",
      "Training step  53 \treward  24.069326639175415\n",
      "Reward after 00053 episodes: 24.069\n",
      "reward on terminal step: 0.0\n",
      "Training step  54 \treward  0.0\n",
      "Reward after 00054 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  55 \treward  0.0\n",
      "Reward after 00055 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  56 \treward  0.0\n",
      "Reward after 00056 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  57 \treward  0.0\n",
      "Reward after 00057 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  58 \treward  0.0\n",
      "Reward after 00058 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  59 \treward  0.0\n",
      "Reward after 00059 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  60 \treward  0.0\n",
      "Reward after 00060 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  61 \treward  0.0\n",
      "Reward after 00061 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  62 \treward  0.0\n",
      "Reward after 00062 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  63 \treward  31.064045071601868\n",
      "Reward after 00063 episodes: 31.064\n",
      "reward on terminal step: 0.0\n",
      "Training step  64 \treward  215.12876123189926\n",
      "Reward after 00064 episodes: 215.129\n",
      "reward on terminal step: 0.0\n",
      "Training step  65 \treward  0.0\n",
      "Reward after 00065 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  66 \treward  0.0\n",
      "Reward after 00066 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  67 \treward  0.0\n",
      "Reward after 00067 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  68 \treward  317.248522400856\n",
      "Reward after 00068 episodes: 317.249\n",
      "reward on terminal step: -1.0\n",
      "Training step  69 \treward  120.06482037901878\n",
      "Reward after 00069 episodes: 120.065\n",
      "reward on terminal step: 0.0\n",
      "Training step  70 \treward  0.0\n",
      "Reward after 00070 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  71 \treward  126.44348466396332\n",
      "Reward after 00071 episodes: 126.443\n",
      "reward on terminal step: 0.0\n",
      "Training step  72 \treward  191.595485329628\n",
      "Reward after 00072 episodes: 191.595\n",
      "reward on terminal step: -1.0\n",
      "Training step  73 \treward  87.65293806791306\n",
      "Reward after 00073 episodes: 87.653\n",
      "reward on terminal step: 0.0\n",
      "Training step  74 \treward  0.0\n",
      "Reward after 00074 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  75 \treward  0.0\n",
      "Reward after 00075 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  76 \treward  0.0\n",
      "Reward after 00076 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  77 \treward  0.0\n",
      "Reward after 00077 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  78 \treward  0.0\n",
      "Reward after 00078 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  79 \treward  0.0\n",
      "Reward after 00079 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  80 \treward  0.0\n",
      "Reward after 00080 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  81 \treward  21.80861672759056\n",
      "Reward after 00081 episodes: 21.809\n",
      "reward on terminal step: 0.0\n",
      "Training step  82 \treward  0.0\n",
      "Reward after 00082 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  83 \treward  139.47891065478325\n",
      "Reward after 00083 episodes: 139.479\n",
      "reward on terminal step: 0.0\n",
      "Training step  84 \treward  0.0\n",
      "Reward after 00084 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  85 \treward  261.22132754325867\n",
      "Reward after 00085 episodes: 261.221\n",
      "reward on terminal step: 0.0\n",
      "Training step  86 \treward  127.98871660232544\n",
      "Reward after 00086 episodes: 127.989\n",
      "reward on terminal step: 0.0\n",
      "Training step  87 \treward  0.0\n",
      "Reward after 00087 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  88 \treward  245.8104648590088\n",
      "Reward after 00088 episodes: 245.810\n",
      "reward on terminal step: 0.0\n",
      "Training step  89 \treward  0.0\n",
      "Reward after 00089 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  90 \treward  0.0\n",
      "Reward after 00090 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  91 \treward  0.0\n",
      "Reward after 00091 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  92 \treward  0.0\n",
      "Reward after 00092 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  93 \treward  5.5081290900707245\n",
      "Reward after 00093 episodes: 5.508\n",
      "reward on terminal step: 0.0\n",
      "Training step  94 \treward  47.34467959403992\n",
      "Reward after 00094 episodes: 47.345\n",
      "reward on terminal step: 0.0\n",
      "Training step  95 \treward  189.00209498405457\n",
      "Reward after 00095 episodes: 189.002\n",
      "reward on terminal step: 0.0\n",
      "Training step  96 \treward  206.01166447997093\n",
      "Reward after 00096 episodes: 206.012\n",
      "reward on terminal step: 0.0\n",
      "Training step  97 \treward  0.0\n",
      "Reward after 00097 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  98 \treward  248.76751109957695\n",
      "Reward after 00098 episodes: 248.768\n",
      "reward on terminal step: -1.0\n",
      "Training step  99 \treward  -1.0\n",
      "Reward after 00099 episodes: -1.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  100 \treward  0.0\n",
      "Reward after 00100 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  101 \treward  0.0\n",
      "Reward after 00101 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  102 \treward  174.16198360919952\n",
      "Reward after 00102 episodes: 174.162\n",
      "reward on terminal step: 0.0\n",
      "Training step  103 \treward  0.0\n",
      "Reward after 00103 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  104 \treward  0.0\n",
      "Reward after 00104 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  105 \treward  181.28788948059082\n",
      "Reward after 00105 episodes: 181.288\n",
      "reward on terminal step: 0.0\n",
      "Training step  106 \treward  90.94064083695412\n",
      "Reward after 00106 episodes: 90.941\n",
      "reward on terminal step: 0.0\n",
      "Training step  107 \treward  0.0\n",
      "Reward after 00107 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  108 \treward  158.31499627232552\n",
      "Reward after 00108 episodes: 158.315\n",
      "reward on terminal step: 0.0\n",
      "Training step  109 \treward  0.0\n",
      "Reward after 00109 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  110 \treward  0.0\n",
      "Reward after 00110 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  111 \treward  0.0\n",
      "Reward after 00111 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  112 \treward  144.20144295692444\n",
      "Reward after 00112 episodes: 144.201\n",
      "reward on terminal step: 0.0\n",
      "Training step  113 \treward  0.0\n",
      "Reward after 00113 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  114 \treward  0.0\n",
      "Reward after 00114 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  115 \treward  0.0\n",
      "Reward after 00115 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  116 \treward  0.0\n",
      "Reward after 00116 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  117 \treward  189.40344062447548\n",
      "Reward after 00117 episodes: 189.403\n",
      "reward on terminal step: 0.0\n",
      "Training step  118 \treward  0.0\n",
      "Reward after 00118 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  119 \treward  0.0\n",
      "Reward after 00119 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  120 \treward  0.0\n",
      "Reward after 00120 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  121 \treward  0.0\n",
      "Reward after 00121 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  122 \treward  131.03784400224686\n",
      "Reward after 00122 episodes: 131.038\n",
      "reward on terminal step: 0.0\n",
      "Training step  123 \treward  0.0\n",
      "Reward after 00123 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  124 \treward  199.45614326000214\n",
      "Reward after 00124 episodes: 199.456\n",
      "reward on terminal step: 0.0\n",
      "Training step  125 \treward  0.0\n",
      "Reward after 00125 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  126 \treward  0.0\n",
      "Reward after 00126 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  127 \treward  92.32203769683838\n",
      "Reward after 00127 episodes: 92.322\n",
      "reward on terminal step: 0.0\n",
      "Training step  128 \treward  0.0\n",
      "Reward after 00128 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  129 \treward  134.90077337622643\n",
      "Reward after 00129 episodes: 134.901\n",
      "reward on terminal step: -1.0\n",
      "Training step  130 \treward  205.64691922068596\n",
      "Reward after 00130 episodes: 205.647\n",
      "reward on terminal step: 0.0\n",
      "Training step  131 \treward  0.0\n",
      "Reward after 00131 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  132 \treward  0.0\n",
      "Reward after 00132 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  133 \treward  75.43627804517746\n",
      "Reward after 00133 episodes: 75.436\n",
      "reward on terminal step: 0.0\n",
      "Training step  134 \treward  0.0\n",
      "Reward after 00134 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  135 \treward  0.0\n",
      "Reward after 00135 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  136 \treward  264.32974675297737\n",
      "Reward after 00136 episodes: 264.330\n",
      "reward on terminal step: 0.0\n",
      "Training step  137 \treward  0.0\n",
      "Reward after 00137 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  138 \treward  327.4802169203758\n",
      "Reward after 00138 episodes: 327.480\n",
      "reward on terminal step: 0.0\n",
      "Training step  139 \treward  169.20268777012825\n",
      "Reward after 00139 episodes: 169.203\n",
      "reward on terminal step: 0.0\n",
      "Training step  140 \treward  0.0\n",
      "Reward after 00140 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  141 \treward  0.0\n",
      "Reward after 00141 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  142 \treward  0.0\n",
      "Reward after 00142 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  143 \treward  113.92615565657616\n",
      "Reward after 00143 episodes: 113.926\n",
      "reward on terminal step: 0.0\n",
      "Training step  144 \treward  0.0\n",
      "Reward after 00144 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  145 \treward  0.6021348834037781\n",
      "Reward after 00145 episodes: 0.602\n",
      "reward on terminal step: 0.0\n",
      "Training step  146 \treward  0.0\n",
      "Reward after 00146 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  147 \treward  114.70004653930664\n",
      "Reward after 00147 episodes: 114.700\n",
      "reward on terminal step: -1.0\n",
      "Training step  148 \treward  75.78926286101341\n",
      "Reward after 00148 episodes: 75.789\n",
      "reward on terminal step: 0.0\n",
      "Training step  149 \treward  65.90889066457748\n",
      "Reward after 00149 episodes: 65.909\n",
      "reward on terminal step: 0.0\n",
      "Training step  150 \treward  0.0\n",
      "Reward after 00150 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  151 \treward  0.0\n",
      "Reward after 00151 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  152 \treward  0.0\n",
      "Reward after 00152 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  153 \treward  24.299064248800278\n",
      "Reward after 00153 episodes: 24.299\n",
      "reward on terminal step: 0.0\n",
      "Training step  154 \treward  0.0\n",
      "Reward after 00154 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  155 \treward  316.33172512054443\n",
      "Reward after 00155 episodes: 316.332\n",
      "reward on terminal step: 0.0\n",
      "Training step  156 \treward  0.0\n",
      "Reward after 00156 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  157 \treward  0.0\n",
      "Reward after 00157 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  158 \treward  53.51096668839455\n",
      "Reward after 00158 episodes: 53.511\n",
      "reward on terminal step: 0.0\n",
      "Training step  159 \treward  0.0\n",
      "Reward after 00159 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  160 \treward  24.52090075612068\n",
      "Reward after 00160 episodes: 24.521\n",
      "reward on terminal step: 0.0\n",
      "Training step  161 \treward  0.0\n",
      "Reward after 00161 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  162 \treward  54.76361787319183\n",
      "Reward after 00162 episodes: 54.764\n",
      "reward on terminal step: 0.0\n",
      "Training step  163 \treward  0.0\n",
      "Reward after 00163 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  164 \treward  307.58726781606674\n",
      "Reward after 00164 episodes: 307.587\n",
      "reward on terminal step: 0.0\n",
      "Training step  165 \treward  106.95207023620605\n",
      "Reward after 00165 episodes: 106.952\n",
      "reward on terminal step: 0.0\n",
      "Training step  166 \treward  0.0\n",
      "Reward after 00166 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  167 \treward  189.90736439824104\n",
      "Reward after 00167 episodes: 189.907\n",
      "reward on terminal step: 0.0\n",
      "Training step  168 \treward  0.0\n",
      "Reward after 00168 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  169 \treward  102.31002897024155\n",
      "Reward after 00169 episodes: 102.310\n",
      "reward on terminal step: 0.0\n",
      "Training step  170 \treward  0.0\n",
      "Reward after 00170 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  171 \treward  0.0\n",
      "Reward after 00171 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  172 \treward  0.0\n",
      "Reward after 00172 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  173 \treward  0.0\n",
      "Reward after 00173 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  174 \treward  338.4273883700371\n",
      "Reward after 00174 episodes: 338.427\n",
      "reward on terminal step: 0.0\n",
      "Training step  175 \treward  0.0\n",
      "Reward after 00175 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  176 \treward  0.0\n",
      "Reward after 00176 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  177 \treward  0.0\n",
      "Reward after 00177 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  178 \treward  274.2560889720917\n",
      "Reward after 00178 episodes: 274.256\n",
      "reward on terminal step: 0.0\n",
      "Training step  179 \treward  0.0\n",
      "Reward after 00179 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  180 \treward  7.560306429862976\n",
      "Reward after 00180 episodes: 7.560\n",
      "reward on terminal step: 0.0\n",
      "Training step  181 \treward  0.0\n",
      "Reward after 00181 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  182 \treward  26.630866289138794\n",
      "Reward after 00182 episodes: 26.631\n",
      "reward on terminal step: 0.0\n",
      "Training step  183 \treward  0.0\n",
      "Reward after 00183 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  184 \treward  0.0\n",
      "Reward after 00184 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  185 \treward  11.658381998538971\n",
      "Reward after 00185 episodes: 11.658\n",
      "reward on terminal step: 0.0\n",
      "Training step  186 \treward  242.7308995127678\n",
      "Reward after 00186 episodes: 242.731\n",
      "reward on terminal step: -1.0\n",
      "Training step  187 \treward  85.33046168088913\n",
      "Reward after 00187 episodes: 85.330\n",
      "reward on terminal step: 0.0\n",
      "Training step  188 \treward  62.459817588329315\n",
      "Reward after 00188 episodes: 62.460\n",
      "reward on terminal step: 0.0\n",
      "Training step  189 \treward  0.0\n",
      "Reward after 00189 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  190 \treward  0.0\n",
      "Reward after 00190 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  191 \treward  0.0\n",
      "Reward after 00191 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  192 \treward  0.0\n",
      "Reward after 00192 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  193 \treward  155.769260764122\n",
      "Reward after 00193 episodes: 155.769\n",
      "reward on terminal step: 0.0\n",
      "Training step  194 \treward  0.0\n",
      "Reward after 00194 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  195 \treward  50.82769626379013\n",
      "Reward after 00195 episodes: 50.828\n",
      "reward on terminal step: 0.0\n",
      "Training step  196 \treward  0.0\n",
      "Reward after 00196 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  197 \treward  93.0310215651989\n",
      "Reward after 00197 episodes: 93.031\n",
      "reward on terminal step: -1.0\n",
      "Training step  198 \treward  3.4656368494033813\n",
      "Reward after 00198 episodes: 3.466\n",
      "reward on terminal step: 0.0\n",
      "Training step  199 \treward  0.0\n",
      "Reward after 00199 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  200 \treward  185.56694293022156\n",
      "Reward after 00200 episodes: 185.567\n",
      "reward on terminal step: -1.0\n",
      "Training step  201 \treward  98.08141285181046\n",
      "Reward after 00201 episodes: 98.081\n",
      "reward on terminal step: 0.0\n",
      "Training step  202 \treward  0.0\n",
      "Reward after 00202 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  203 \treward  0.0\n",
      "Reward after 00203 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  204 \treward  190.54020205140114\n",
      "Reward after 00204 episodes: 190.540\n",
      "reward on terminal step: 0.0\n",
      "Training step  205 \treward  0.0\n",
      "Reward after 00205 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  206 \treward  7.33667528629303\n",
      "Reward after 00206 episodes: 7.337\n",
      "reward on terminal step: -1.0\n",
      "Training step  207 \treward  381.5734239220619\n",
      "Reward after 00207 episodes: 381.573\n",
      "reward on terminal step: 0.0\n",
      "Training step  208 \treward  0.0\n",
      "Reward after 00208 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  209 \treward  0.0\n",
      "Reward after 00209 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  210 \treward  19.452088117599487\n",
      "Reward after 00210 episodes: 19.452\n",
      "reward on terminal step: 0.0\n",
      "Training step  211 \treward  302.9956277012825\n",
      "Reward after 00211 episodes: 302.996\n",
      "reward on terminal step: 0.0\n",
      "Training step  212 \treward  0.0\n",
      "Reward after 00212 episodes: 0.000\n",
      "reward on terminal step: 0.9640111\n",
      "Training step  213 \treward  174.5557433962822\n",
      "Reward after 00213 episodes: 174.556\n",
      "reward on terminal step: 0.0\n",
      "Training step  214 \treward  0.0\n",
      "Reward after 00214 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  215 \treward  0.0\n",
      "Reward after 00215 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  216 \treward  0.0\n",
      "Reward after 00216 episodes: 0.000\n",
      "reward on terminal step: 2.164868\n",
      "Training step  217 \treward  7.278456032276154\n",
      "Reward after 00217 episodes: 7.278\n",
      "reward on terminal step: 0.0\n",
      "Training step  218 \treward  0.0\n",
      "Reward after 00218 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  219 \treward  0.0\n",
      "Reward after 00219 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  220 \treward  0.0\n",
      "Reward after 00220 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  221 \treward  0.0\n",
      "Reward after 00221 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  222 \treward  4.083344548940659\n",
      "Reward after 00222 episodes: 4.083\n",
      "reward on terminal step: 0.0\n",
      "Training step  223 \treward  0.0\n",
      "Reward after 00223 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  224 \treward  0.0\n",
      "Reward after 00224 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  225 \treward  0.0\n",
      "Reward after 00225 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  226 \treward  0.0\n",
      "Reward after 00226 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  227 \treward  16.74015784263611\n",
      "Reward after 00227 episodes: 16.740\n",
      "reward on terminal step: 0.0\n",
      "Training step  228 \treward  0.0\n",
      "Reward after 00228 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  229 \treward  0.0\n",
      "Reward after 00229 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  230 \treward  0.0\n",
      "Reward after 00230 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  231 \treward  0.0\n",
      "Reward after 00231 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  232 \treward  0.0\n",
      "Reward after 00232 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  233 \treward  0.0\n",
      "Reward after 00233 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  234 \treward  16.419668078422546\n",
      "Reward after 00234 episodes: 16.420\n",
      "reward on terminal step: 0.0\n",
      "Training step  235 \treward  0.0\n",
      "Reward after 00235 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  236 \treward  7.707404136657715\n",
      "Reward after 00236 episodes: 7.707\n",
      "reward on terminal step: -1.0\n",
      "Training step  237 \treward  76.54296359419823\n",
      "Reward after 00237 episodes: 76.543\n",
      "reward on terminal step: 0.0\n",
      "Training step  238 \treward  130.2257729768753\n",
      "Reward after 00238 episodes: 130.226\n",
      "reward on terminal step: 0.0\n",
      "Training step  239 \treward  184.81777679920197\n",
      "Reward after 00239 episodes: 184.818\n",
      "reward on terminal step: 0.0\n",
      "Training step  240 \treward  347.7732498049736\n",
      "Reward after 00240 episodes: 347.773\n",
      "reward on terminal step: 0.0\n",
      "Training step  241 \treward  0.0\n",
      "Reward after 00241 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  242 \treward  0.0\n",
      "Reward after 00242 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  243 \treward  0.0\n",
      "Reward after 00243 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  244 \treward  24.851968616247177\n",
      "Reward after 00244 episodes: 24.852\n",
      "reward on terminal step: 0.0\n",
      "Training step  245 \treward  0.0\n",
      "Reward after 00245 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  246 \treward  153.04301378130913\n",
      "Reward after 00246 episodes: 153.043\n",
      "reward on terminal step: 0.0\n",
      "Training step  247 \treward  0.0\n",
      "Reward after 00247 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  248 \treward  0.0\n",
      "Reward after 00248 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  249 \treward  16.0705087184906\n",
      "Reward after 00249 episodes: 16.071\n",
      "reward on terminal step: 0.0\n",
      "Training step  250 \treward  0.0\n",
      "Reward after 00250 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  251 \treward  0.0\n",
      "Reward after 00251 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  252 \treward  0.0\n",
      "Reward after 00252 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  253 \treward  0.0\n",
      "Reward after 00253 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  254 \treward  0.0\n",
      "Reward after 00254 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  255 \treward  31.179425448179245\n",
      "Reward after 00255 episodes: 31.179\n",
      "reward on terminal step: -1.0\n",
      "Training step  256 \treward  32.717336654663086\n",
      "Reward after 00256 episodes: 32.717\n",
      "reward on terminal step: -1.0\n",
      "Training step  257 \treward  300.68060916662216\n",
      "Reward after 00257 episodes: 300.681\n",
      "reward on terminal step: 0.0\n",
      "Training step  258 \treward  165.81790417432785\n",
      "Reward after 00258 episodes: 165.818\n",
      "reward on terminal step: 0.0\n",
      "Training step  259 \treward  0.0\n",
      "Reward after 00259 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  260 \treward  0.0\n",
      "Reward after 00260 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  261 \treward  42.133475959300995\n",
      "Reward after 00261 episodes: 42.133\n",
      "reward on terminal step: 0.0\n",
      "Training step  262 \treward  27.60027700662613\n",
      "Reward after 00262 episodes: 27.600\n",
      "reward on terminal step: 0.0\n",
      "Training step  263 \treward  0.0\n",
      "Reward after 00263 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  264 \treward  0.0\n",
      "Reward after 00264 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  265 \treward  0.0\n",
      "Reward after 00265 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  266 \treward  76.37919735908508\n",
      "Reward after 00266 episodes: 76.379\n",
      "reward on terminal step: 0.0\n",
      "Training step  267 \treward  0.0\n",
      "Reward after 00267 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  268 \treward  0.0\n",
      "Reward after 00268 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  269 \treward  80.70052313804626\n",
      "Reward after 00269 episodes: 80.701\n",
      "reward on terminal step: -1.0\n",
      "Training step  270 \treward  289.44769191741943\n",
      "Reward after 00270 episodes: 289.448\n",
      "reward on terminal step: -1.0\n",
      "Training step  271 \treward  294.7904590666294\n",
      "Reward after 00271 episodes: 294.790\n",
      "reward on terminal step: 0.0\n",
      "Training step  272 \treward  0.0\n",
      "Reward after 00272 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  273 \treward  8.72112461924553\n",
      "Reward after 00273 episodes: 8.721\n",
      "reward on terminal step: 0.0\n",
      "Training step  274 \treward  9.30902448296547\n",
      "Reward after 00274 episodes: 9.309\n",
      "reward on terminal step: -1.0\n",
      "Training step  275 \treward  183.04041409492493\n",
      "Reward after 00275 episodes: 183.040\n",
      "reward on terminal step: 0.0\n",
      "Training step  276 \treward  162.9343104660511\n",
      "Reward after 00276 episodes: 162.934\n",
      "reward on terminal step: -1.0\n",
      "Training step  277 \treward  338.49690240621567\n",
      "Reward after 00277 episodes: 338.497\n",
      "reward on terminal step: 0.0\n",
      "Training step  278 \treward  0.0\n",
      "Reward after 00278 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  279 \treward  141.43411564826965\n",
      "Reward after 00279 episodes: 141.434\n",
      "reward on terminal step: -1.0\n",
      "Training step  280 \treward  67.48445296287537\n",
      "Reward after 00280 episodes: 67.484\n",
      "reward on terminal step: 0.0\n",
      "Training step  281 \treward  0.0\n",
      "Reward after 00281 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  282 \treward  0.0\n",
      "Reward after 00282 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  283 \treward  0.0\n",
      "Reward after 00283 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  284 \treward  40.78170734643936\n",
      "Reward after 00284 episodes: 40.782\n",
      "reward on terminal step: 0.0\n",
      "Training step  285 \treward  0.0\n",
      "Reward after 00285 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  286 \treward  0.0\n",
      "Reward after 00286 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  287 \treward  281.46989274024963\n",
      "Reward after 00287 episodes: 281.470\n",
      "reward on terminal step: -1.0\n",
      "Training step  288 \treward  269.511178702116\n",
      "Reward after 00288 episodes: 269.511\n",
      "reward on terminal step: 0.0\n",
      "Training step  289 \treward  23.440168142318726\n",
      "Reward after 00289 episodes: 23.440\n",
      "reward on terminal step: 0.0\n",
      "Training step  290 \treward  0.0\n",
      "Reward after 00290 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  291 \treward  253.92954522371292\n",
      "Reward after 00291 episodes: 253.930\n",
      "reward on terminal step: 0.0\n",
      "Training step  292 \treward  0.0\n",
      "Reward after 00292 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  293 \treward  114.10660970211029\n",
      "Reward after 00293 episodes: 114.107\n",
      "reward on terminal step: 0.0\n",
      "Training step  294 \treward  0.0\n",
      "Reward after 00294 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  295 \treward  194.86958527565002\n",
      "Reward after 00295 episodes: 194.870\n",
      "reward on terminal step: 0.0\n",
      "Training step  296 \treward  0.0\n",
      "Reward after 00296 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  297 \treward  0.0\n",
      "Reward after 00297 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  298 \treward  259.41537779569626\n",
      "Reward after 00298 episodes: 259.415\n",
      "reward on terminal step: 0.0\n",
      "Training step  299 \treward  0.0\n",
      "Reward after 00299 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  300 \treward  0.0\n",
      "Reward after 00300 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  301 \treward  89.47804629802704\n",
      "Reward after 00301 episodes: 89.478\n",
      "reward on terminal step: -1.0\n",
      "Training step  302 \treward  93.12740540504456\n",
      "Reward after 00302 episodes: 93.127\n",
      "reward on terminal step: 0.0\n",
      "Training step  303 \treward  0.0\n",
      "Reward after 00303 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  304 \treward  0.0\n",
      "Reward after 00304 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  305 \treward  0.0\n",
      "Reward after 00305 episodes: 0.000\n",
      "reward on terminal step: 0.0\n",
      "Training step  306 \treward  0.0\n",
      "Reward after 00306 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  307 \treward  42.582351595163345\n",
      "Reward after 00307 episodes: 42.582\n",
      "reward on terminal step: 0.0\n",
      "Training step  308 \treward  104.24551904201508\n",
      "Reward after 00308 episodes: 104.246\n",
      "reward on terminal step: 0.0\n",
      "Training step  309 \treward  0.0\n",
      "Reward after 00309 episodes: 0.000\n",
      "reward on terminal step: -1.0\n",
      "Training step  310 \treward  147.87159314751625\n",
      "Reward after 00310 episodes: 147.872\n",
      "Total execution time: 163.85923333333332 minutes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 12936<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/host/unity_builds/run32_training/wandb/run-20210604_160744-nrbf9n2z/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/host/unity_builds/run32_training/wandb/run-20210604_160744-nrbf9n2z/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>episode</td><td>310</td></tr><tr><td>reward</td><td>147.87159</td></tr><tr><td>_runtime</td><td>9816</td></tr><tr><td>_timestamp</td><td>1622832680</td></tr><tr><td>_step</td><td>309</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>episode</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>reward</td><td>‚ñÅ‚ñÑ‚ñÅ‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÖ‚ñÅ‚ñá‚ñÜ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÉ</td></tr><tr><td>_runtime</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>_timestamp</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">earthy-water-23</strong>: <a href=\"https://wandb.ai/ademord/pytorch-demo1/runs/nrbf9n2z\" target=\"_blank\">https://wandb.ai/ademord/pytorch-demo1/runs/nrbf9n2z</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-318ae2ededa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build, train and analyze the model with the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-e11be425bb88>\u001b[0m in \u001b[0;36mmodel_pipeline\u001b[0;34m(hyperparameters)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# and use them to train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# and test its final performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-2872183460f3>\u001b[0m in \u001b[0;36m_time_it\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mend_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-e2ed565d70eb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, env, criterion, optimizer, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# Move the simulation forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# Get the new simulation results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/mlagents_envs/timers.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/mlagents_envs/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mstep_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_step_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"communicator.exchange\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexchange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll_process\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUnityCommunicatorStoppedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Communicator has exited.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/mlagents_envs/rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[0;34m(self, inputs, poll_callback)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll_for_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/mlagents_envs/rpc_communicator.py\u001b[0m in \u001b[0;36mpoll_for_timeout\u001b[0;34m(self, poll_callback)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mcallback_timeout_wait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout_wait\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback_timeout_wait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0;31m# Got an acknowledgment from the connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Build, train and analyze the model with the pipeline\n",
    "model, env = model_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6a7a3de-8636-4436-be16-b8084e1b3025",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ce5513-e1eb-453b-a0b4-689330842816",
   "metadata": {},
   "source": [
    "# üßπ Test Hyperparameters with Sweeps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13526307-58d0-4263-8b59-714525a51117",
   "metadata": {},
   "source": [
    "## [Check out Hyperparameter Optimization in PyTorch using W&B Sweep $\\rightarrow$](https://colab.research.google.com/drive/1QTIK23LBuAkdejbrvdP5hwBGyYlyEJpT?usp=sharing)\n",
    "\n",
    "Running a hyperparameter sweep with Weights & Biases is very easy. There are just 3 simple steps:\n",
    "\n",
    "1. **Define the sweep:** We do this by creating a dictionary or a [YAML file](https://docs.wandb.com/library/sweeps/configuration) that specifies the parameters to search through, the search strategy, the optimization metric et all.\n",
    "\n",
    "2. **Initialize the sweep:** \n",
    "`sweep_id = wandb.sweep(sweep_config)`\n",
    "\n",
    "3. **Run the sweep agent:** \n",
    "`wandb.agent(sweep_id, function=train)`\n",
    "\n",
    "And voila! That's all there is to running a hyperparameter sweep!\n",
    "<img src=\"https://imgur.com/UiQKg0L.png\" alt=\"Weights & Biases\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215b498a-9711-46b3-afc7-4cd3742df753",
   "metadata": {},
   "source": [
    "# ü§ì Advanced Setup\n",
    "1. [Environment variables](https://docs.wandb.com/library/environment-variables): Set API keys in environment variables so you can run training on a managed cluster.\n",
    "2. [Offline mode](https://docs.wandb.com/library/technical-faq#can-i-run-wandb-offline): Use `dryrun` mode to train offline and sync results later.\n",
    "3. [On-prem](https://docs.wandb.com/self-hosted): Install W&B in a private cloud or air-gapped servers in your own infrastructure. We have local installations for everyone from academics to enterprise teams.\n",
    "4. [Sweeps](https://docs.wandb.com/sweeps): Set up hyperparameter search quickly with our lightweight tool for tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9360baa-2b59-4cb6-86f9-8dca4deb31f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n in range(NUM_TRAINING_STEPS):\n",
    "#   new_exp,_ = Trainer.generate_trajectories(env, qnet, NUM_NEW_EXP, epsilon=0.1)\n",
    "#   random.shuffle(experiences)\n",
    "#   if len(experiences) > BUFFER_SIZE:\n",
    "#     experiences = experiences[:BUFFER_SIZE]\n",
    "#   experiences.extend(new_exp)\n",
    "#   Trainer.update_q_net(qnet, optim, experiences, 3)\n",
    "#   _, rewards = Trainer.generate_trajectories(env, qnet, 100, epsilon=0)\n",
    "#   cumulative_rewards.append(rewards)\n",
    "#   print(\"Training step \", n+1, \"\\treward \", rewards)\n",
    "#   print()\n",
    "\n",
    "\n",
    "# env.close()\n",
    "\n",
    "# # Show the training graph\n",
    "# plt.plot(range(NUM_TRAINING_STEPS), cumulative_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f84bef9-4e0b-4829-98f6-9d5548a5b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  class UnityEnv(gym.Env):\n",
    "#     \"\"\"\n",
    "#     Provides Gym wrapper for Unity Learning Environments.\n",
    "#     Multi-agent environments use lists for object types, as done here:\n",
    "#     https://github.com/openai/multiagent-particle-envs\n",
    "#     \"\"\"\n",
    " \n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         environment_filename: str,\n",
    "#         dimensions: int = [],   #Added\n",
    "#         timescale: int = 1,     #Added\n",
    "#         worker_id: int = 0,\n",
    "#         use_visual: bool = False,\n",
    "#         uint8_visual: bool = False,\n",
    "#         multiagent: bool = False,\n",
    "#         flatten_branched: bool = False,\n",
    "#         no_graphics: bool = False,\n",
    "#         allow_multiple_visual_obs: bool = False,\n",
    "#         set_config: bool = True,    #Added\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Environment initialization\n",
    "#         :param environment_filename: The UnityEnvironment path or file to be wrapped in the gym.\n",
    "#         :param worker_id: Worker number for environment.\n",
    "#         :param use_visual: Whether to use visual observation or vector observation.\n",
    "#         :param uint8_visual: Return visual observations as uint8 (0-255) matrices instead of float (0.0-1.0).\n",
    "#         :param multiagent: Whether to run in multi-agent mode (lists of obs, reward, done).\n",
    "#         :param flatten_branched: If True, turn branched discrete action spaces into a Discrete space rather than\n",
    "#             MultiDiscrete.\n",
    "#         :param no_graphics: Whether to run the Unity simulator in no-graphics mode\n",
    "#         :param allow_multiple_visual_obs: If True, return a list of visual observations instead of only one.\n",
    "#         \"\"\"\n",
    "#         base_port = 5005\n",
    "#         if environment_filename is None:\n",
    "#             base_port = UnityEnvironment.DEFAULT_EDITOR_PORT\n",
    " \n",
    "#         channel = EngineConfigurationChannel()        # Added\n",
    " \n",
    " \n",
    "#         #Added\n",
    "#         if set_config == True:\n",
    "#             channel.set_configuration_parameters(time_scale=timescale, width=dimensions[0], height=dimensions[1])\n",
    "#         #Added\n",
    " \n",
    "#         self._env = UnityEnvironment(\n",
    "#             environment_filename,\n",
    "#             worker_id,\n",
    "#             base_port=base_port,\n",
    "#             no_graphics=no_graphics,\n",
    "#             side_channels=[channel],        # Added\n",
    "#         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
