{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# # wandb.init(config=args)a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml-agents already installed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import mlagents\n",
    "    from mlagents_envs.environment import UnityEnvironment as UE\n",
    "    from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "    print(\"ml-agents already installed\")\n",
    "except ImportError:\n",
    "#     !pip install mlagents==0.26.0\n",
    "    print(\"Installed ml-agents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "from math import floor\n",
    "\n",
    "\n",
    "class VisualQNetwork(torch.nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    input_shape: Tuple[int], \n",
    "    encoding_size: int, \n",
    "    output_size: int\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Creates a neural network that takes as input a batch of images (3\n",
    "    dimensional tensors) and outputs a batch of outputs (1 dimensional\n",
    "    tensors)\n",
    "    \"\"\"\n",
    "    super(VisualQNetwork, self).__init__()\n",
    "#     height = input_shape[0]\n",
    "#     width = input_shape[1]\n",
    "#     initial_channels = input_shape[2]\n",
    "#     conv_1_hw = self.conv_output_shape((height, width), 8, 4)\n",
    "#     conv_2_hw = self.conv_output_shape(conv_1_hw, 4, 2)\n",
    "#     self.final_flat = conv_2_hw[0] * conv_2_hw[1] * 32\n",
    "    \n",
    "    \n",
    "#     self.conv1 = torch.nn.Conv2d(initial_channels, 16, [8, 8], [4, 4])\n",
    "#     self.conv2 = torch.nn.Conv2d(16, 32, [4, 4], [2, 2])\n",
    "#     self.dense1 = torch.nn.Linear(self.final_flat, encoding_size)\n",
    "\n",
    "    \n",
    "    self.dense1 = torch.nn.Linear(input_shape[0], encoding_size)\n",
    "    self.dense2 = torch.nn.Linear(encoding_size, encoding_size)\n",
    "    \n",
    "    self.dense2_x1 = torch.nn.Linear(encoding_size, output_size)\n",
    "    self.dense2_x2 = torch.nn.Linear(encoding_size, output_size)\n",
    "    self.dense2_x3 = torch.nn.Linear(encoding_size, output_size)\n",
    "   \n",
    "    self.act = torch.nn.Sigmoid() # ReLU\n",
    "\n",
    "    \n",
    "    \n",
    "  def forward(self, visual_obs: torch.tensor):\n",
    "#     print(\"torch input size:\", visual_obs.size())\n",
    "#     visual_obs = visual_obs.permute(0, 3, 1, 2)\n",
    "#     conv_1 = torch.relu(self.conv1(visual_obs))\n",
    "#     conv_2 = torch.relu(self.conv2(conv_1))\n",
    "#     hidden = self.dense1(conv_2.reshape([-1, self.final_flat]))\n",
    "\n",
    "    hidden = self.dense1(visual_obs)\n",
    "    hidden = self.act(hidden)\n",
    "\n",
    "    hidden = self.dense2(hidden)\n",
    "    hidden = self.act(hidden)\n",
    "\n",
    "    x1 = self.dense2_x1(hidden)\n",
    "#     x1 = self.act(x1)\n",
    "    x2 = self.dense2_x2(hidden)\n",
    "#     x2 = self.act(x2)\n",
    "    x3 = self.dense2_x3(hidden)\n",
    "#     x3 = self.act(x3)\n",
    "\n",
    "    return x1, x2, x3\n",
    "\n",
    "  @staticmethod\n",
    "  def conv_output_shape(\n",
    "    h_w: Tuple[int, int],\n",
    "    kernel_size: int = 1,\n",
    "    stride: int = 1,\n",
    "    pad: int = 0,\n",
    "    dilation: int = 1,\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Computes the height and width of the output of a convolution layer.\n",
    "    \"\"\"\n",
    "    h = floor(\n",
    "      ((h_w[0] + (2 * pad) - (dilation * (kernel_size - 1)) - 1) / stride) + 1\n",
    "    )\n",
    "    w = floor(\n",
    "      ((h_w[1] + (2 * pad) - (dilation * (kernel_size - 1)) - 1) / stride) + 1\n",
    "    )\n",
    "    return h, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import NamedTuple, List\n",
    "\n",
    "\n",
    "class Experience(NamedTuple):\n",
    "  \"\"\"\n",
    "  An experience contains the data of one Agent transition.\n",
    "  - Observation\n",
    "  - Action\n",
    "  - Reward\n",
    "  - Done flag\n",
    "  - Next Observation\n",
    "  \"\"\"\n",
    "\n",
    "  obs: np.ndarray\n",
    "  action: np.ndarray\n",
    "  reward: float\n",
    "  done: bool\n",
    "  next_obs: np.ndarray\n",
    "\n",
    "# A Trajectory is an ordered sequence of Experiences\n",
    "Trajectory = List[Experience]\n",
    "\n",
    "# A Buffer is an unordered list of Experiences from multiple Trajectories\n",
    "Buffer = List[Experience]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import ActionTuple, BaseEnv\n",
    "from typing import Dict\n",
    "import random\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "  @staticmethod\n",
    "  def generate_trajectories(\n",
    "    env: BaseEnv, q_net: VisualQNetwork, buffer_size: int, epsilon: float\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Given a Unity Environment and a Q-Network, this method will generate a\n",
    "    buffer of Experiences obtained by running the Environment with the Policy\n",
    "    derived from the Q-Network.\n",
    "    :param BaseEnv: The UnityEnvironment used.\n",
    "    :param q_net: The Q-Network used to collect the data.\n",
    "    :param buffer_size: The minimum size of the buffer this method will return.\n",
    "    :param epsilon: Will add a random normal variable with standard deviation.\n",
    "    epsilon to the value heads of the Q-Network to encourage exploration.\n",
    "    :returns: a Tuple containing the created buffer and the average cumulative\n",
    "    the Agents obtained.\n",
    "    \"\"\"\n",
    "    # Create an empty Buffer\n",
    "    buffer: Buffer = []\n",
    "\n",
    "    # Reset the environment\n",
    "    env.reset()\n",
    "    # Read and store the Behavior Name of the Environment\n",
    "    behavior_name = list(env.behavior_specs)[0]\n",
    "    # Read and store the Behavior Specs of the Environment\n",
    "    spec = env.behavior_specs[behavior_name]\n",
    "\n",
    "    # Create a Mapping from AgentId to Trajectories. This will help us create\n",
    "    # trajectories for each Agents\n",
    "    dict_trajectories_from_agent: Dict[int, Trajectory] = {}\n",
    "    # Create a Mapping from AgentId to the last observation of the Agent\n",
    "    dict_last_obs_from_agent: Dict[int, np.ndarray] = {}\n",
    "    # Create a Mapping from AgentId to the last observation of the Agent\n",
    "    dict_last_action_from_agent: Dict[int, np.ndarray] = {}\n",
    "    # Create a Mapping from AgentId to cumulative reward (Only for reporting)\n",
    "    dict_cumulative_reward_from_agent: Dict[int, float] = {}\n",
    "    # Create a list to store the cumulative rewards obtained so far\n",
    "    cumulative_rewards: List[float] = []\n",
    "    \n",
    "    \n",
    "    entered_terminal = False\n",
    "    while len(buffer) < buffer_size:  # While not enough data in the buffer\n",
    "      # Get the Decision Steps and Terminal Steps of the Agents\n",
    "      decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    \n",
    "        # For all Agents with a Terminal Step:\n",
    "      for agent_id_terminated in terminal_steps:\n",
    "#         print(\"entered agent with terminal step\")\n",
    "#         print(agent_id_terminated)\n",
    "\n",
    "        # Create its last experience (is last because the Agent terminated)\n",
    "        last_experience = Experience(\n",
    "          obs=dict_last_obs_from_agent[agent_id_terminated].copy(),\n",
    "          reward=terminal_steps[agent_id_terminated].reward,\n",
    "          done=not terminal_steps[agent_id_terminated].interrupted,\n",
    "          action=dict_last_action_from_agent[agent_id_terminated].copy(),\n",
    "          next_obs=terminal_steps[agent_id_terminated].obs[0],\n",
    "        )\n",
    "        # Clear its last observation and action (Since the trajectory is over)\n",
    "        dict_last_obs_from_agent.pop(agent_id_terminated)\n",
    "        dict_last_action_from_agent.pop(agent_id_terminated)\n",
    "        # Report the cumulative reward\n",
    "        cumulative_reward = (\n",
    "          dict_cumulative_reward_from_agent.pop(agent_id_terminated)\n",
    "          + terminal_steps[agent_id_terminated].reward\n",
    "        )\n",
    "#         print(\"cumulative reward: \", cumulative_reward)\n",
    "        cumulative_rewards.append(cumulative_reward) #  - 50\n",
    "        # Add the Trajectory and the last experience to the buffer\n",
    "        buffer.extend(dict_trajectories_from_agent.pop(agent_id_terminated))\n",
    "        buffer.append(last_experience)\n",
    "        entered_terminal = True\n",
    "\n",
    "      # For all Agents with a Decision Step:\n",
    "      for agent_id_decisions in decision_steps:\n",
    "        # If the Agent does not have a Trajectory, create an empty one\n",
    "        if agent_id_decisions not in dict_trajectories_from_agent:\n",
    "          dict_trajectories_from_agent[agent_id_decisions] = []\n",
    "          dict_cumulative_reward_from_agent[agent_id_decisions] = 0\n",
    "\n",
    "        # If the Agent requesting a decision has a \"last observation\"\n",
    "        if agent_id_decisions in dict_last_obs_from_agent:\n",
    "          # Create an Experience from the last observation and the Decision Step\n",
    "          exp = Experience(\n",
    "            obs=dict_last_obs_from_agent[agent_id_decisions].copy(),\n",
    "            reward=decision_steps[agent_id_decisions].reward, #  - 0.05\n",
    "            done=False,\n",
    "            action=dict_last_action_from_agent[agent_id_decisions].copy(),\n",
    "            next_obs=decision_steps[agent_id_decisions].obs[0],\n",
    "          )\n",
    "          # Update the Trajectory of the Agent and its cumulative reward\n",
    "          dict_trajectories_from_agent[agent_id_decisions].append(exp)\n",
    "          dict_cumulative_reward_from_agent[agent_id_decisions] += (\n",
    "            decision_steps[agent_id_decisions].reward\n",
    "          )\n",
    "        # Store the observation as the new \"last observation\"\n",
    "        dict_last_obs_from_agent[agent_id_decisions] = (\n",
    "          decision_steps[agent_id_decisions].obs[0]\n",
    "        )\n",
    "\n",
    "      # Generate an action for all the Agents that requested a decision\n",
    "      # Compute the values for each action given the observation    \n",
    "      act1, act2, act3 = q_net(torch.from_numpy(decision_steps.obs[0]))\n",
    "    \n",
    "      if len(decision_steps) == 0:\n",
    "#             print(\"error: no more observations ! \")\n",
    "            env.step()\n",
    "            continue\n",
    "#       if act1.size == 0:\n",
    "#             print(\"error: Action space received = 0\")\n",
    "#             env.step()\n",
    "#             continue\n",
    "            \n",
    "      # get actions as arrays\n",
    "      act1 = act1.detach().numpy()\n",
    "      act2 = act2.detach().numpy()\n",
    "      act3 = act3.detach().numpy()\n",
    "    \n",
    "#       print(\"action received from QNetwork: \", act1)\n",
    "#       print(\"action received from QNetwork: \", act2)\n",
    "#       print(\"action received from QNetwork: \", act3)\n",
    "      act1 += epsilon * np.random.randn(act1.shape[0], act1.shape[1]).astype(np.float32)\n",
    "      act2 += epsilon * np.random.randn(act1.shape[0], act1.shape[1]).astype(np.float32)\n",
    "      act3 += epsilon * np.random.randn(act1.shape[0], act1.shape[1]).astype(np.float32)\n",
    "      \n",
    "      # pick the best action using argmax\n",
    "      act1 = np.argmax(act1, axis=1)\n",
    "      act2 = np.argmax(act2, axis=1)\n",
    "      act3 = np.argmax(act3, axis=1)\n",
    "#       print(\"action received from argmax: \", act1)\n",
    "#       print(\"action received from argmax: \", act1.shape)\n",
    "#       act1 = np.array([act1])\n",
    "#       act2 = np.array([act2])\n",
    "#       act3 = np.array([act3])\n",
    "      act1 = np.expand_dims(act1, axis=1)\n",
    "      act2 = np.expand_dims(act2, axis=1)\n",
    "      act3 = np.expand_dims(act3, axis=1)\n",
    "#       print(\"action received from argmax expanded: \", act1)\n",
    "#       print(\"action received from argmax expanded: \", act1.shape)\n",
    "\n",
    "      # map action index 2 to -1 for the agent to move backwards, left, and rotate left\n",
    "      act1[act1 > 1] = -1\n",
    "      act2[act2 > 1] = -1\n",
    "      act3[act3 > 1] = -1\n",
    "\n",
    "      # format to numpy arrays\n",
    "#       print(\"action received from mapping: \", act1)\n",
    "#       try:\n",
    "# #         actions_values = np.array([act1, act2, act3]).reshape(3,3)\n",
    "#         actions_values = np.vstack((act1, act2, act3))\n",
    "#       except:\n",
    "#         actions_values = np.zeros((3,3))\n",
    "#         print(\"error: network received an input of size 0 and i caught the error :/\")\n",
    "\n",
    "        #      0-8nt(type(actions_values))\n",
    "    \n",
    "    \n",
    "#       actions =  np.vstack((act1, act2, act3))\n",
    "#       actions =  np.vstack((act1, act2, act3))\n",
    "\n",
    "      temp = np.hstack((act1, act2, act3))\n",
    "#       print(\"actions stacked with hstack:\", temp)\n",
    "#       print(\"actions stacked with hstack: shape:\", temp.shape)\n",
    "#       temp = np.concatenate((act1, act2, act3), axis=1)\n",
    "#       print(\"actions stacked with concat:\", temp)\n",
    "#       print(\"actions stacked with concat: shape:\", temp.shape)\n",
    "        \n",
    "#       actions_values = np.zeros((3,3))\n",
    "#       # Add some noise with epsilon to the values\n",
    "#       actions_values += epsilon * np.random.randn(actions_values.shape[0], actions_values.shape[1]).astype(np.float32)\n",
    "#       actions = np.argmax(actions_values, axis=1)\n",
    "      \n",
    "      actions = temp\n",
    "#       print(\"final actions: \", actions)\n",
    "#       print(\"final actions shape: \", actions.shape)\n",
    "      actions.resize((len(decision_steps), 3))\n",
    "#       print(\"decision steps size:\", len(decision_steps))\n",
    "#       print(\"final actions after resize: \", actions)\n",
    "#       print(\"final actions shape: \", actions.shape)\n",
    "\n",
    "\n",
    "      # Store the action that was picked, it will be put in the trajectory later\n",
    "      for agent_index, agent_id in enumerate(decision_steps.agent_id):\n",
    "        dict_last_action_from_agent[agent_id] = actions[agent_index]\n",
    "#       print(\"dict last action: \", dict_last_action_from_agent)\n",
    "\n",
    "        \n",
    "      # Set the actions in the environment\n",
    "      # Unity Environments expect ActionTuple instances.\n",
    "      action_tuple = ActionTuple()\n",
    "      action_tuple.add_discrete(actions)\n",
    "#       print(\"filtered action received from QNetwork: \", action_tuple.discrete)\n",
    "      env.set_actions(behavior_name, action_tuple)\n",
    "      # Perform a step in the simulation\n",
    "      env.step()\n",
    "    return buffer, np.mean(cumulative_rewards)\n",
    "\n",
    "  @staticmethod\n",
    "  def update_q_net(\n",
    "    q_net: VisualQNetwork, \n",
    "    optimizer: torch.optim, \n",
    "    buffer: Buffer, \n",
    "    action_size: int\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Performs an update of the Q-Network using the provided optimizer and buffer\n",
    "    \"\"\"\n",
    "    def calculate_bellman_loss(next_pred_action, pred_action, reward, done, GAMMA, batch, action_size, action):\n",
    "        # Use the Bellman equation to update the Q-Network\n",
    "        target = (\n",
    "          reward\n",
    "          + (1.0 - done)\n",
    "          * GAMMA\n",
    "          * torch.max(next_pred_action.detach(), dim=1, keepdim=True).values\n",
    "        ).double()\n",
    "#         print(\"next_act_prediction:\", next_pred_action.detach().numpy())\n",
    "        \n",
    "#         print(\"Target:\", target)\n",
    "#         print(\"Target shape:\", target.shape)\n",
    "\n",
    "#         print(\"action:\", action)\n",
    "#         print(\"action shape: \", action.shape)\n",
    "        assert(action.shape[0] == len(batch))\n",
    "        action[action < 0] = 2\n",
    "#         print(\"action after correction:\", action)\n",
    "        \n",
    "        mask = np.eye(action_size)[action]\n",
    "#         mask = torch.zeros((len(batch), action_size))  \n",
    "#         print(\"mask: \", mask)\n",
    "#         print(\"mask shape: \", mask.shape)\n",
    "#         mask.scatter_(1, action, 1)\n",
    "#         print(\"mask after scatter: \", mask)\n",
    "        mask = torch.from_numpy(mask).double()\n",
    "#         print(\"pred_action: error\", pred_action)\n",
    "#         print(type(pred_action))\n",
    "#         print(pred_action.dtype)\n",
    "#         print(type(mask))\n",
    "#         print(mask.dtype)\n",
    "        prediction = torch.sum(pred_action.double() * mask, dim=1, keepdim=True)\n",
    "#         print(\"act_prediction:\", pred_action.detach().numpy())\n",
    "#         print(\"prediction: \", prediction)\n",
    "#         print(\"prediction shaPE: \", prediction.shape)\n",
    "#         print(\"prediction type: \", type(prediction))\n",
    "#         print(\"prediction dtype: \", prediction.dtype)\n",
    "        \n",
    "        criterion = torch.nn.MSELoss()\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    BATCH_SIZE = 1000\n",
    "    NUM_EPOCH = 3\n",
    "    GAMMA = 0.9\n",
    "    batch_size = min(len(buffer), BATCH_SIZE)\n",
    "    random.shuffle(buffer)\n",
    "    # Split the buffer into batches\n",
    "    batches = [\n",
    "      buffer[batch_size * start : batch_size * (start + 1)]\n",
    "      for start in range(int(len(buffer) / batch_size))\n",
    "    ]\n",
    "    for _ in range(NUM_EPOCH):\n",
    "      for batch in batches:\n",
    "        # Create the Tensors that will be fed in the network\n",
    "        obs = torch.from_numpy(np.stack([ex.obs for ex in batch]))\n",
    "        reward = torch.from_numpy(\n",
    "          np.array([ex.reward for ex in batch], dtype=np.float32).reshape(-1, 1)\n",
    "        )\n",
    "        done = torch.from_numpy(\n",
    "          np.array([ex.done for ex in batch], dtype=np.float32).reshape(-1, 1)\n",
    "        )\n",
    "        action = torch.from_numpy(np.stack([ex.action for ex in batch]))\n",
    "        next_obs = torch.from_numpy(np.stack([ex.next_obs for ex in batch]))\n",
    "        \n",
    "        # Prerequisite: collect outputs\n",
    "        pnext_a1, pnext_a2, pnext_a3 = q_net(next_obs)\n",
    "        p_a1, p_a2, p_a3 = q_net(obs)\n",
    "        \n",
    "        # bellman equation for each loss\n",
    "        loss1 = calculate_bellman_loss(pnext_a1, p_a1, reward, done, GAMMA, batch, action_size, action[:, 0])\n",
    "        loss2 = calculate_bellman_loss(pnext_a2, p_a2, reward, done, GAMMA, batch, action_size, action[:, 1])\n",
    "        loss3 = calculate_bellman_loss(pnext_a3, p_a3, reward, done, GAMMA, batch, action_size, action[:, 2])\n",
    "        loss = loss1 + loss2 + loss3\n",
    "        \n",
    "        # Perform the backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridWorld environment created.\n",
      "Training step  1 \treward  -0.7778525451819102\n",
      "\n",
      "Training step  2 \treward  0.0\n",
      "\n",
      "Training step  3 \treward  76.12729744116466\n",
      "\n",
      "Training step  4 \treward  58.666186183691025\n",
      "\n",
      "Training step  5 \treward  15.047052213123866\n",
      "\n",
      "Training step  6 \treward  39.25225846469402\n",
      "\n",
      "Training step  7 \treward  41.230863124132156\n",
      "\n",
      "Training step  8 \treward  46.351544896761574\n",
      "\n",
      "Training step  9 \treward  4.609278580877516\n",
      "\n",
      "Training step  10 \treward  39.16591825485229\n",
      "\n",
      "Training step  11 \treward  7.232579523987241\n",
      "\n",
      "Training step  12 \treward  54.03759209314982\n",
      "\n",
      "Training step  13 \treward  33.48847806453705\n",
      "\n",
      "Training step  14 \treward  35.784628450870514\n",
      "\n",
      "Training step  15 \treward  98.49346090753873\n",
      "\n",
      "Training step  16 \treward  0.0\n",
      "\n",
      "Training step  17 \treward  69.65252600610256\n",
      "\n",
      "Training step  18 \treward  63.15271744132042\n",
      "\n",
      "Training step  19 \treward  -1.0\n",
      "\n",
      "Training step  20 \treward  59.693495362997055\n",
      "\n",
      "Training step  21 \treward  7.090074375271797\n",
      "\n",
      "Training step  22 \treward  38.49213224649429\n",
      "\n",
      "Training step  23 \treward  36.264944434165955\n",
      "\n",
      "Training step  24 \treward  38.608872309327126\n",
      "\n",
      "Training step  25 \treward  49.43099117279053\n",
      "\n",
      "Training step  26 \treward  48.31121149659157\n",
      "\n",
      "Training step  27 \treward  0.0\n",
      "\n",
      "Training step  28 \treward  36.47929050773382\n",
      "\n",
      "Training step  29 \treward  0.0\n",
      "\n",
      "Training step  30 \treward  15.351161109076607\n",
      "\n",
      "Training step  31 \treward  3.8767850346035426\n",
      "\n",
      "Training step  32 \treward  50.219058573246\n",
      "\n",
      "Training step  33 \treward  31.815795528888703\n",
      "\n",
      "Training step  34 \treward  38.26199218630791\n",
      "\n",
      "Training step  35 \treward  34.95854891836643\n",
      "\n",
      "Training step  36 \treward  65.71682883054018\n",
      "\n",
      "Training step  37 \treward  45.31102045377096\n",
      "\n",
      "Training step  38 \treward  103.06348279118538\n",
      "\n",
      "Training step  39 \treward  11.979228508472442\n",
      "\n",
      "Training step  40 \treward  0.0\n",
      "\n",
      "Training step  41 \treward  -1.0\n",
      "\n",
      "Training step  42 \treward  -1.0\n",
      "\n",
      "Training step  43 \treward  147.66764569282532\n",
      "\n",
      "Training step  44 \treward  0.0\n",
      "\n",
      "Training step  45 \treward  0.0\n",
      "\n",
      "Training step  46 \treward  39.541909366846085\n",
      "\n",
      "Training step  47 \treward  38.60347709059715\n",
      "\n",
      "Training step  48 \treward  63.05210368831953\n",
      "\n",
      "Training step  49 \treward  33.37760556936264\n",
      "\n",
      "Training step  50 \treward  32.38615361849467\n",
      "\n",
      "Training step  51 \treward  47.95145685970783\n",
      "\n",
      "Training step  52 \treward  37.69612966477871\n",
      "\n",
      "Training step  53 \treward  37.537167087197304\n",
      "\n",
      "Training step  54 \treward  36.13323771953583\n",
      "\n",
      "Training step  55 \treward  39.927410051226616\n",
      "\n",
      "Training step  56 \treward  45.61880590518316\n",
      "\n",
      "Training step  57 \treward  39.09704717000326\n",
      "\n",
      "Training step  58 \treward  41.845175951719284\n",
      "\n",
      "Training step  59 \treward  43.15473058074713\n",
      "\n",
      "Training step  60 \treward  36.773511519034706\n",
      "\n",
      "Training step  61 \treward  29.501304790377617\n",
      "\n",
      "Training step  62 \treward  42.63144667446613\n",
      "\n",
      "Training step  63 \treward  67.90684840083122\n",
      "\n",
      "Training step  64 \treward  -1.0\n",
      "\n",
      "Training step  65 \treward  -1.0\n",
      "\n",
      "Training step  66 \treward  32.766370475292206\n",
      "\n",
      "Training step  67 \treward  43.40052016079426\n",
      "\n",
      "Training step  68 \treward  0.0\n",
      "\n",
      "Training step  69 \treward  8.297897573312124\n",
      "\n",
      "Training step  70 \treward  6.302176813284556\n",
      "\n",
      "Training step  71 \treward  0.10429844260215759\n",
      "\n",
      "Training step  72 \treward  54.23303288221359\n",
      "\n",
      "Training step  73 \treward  0.27774498611688614\n",
      "\n",
      "Training step  74 \treward  24.399069874154197\n",
      "\n",
      "Training step  75 \treward  86.66351464390755\n",
      "\n",
      "Training step  76 \treward  61.747283364666835\n",
      "\n",
      "Training step  77 \treward  0.0\n",
      "\n",
      "Training step  78 \treward  35.781152188777924\n",
      "\n",
      "Training step  79 \treward  26.870683774352074\n",
      "\n",
      "Training step  80 \treward  6.803549131419923\n",
      "\n",
      "Training step  81 \treward  7.334648132324219\n",
      "\n",
      "Training step  82 \treward  103.4859724342823\n",
      "\n",
      "Training step  83 \treward  10.1964121311903\n",
      "\n",
      "Training step  84 \treward  0.0\n",
      "\n",
      "Training step  85 \treward  0.0\n",
      "\n",
      "Training step  86 \treward  0.0\n",
      "\n",
      "Training step  87 \treward  0.0\n",
      "\n",
      "Training step  88 \treward  25.247392237186432\n",
      "\n",
      "Training step  89 \treward  0.7905239873462253\n",
      "\n",
      "Training step  90 \treward  -1.0\n",
      "\n",
      "Training step  91 \treward  -1.0\n",
      "\n",
      "Training step  92 \treward  -1.0\n",
      "\n",
      "Training step  93 \treward  0.0\n",
      "\n",
      "Training step  94 \treward  -1.0\n",
      "\n",
      "Training step  95 \treward  0.0\n",
      "\n",
      "Training step  96 \treward  0.0\n",
      "\n",
      "Training step  97 \treward  0.0\n",
      "\n",
      "Training step  98 \treward  0.0\n",
      "\n",
      "Training step  99 \treward  0.9754130886660681\n",
      "\n",
      "Training step  100 \treward  0.0\n",
      "\n",
      "Training step  101 \treward  0.0\n",
      "\n",
      "Training step  102 \treward  7.503574857446883\n",
      "\n",
      "Training step  103 \treward  -1.0\n",
      "\n",
      "Training step  104 \treward  24.507884583208295\n",
      "\n",
      "Training step  105 \treward  2.838064946068658\n",
      "\n",
      "Training step  106 \treward  9.627154738373227\n",
      "\n",
      "Training step  107 \treward  129.9369159936905\n",
      "\n",
      "Training step  108 \treward  426.9838281273842\n",
      "\n",
      "Training step  109 \treward  0.0\n",
      "\n",
      "Training step  110 \treward  3.895026324854957\n",
      "\n",
      "Training step  111 \treward  0.0\n",
      "\n",
      "Training step  112 \treward  0.0\n",
      "\n",
      "Training step  113 \treward  0.0\n",
      "\n",
      "Training step  114 \treward  0.0\n",
      "\n",
      "Training step  115 \treward  0.0\n",
      "\n",
      "Training step  116 \treward  19.62901733716329\n",
      "\n",
      "Training step  117 \treward  13.49290992948744\n",
      "\n",
      "Training step  118 \treward  143.7493570446968\n",
      "\n",
      "Training step  119 \treward  292.48338943719864\n",
      "\n",
      "Training step  120 \treward  -1.0\n",
      "\n",
      "Training step  121 \treward  39.24751730759939\n",
      "\n",
      "Training step  122 \treward  363.1276378929615\n",
      "\n",
      "Training step  123 \treward  0.0\n",
      "\n",
      "Training step  124 \treward  55.20510903994242\n",
      "\n",
      "Training step  125 \treward  36.73527765274048\n",
      "\n",
      "Training step  126 \treward  37.64358693361282\n",
      "\n",
      "Training step  127 \treward  115.13870245218277\n",
      "\n",
      "Training step  128 \treward  144.1186535358429\n",
      "\n",
      "Training step  129 \treward  0.0\n",
      "\n",
      "Training step  130 \treward  0.0\n",
      "\n",
      "Training step  131 \treward  -1.0\n",
      "\n",
      "Training step  132 \treward  0.0\n",
      "\n",
      "Training step  133 \treward  92.76138043403625\n",
      "\n",
      "Training step  134 \treward  76.65985262393951\n",
      "\n",
      "Training step  135 \treward  3.1870031555493674\n",
      "\n",
      "Training step  136 \treward  30.05356976389885\n",
      "\n",
      "Training step  137 \treward  52.66314244270325\n",
      "\n",
      "Training step  138 \treward  28.99589277803898\n",
      "\n",
      "Training step  139 \treward  8.25197647511959\n",
      "\n",
      "Training step  140 \treward  112.33663634955883\n",
      "\n",
      "Training step  141 \treward  -1.0\n",
      "\n",
      "Training step  142 \treward  -1.0\n",
      "\n",
      "Training step  143 \treward  -1.0\n",
      "\n",
      "Training step  144 \treward  -1.0\n",
      "\n",
      "Training step  145 \treward  -1.0\n",
      "\n",
      "Training step  146 \treward  -1.0\n",
      "\n",
      "Training step  147 \treward  3.2187816076808504\n",
      "\n",
      "Training step  148 \treward  0.0\n",
      "\n",
      "Training step  149 \treward  0.0\n",
      "\n",
      "Training step  150 \treward  38.1317093345854\n",
      "\n",
      "Training step  151 \treward  181.17014479637146\n",
      "\n",
      "Training step  152 \treward  0.0\n",
      "\n",
      "Training step  153 \treward  0.0\n",
      "\n",
      "Training step  154 \treward  0.0\n",
      "\n",
      "Training step  155 \treward  0.0\n",
      "\n",
      "Training step  156 \treward  0.0\n",
      "\n",
      "Training step  157 \treward  0.0\n",
      "\n",
      "Training step  158 \treward  0.0\n",
      "\n",
      "Training step  159 \treward  0.0\n",
      "\n",
      "Training step  160 \treward  -1.0\n",
      "\n",
      "Training step  161 \treward  0.0\n",
      "\n",
      "Training step  162 \treward  0.0\n",
      "\n",
      "Training step  163 \treward  0.0\n",
      "\n",
      "Training step  164 \treward  0.0\n",
      "\n",
      "Training step  165 \treward  0.0\n",
      "\n",
      "Training step  166 \treward  0.0\n",
      "\n",
      "Training step  167 \treward  0.0\n",
      "\n",
      "Training step  168 \treward  3.0497111797332765\n",
      "\n",
      "Training step  169 \treward  0.0\n",
      "\n",
      "Training step  170 \treward  0.0\n",
      "\n",
      "Training step  171 \treward  0.0\n",
      "\n",
      "Training step  172 \treward  0.0\n",
      "\n",
      "Training step  173 \treward  0.0\n",
      "\n",
      "Training step  174 \treward  1.9989462812741599\n",
      "\n",
      "Training step  175 \treward  0.0\n",
      "\n",
      "Training step  176 \treward  -1.0\n",
      "\n",
      "Training step  177 \treward  -1.0\n",
      "\n",
      "Training step  178 \treward  0.10998857021331787\n",
      "\n",
      "Training step  179 \treward  1.9765601489279006\n",
      "\n",
      "Training step  180 \treward  0.6171103762255774\n",
      "\n",
      "Training step  181 \treward  0.0\n",
      "\n",
      "Training step  182 \treward  0.0\n",
      "\n",
      "Training step  183 \treward  0.0\n",
      "\n",
      "Training step  184 \treward  0.0\n",
      "\n",
      "Training step  185 \treward  1.4578494323624505\n",
      "\n",
      "Training step  186 \treward  -1.0\n",
      "\n",
      "Training step  187 \treward  -1.0\n",
      "\n",
      "Training step  188 \treward  0.0\n",
      "\n",
      "Training step  189 \treward  -1.0\n",
      "\n",
      "Training step  190 \treward  -1.0\n",
      "\n",
      "Training step  191 \treward  -1.0\n",
      "\n",
      "Training step  192 \treward  0.0\n",
      "\n",
      "Training step  193 \treward  -1.0\n",
      "\n",
      "Training step  194 \treward  -1.0\n",
      "\n",
      "Training step  195 \treward  0.0\n",
      "\n",
      "Training step  196 \treward  0.0\n",
      "\n",
      "Training step  197 \treward  0.0\n",
      "\n",
      "Training step  198 \treward  0.0\n",
      "\n",
      "Training step  199 \treward  0.0\n",
      "\n",
      "Training step  200 \treward  0.0\n",
      "\n",
      "Training step  201 \treward  0.0\n",
      "\n",
      "Training step  202 \treward  3.3818991422653197\n",
      "\n",
      "Training step  203 \treward  11.890174372990925\n",
      "\n",
      "Training step  204 \treward  6.178642776939604\n",
      "\n",
      "Training step  205 \treward  -1.0\n",
      "\n",
      "Training step  206 \treward  10.950029283761978\n",
      "\n",
      "Training step  207 \treward  -1.0\n",
      "\n",
      "Training step  208 \treward  -1.0\n",
      "\n",
      "Training step  209 \treward  -1.0\n",
      "\n",
      "Training step  210 \treward  25.96223467787107\n",
      "\n",
      "Training step  211 \treward  0.0\n",
      "\n",
      "Training step  212 \treward  -1.0\n",
      "\n",
      "Training step  213 \treward  -1.0\n",
      "\n",
      "Training step  214 \treward  -1.0\n",
      "\n",
      "Training step  215 \treward  -1.0\n",
      "\n",
      "Training step  216 \treward  -1.0\n",
      "\n",
      "Training step  217 \treward  0.0\n",
      "\n",
      "Training step  218 \treward  -1.0\n",
      "\n",
      "Training step  219 \treward  1.8477524280548097\n",
      "\n",
      "Training step  220 \treward  0.0\n",
      "\n",
      "Training step  221 \treward  0.0\n",
      "\n",
      "Training step  222 \treward  -1.0\n",
      "\n",
      "Training step  223 \treward  -1.0\n",
      "\n",
      "Training step  224 \treward  0.0\n",
      "\n",
      "Training step  225 \treward  -1.0\n",
      "\n",
      "Training step  226 \treward  0.0\n",
      "\n",
      "Training step  227 \treward  0.2373275385962592\n",
      "\n",
      "Training step  228 \treward  0.0\n",
      "\n",
      "Training step  229 \treward  -1.0\n",
      "\n",
      "Training step  230 \treward  1.5324482470750809\n",
      "\n",
      "Training step  231 \treward  -1.0\n",
      "\n",
      "Training step  232 \treward  -1.0\n",
      "\n",
      "Training step  233 \treward  0.5054527074098587\n",
      "\n",
      "Training step  234 \treward  0.0\n",
      "\n",
      "Training step  235 \treward  -1.0\n",
      "\n",
      "Training step  236 \treward  -1.0\n",
      "\n",
      "Training step  237 \treward  -1.0\n",
      "\n",
      "Training step  238 \treward  -1.0\n",
      "\n",
      "Training step  239 \treward  0.0\n",
      "\n",
      "Training step  240 \treward  -1.0\n",
      "\n",
      "Training step  241 \treward  0.0\n",
      "\n",
      "Training step  242 \treward  0.0\n",
      "\n",
      "Training step  243 \treward  0.0\n",
      "\n",
      "Training step  244 \treward  0.0\n",
      "\n",
      "Training step  245 \treward  1.0763858199119567\n",
      "\n",
      "Training step  246 \treward  0.0\n",
      "\n",
      "Training step  247 \treward  0.0\n",
      "\n",
      "Training step  248 \treward  0.0\n",
      "\n",
      "Training step  249 \treward  0.0\n",
      "\n",
      "Training step  250 \treward  0.0\n",
      "\n",
      "Training step  251 \treward  0.0\n",
      "\n",
      "Training step  252 \treward  0.0\n",
      "\n",
      "Training step  253 \treward  0.0\n",
      "\n",
      "Training step  254 \treward  0.0\n",
      "\n",
      "Training step  255 \treward  0.0\n",
      "\n",
      "Training step  256 \treward  0.0\n",
      "\n",
      "Training step  257 \treward  0.0\n",
      "\n",
      "Training step  258 \treward  0.0\n",
      "\n",
      "Training step  259 \treward  0.5681565648979611\n",
      "\n",
      "Training step  260 \treward  0.0\n",
      "\n",
      "Training step  261 \treward  0.0\n",
      "\n",
      "Training step  262 \treward  0.024093428585264417\n",
      "\n",
      "Training step  263 \treward  0.0\n",
      "\n",
      "Training step  264 \treward  0.0\n",
      "\n",
      "Training step  265 \treward  -1.0\n",
      "\n",
      "Training step  266 \treward  80.37385685178968\n",
      "\n",
      "Training step  267 \treward  38.71254754066467\n",
      "\n",
      "Training step  268 \treward  453.2926226258278\n",
      "\n",
      "Training step  269 \treward  575.2777989506721\n",
      "\n",
      "Training step  270 \treward  1.098910829755995\n",
      "\n",
      "Training step  271 \treward  313.85529014468193\n",
      "\n",
      "Training step  272 \treward  0.0\n",
      "\n",
      "Training step  273 \treward  1.3566349645455678\n",
      "\n",
      "Training step  274 \treward  112.46629890799522\n",
      "\n",
      "Training step  275 \treward  103.41953912642268\n",
      "\n",
      "Training step  276 \treward  0.0\n",
      "\n",
      "Training step  277 \treward  131.14159053564072\n",
      "\n",
      "Training step  278 \treward  -1.0\n",
      "\n",
      "Training step  279 \treward  -1.0\n",
      "\n",
      "Training step  280 \treward  61.949396000968086\n",
      "\n",
      "Training step  281 \treward  -1.0\n",
      "\n",
      "Training step  282 \treward  10.015027936299642\n",
      "\n",
      "Training step  283 \treward  0.0\n",
      "\n",
      "Training step  284 \treward  0.0\n",
      "\n",
      "Training step  285 \treward  -1.0\n",
      "\n",
      "Training step  286 \treward  0.0\n",
      "\n",
      "Training step  287 \treward  -1.0\n",
      "\n",
      "Training step  288 \treward  -1.0\n",
      "\n",
      "Training step  289 \treward  -1.0\n",
      "\n",
      "Training step  290 \treward  7.191493146949345\n",
      "\n",
      "Training step  291 \treward  0.0\n",
      "\n",
      "Training step  292 \treward  0.0\n",
      "\n",
      "Training step  293 \treward  0.0\n",
      "\n",
      "Training step  294 \treward  0.0\n",
      "\n",
      "Training step  295 \treward  0.0\n",
      "\n",
      "Training step  296 \treward  22.609358537197114\n",
      "\n",
      "Training step  297 \treward  -1.0\n",
      "\n",
      "Training step  298 \treward  16.39293205142021\n",
      "\n",
      "Training step  299 \treward  0.0\n",
      "\n",
      "Training step  300 \treward  37.63790982961655\n",
      "\n",
      "Training step  301 \treward  0.0\n",
      "\n",
      "Training step  302 \treward  281.77516627311707\n",
      "\n",
      "Training step  303 \treward  0.0\n",
      "\n",
      "Training step  304 \treward  30.456289013226826\n",
      "\n",
      "Training step  305 \treward  37.31929496924082\n",
      "\n",
      "Training step  306 \treward  0.0\n",
      "\n",
      "Training step  307 \treward  39.842172265052795\n",
      "\n",
      "Training step  308 \treward  40.473242342472076\n",
      "\n",
      "Training step  309 \treward  164.56139221787453\n",
      "\n",
      "Training step  310 \treward  38.13047310709953\n",
      "\n",
      "Training step  311 \treward  71.60756753385067\n",
      "\n",
      "Training step  312 \treward  39.80399516224861\n",
      "\n",
      "Training step  313 \treward  34.157754987478256\n",
      "\n",
      "Training step  314 \treward  36.9982727766037\n",
      "\n",
      "Training step  315 \treward  0.0\n",
      "\n",
      "Training step  316 \treward  37.031626760959625\n",
      "\n",
      "Training step  317 \treward  344.1529638171196\n",
      "\n",
      "Training step  318 \treward  -1.0\n",
      "\n",
      "Training step  319 \treward  70.97140742838383\n",
      "\n",
      "Training step  320 \treward  568.9746616482735\n",
      "\n",
      "Training step  321 \treward  30.71823364496231\n",
      "\n",
      "Training step  322 \treward  43.589130322138466\n",
      "\n",
      "Training step  323 \treward  46.14339021593332\n",
      "\n",
      "Training step  324 \treward  69.30002999305725\n",
      "\n",
      "Training step  325 \treward  86.91014760732651\n",
      "\n",
      "Training step  326 \treward  636.6338849067688\n",
      "\n",
      "Training step  327 \treward  647.4763020277023\n",
      "\n",
      "Training step  328 \treward  336.93877732753754\n",
      "\n",
      "Training step  329 \treward  48.52688620984554\n",
      "\n",
      "Training step  330 \treward  282.0300169289112\n",
      "\n",
      "Training step  331 \treward  36.564742743968964\n",
      "\n",
      "Training step  332 \treward  43.0715307444334\n",
      "\n",
      "Training step  333 \treward  37.015987038612366\n",
      "\n",
      "Training step  334 \treward  47.41598841547966\n",
      "\n",
      "Training step  335 \treward  349.65902200341225\n",
      "\n",
      "Training step  336 \treward  -1.0\n",
      "\n",
      "Training step  337 \treward  38.43863555788994\n",
      "\n",
      "Training step  338 \treward  -1.0\n",
      "\n",
      "Training step  339 \treward  484.85487562417984\n",
      "\n",
      "Training step  340 \treward  585.840885579586\n",
      "\n",
      "Training step  341 \treward  510.81537199020386\n",
      "\n",
      "Training step  342 \treward  509.06602692604065\n",
      "\n",
      "Training step  343 \treward  -1.0\n",
      "\n",
      "Training step  344 \treward  79.35701107978821\n",
      "\n",
      "Training step  345 \treward  196.9070476690928\n",
      "\n",
      "Training step  346 \treward  32.23788098990917\n",
      "\n",
      "Training step  347 \treward  52.96599471569061\n",
      "\n",
      "Training step  348 \treward  182.12578609585762\n",
      "\n",
      "Training step  349 \treward  -1.0\n",
      "\n",
      "Training step  350 \treward  650.6923580765724\n",
      "\n",
      "Training step  351 \treward  -1.0\n",
      "\n",
      "Training step  352 \treward  106.85704243183136\n",
      "\n",
      "Training step  353 \treward  0.6928030252456665\n",
      "\n",
      "Training step  354 \treward  119.60986256599426\n",
      "\n",
      "Training step  355 \treward  341.56946790218353\n",
      "\n",
      "Training step  356 \treward  261.17585358023643\n",
      "\n",
      "Training step  357 \treward  3.076436385512352\n",
      "\n",
      "Training step  358 \treward  -0.724165270725886\n",
      "\n",
      "Training step  359 \treward  66.37984232604504\n",
      "\n",
      "Training step  360 \treward  0.3944973200559616\n",
      "\n",
      "Training step  361 \treward  160.27207905054092\n",
      "\n",
      "Training step  362 \treward  65.74247334897518\n",
      "\n",
      "Training step  363 \treward  315.1206154823303\n",
      "\n",
      "Training step  364 \treward  117.16640359163284\n",
      "\n",
      "Training step  365 \treward  625.3955500125885\n",
      "\n",
      "Training step  366 \treward  621.0172822475433\n",
      "\n",
      "Training step  367 \treward  646.6180044412613\n",
      "\n",
      "Training step  368 \treward  637.0279489159584\n",
      "\n",
      "Training step  369 \treward  641.8253099918365\n",
      "\n",
      "Training step  370 \treward  650.663899064064\n",
      "\n",
      "Training step  371 \treward  640.7467554807663\n",
      "\n",
      "Training step  372 \treward  643.3711236715317\n",
      "\n",
      "Training step  373 \treward  611.750389277935\n",
      "\n",
      "Training step  374 \treward  646.284261226654\n",
      "\n",
      "Training step  375 \treward  646.1943092942238\n",
      "\n",
      "Training step  376 \treward  644.4693723320961\n",
      "\n",
      "Training step  377 \treward  645.9579781293869\n",
      "\n",
      "Training step  378 \treward  646.8549913167953\n",
      "\n",
      "Training step  379 \treward  645.1335991024971\n",
      "\n",
      "Training step  380 \treward  638.1672407388687\n",
      "\n",
      "Training step  381 \treward  649.9737899899483\n",
      "\n",
      "Training step  382 \treward  640.4371318817139\n",
      "\n",
      "Training step  383 \treward  646.3083387613297\n",
      "\n",
      "Training step  384 \treward  641.69308334589\n",
      "\n",
      "Training step  385 \treward  645.2355836033821\n",
      "\n",
      "Training step  386 \treward  641.385368347168\n",
      "\n",
      "Training step  387 \treward  642.9963486194611\n",
      "\n",
      "Training step  388 \treward  641.0695343017578\n",
      "\n",
      "Training step  389 \treward  640.48610663414\n",
      "\n",
      "Training step  390 \treward  648.5836416482925\n",
      "\n",
      "Training step  391 \treward  647.3510776758194\n",
      "\n",
      "Training step  392 \treward  652.8239867687225\n",
      "\n",
      "Training step  393 \treward  642.2860093712807\n",
      "\n",
      "Training step  394 \treward  637.474134683609\n",
      "\n",
      "Training step  395 \treward  639.6730426549911\n",
      "\n",
      "Training step  396 \treward  647.1545147895813\n",
      "\n",
      "Training step  397 \treward  639.2871649265289\n",
      "\n",
      "Training step  398 \treward  638.589962720871\n",
      "\n",
      "Training step  399 \treward  644.965913772583\n",
      "\n",
      "Training step  400 \treward  647.2184364199638\n",
      "\n",
      "Training step  401 \treward  632.8057252168655\n",
      "\n",
      "Training step  402 \treward  642.326964199543\n",
      "\n",
      "Training step  403 \treward  647.3311114311218\n",
      "\n",
      "Training step  404 \treward  639.4545271992683\n",
      "\n",
      "Training step  405 \treward  645.965241253376\n",
      "\n",
      "Training step  406 \treward  647.1655979752541\n",
      "\n",
      "Training step  407 \treward  648.0469189882278\n",
      "\n",
      "Training step  408 \treward  646.2404726743698\n",
      "\n",
      "Training step  409 \treward  636.9706757068634\n",
      "\n",
      "Training step  410 \treward  116.02803248167038\n",
      "\n",
      "Training step  411 \treward  656.6529177427292\n",
      "\n",
      "Training step  412 \treward  3.7609743624925613\n",
      "\n",
      "Training step  413 \treward  7.893938489258289\n",
      "\n",
      "Training step  414 \treward  542.5199322104454\n",
      "\n",
      "Training step  415 \treward  683.5549232959747\n",
      "\n",
      "Training step  416 \treward  709.3830244541168\n",
      "\n",
      "Training step  417 \treward  693.4629203677177\n",
      "\n",
      "Training step  418 \treward  716.5715618133545\n",
      "\n",
      "Training step  419 \treward  0.3979865743054284\n",
      "\n",
      "Training step  420 \treward  -1.0\n",
      "\n",
      "Training step  421 \treward  0.0\n",
      "\n",
      "Training step  422 \treward  714.1877101659775\n",
      "\n",
      "Training step  423 \treward  711.052568256855\n",
      "\n",
      "Training step  424 \treward  -1.0\n",
      "\n",
      "Training step  425 \treward  77.06842425929176\n",
      "\n",
      "Training step  426 \treward  170.82627002398172\n",
      "\n",
      "Training step  427 \treward  731.6960101127625\n",
      "\n",
      "Training step  428 \treward  738.4438339471817\n",
      "\n",
      "Training step  429 \treward  726.0777770280838\n",
      "\n",
      "Training step  430 \treward  717.1479477882385\n",
      "\n",
      "Training step  431 \treward  735.7006349563599\n",
      "\n",
      "Training step  432 \treward  735.8314226269722\n",
      "\n",
      "Training step  433 \treward  729.1834635138512\n",
      "\n",
      "Training step  434 \treward  744.5967901945114\n",
      "\n",
      "Training step  435 \treward  743.3980693817139\n",
      "\n",
      "Training step  436 \treward  738.2007903456688\n",
      "\n",
      "Training step  437 \treward  731.2899722456932\n",
      "\n",
      "Training step  438 \treward  658.2912575006485\n",
      "\n",
      "Training step  439 \treward  669.8174224495888\n",
      "\n",
      "Training step  440 \treward  647.4583081007004\n",
      "\n",
      "Training step  441 \treward  14.122014649709065\n",
      "\n",
      "Training step  442 \treward  72.37358060412937\n",
      "\n",
      "Training step  443 \treward  -1.0\n",
      "\n",
      "Training step  444 \treward  -1.0\n",
      "\n",
      "Training step  445 \treward  -1.0\n",
      "\n",
      "Training step  446 \treward  0.0\n",
      "\n",
      "Training step  447 \treward  0.0\n",
      "\n",
      "Training step  448 \treward  0.0\n",
      "\n",
      "Training step  449 \treward  0.0\n",
      "\n",
      "Training step  450 \treward  736.0212963819504\n",
      "\n",
      "Training step  451 \treward  0.0\n",
      "\n",
      "Training step  452 \treward  -1.0\n",
      "\n",
      "Training step  453 \treward  0.0\n",
      "\n",
      "Training step  454 \treward  45.47124860419167\n",
      "\n",
      "Training step  455 \treward  0.0\n",
      "\n",
      "Training step  456 \treward  29.81025121079551\n",
      "\n",
      "Training step  457 \treward  0.0\n",
      "\n",
      "Training step  458 \treward  0.0\n",
      "\n",
      "Training step  459 \treward  0.0\n",
      "\n",
      "Training step  460 \treward  0.0\n",
      "\n",
      "Training step  461 \treward  0.0\n",
      "\n",
      "Training step  462 \treward  0.0\n",
      "\n",
      "Training step  463 \treward  0.0\n",
      "\n",
      "Training step  464 \treward  0.0\n",
      "\n",
      "Training step  465 \treward  0.0\n",
      "\n",
      "Training step  466 \treward  0.0\n",
      "\n",
      "Training step  467 \treward  0.0\n",
      "\n",
      "Training step  468 \treward  2.8721693224377103\n",
      "\n",
      "Training step  469 \treward  -1.0\n",
      "\n",
      "Training step  470 \treward  21.350622966885567\n",
      "\n",
      "Training step  471 \treward  17.940597355365753\n",
      "\n",
      "Training step  472 \treward  159.0915968378385\n",
      "\n",
      "Training step  473 \treward  663.604354262352\n",
      "\n",
      "Training step  474 \treward  97.96261434025234\n",
      "\n",
      "Training step  475 \treward  112.74751968913608\n",
      "\n",
      "Training step  476 \treward  169.69188884761599\n",
      "\n",
      "Training step  477 \treward  102.42449642817179\n",
      "\n",
      "Training step  478 \treward  108.89031380547418\n",
      "\n",
      "Training step  479 \treward  211.78469540543026\n",
      "\n",
      "Training step  480 \treward  279.0937110490269\n",
      "\n",
      "Training step  481 \treward  746.4728392362595\n",
      "\n",
      "Training step  482 \treward  748.2925604581833\n",
      "\n",
      "Training step  483 \treward  751.5828102827072\n",
      "\n",
      "Training step  484 \treward  237.17046517464848\n",
      "\n",
      "Training step  485 \treward  82.612235127555\n",
      "\n",
      "Training step  486 \treward  61.464460331863826\n",
      "\n",
      "Training step  487 \treward  100.59436211188634\n",
      "\n",
      "Training step  488 \treward  393.33587980270386\n",
      "\n",
      "Training step  489 \treward  92.39019109540516\n",
      "\n",
      "Training step  490 \treward  739.3520728349686\n",
      "\n",
      "Training step  491 \treward  73.63879754278395\n",
      "\n",
      "Training step  492 \treward  748.7634845376015\n",
      "\n",
      "Training step  493 \treward  27.24335378408432\n",
      "\n",
      "Training step  494 \treward  161.82867351174355\n",
      "\n",
      "Training step  495 \treward  24.716294771432878\n",
      "\n",
      "Training step  496 \treward  33.8086523214976\n",
      "\n",
      "Training step  497 \treward  105.3908800025781\n",
      "\n",
      "Training step  498 \treward  154.83623069326083\n",
      "\n",
      "Training step  499 \treward  91.52752671771579\n",
      "\n",
      "Training step  500 \treward  72.00222986936569\n",
      "\n",
      "Training step  501 \treward  7.6976801143752205\n",
      "\n",
      "Training step  502 \treward  31.675688099861144\n",
      "\n",
      "Training step  503 \treward  13.718524267938403\n",
      "\n",
      "Training step  504 \treward  384.4158967137337\n",
      "\n",
      "Training step  505 \treward  4.407354142930773\n",
      "\n",
      "Training step  506 \treward  -1.0\n",
      "\n",
      "Training step  507 \treward  35.301034945249555\n",
      "\n",
      "Training step  508 \treward  212.68578392267227\n",
      "\n",
      "Training step  509 \treward  6.19052317738533\n",
      "\n",
      "Training step  510 \treward  115.9771240055561\n",
      "\n",
      "Training step  511 \treward  4.343410703870985\n",
      "\n",
      "Training step  512 \treward  318.2737247347832\n",
      "\n",
      "Training step  513 \treward  0.0\n",
      "\n",
      "Training step  514 \treward  25.146279335021973\n",
      "\n",
      "Training step  515 \treward  0.0\n",
      "\n",
      "Training step  516 \treward  0.0\n",
      "\n",
      "Training step  517 \treward  0.0\n",
      "\n",
      "Training step  518 \treward  0.341697977648841\n",
      "\n",
      "Training step  519 \treward  1.7715016424655914\n",
      "\n",
      "Training step  520 \treward  0.309315201971266\n",
      "\n",
      "Training step  521 \treward  0.0\n",
      "\n",
      "Training step  522 \treward  0.0\n",
      "\n",
      "Training step  523 \treward  0.0\n",
      "\n",
      "Training step  524 \treward  0.0\n",
      "\n",
      "Training step  525 \treward  0.0\n",
      "\n",
      "Training step  526 \treward  6.109198990795347\n",
      "\n",
      "Training step  527 \treward  0.0\n",
      "\n",
      "Training step  528 \treward  6.9916753749052685\n",
      "\n",
      "Training step  529 \treward  8.372852652602726\n",
      "\n",
      "Training step  530 \treward  0.0\n",
      "\n",
      "Training step  531 \treward  0.0\n",
      "\n",
      "Training step  532 \treward  2.860362850295173\n",
      "\n",
      "Training step  533 \treward  1.7599634991751776\n",
      "\n",
      "Training step  534 \treward  1.8237482382191552\n",
      "\n",
      "Training step  535 \treward  0.0\n",
      "\n",
      "Training step  536 \treward  0.8823860870467292\n",
      "\n",
      "Training step  537 \treward  4.5527010063330335\n",
      "\n",
      "Training step  538 \treward  0.0\n",
      "\n",
      "Training step  539 \treward  0.0\n",
      "\n",
      "Training step  540 \treward  0.0\n",
      "\n",
      "Training step  541 \treward  0.0\n",
      "\n",
      "Training step  542 \treward  2.5590158462524415\n",
      "\n",
      "Training step  543 \treward  0.0\n",
      "\n",
      "Training step  544 \treward  0.0\n",
      "\n",
      "Training step  545 \treward  0.0\n",
      "\n",
      "Training step  546 \treward  4.911722318993674\n",
      "\n",
      "Training step  547 \treward  0.0\n",
      "\n",
      "Training step  548 \treward  0.0\n",
      "\n",
      "Training step  549 \treward  1.1678644869062635\n",
      "\n",
      "Training step  550 \treward  4.995896487765842\n",
      "\n",
      "Training step  551 \treward  0.5595221824116177\n",
      "\n",
      "Training step  552 \treward  3.0286979834238688\n",
      "\n",
      "Training step  553 \treward  3.136694566408793\n",
      "\n",
      "Training step  554 \treward  0.0\n",
      "\n",
      "Training step  555 \treward  4.6402418838606945\n",
      "\n",
      "Training step  556 \treward  0.0\n",
      "\n",
      "Training step  557 \treward  0.0\n",
      "\n",
      "Training step  558 \treward  1.8319478723737928\n",
      "\n",
      "Training step  559 \treward  0.0\n",
      "\n",
      "Training step  560 \treward  0.0\n",
      "\n",
      "Training step  561 \treward  0.0\n",
      "\n",
      "Training step  562 \treward  0.0\n",
      "\n",
      "Training step  563 \treward  5.682111348046197\n",
      "\n",
      "Training step  564 \treward  0.0\n",
      "\n",
      "Training step  565 \treward  1.8255844434102377\n",
      "\n",
      "Training step  566 \treward  0.0\n",
      "\n",
      "Training step  567 \treward  0.36789018114407857\n",
      "\n",
      "Training step  568 \treward  0.0\n",
      "\n",
      "Training step  569 \treward  3.4157398369577194\n",
      "\n",
      "Training step  570 \treward  3.5156153705385\n",
      "\n",
      "Training step  571 \treward  0.10832174022992452\n",
      "\n",
      "Training step  572 \treward  3.427304294374254\n",
      "\n",
      "Training step  573 \treward  0.0\n",
      "\n",
      "Training step  574 \treward  2.965845623281267\n",
      "\n",
      "Training step  575 \treward  3.2698273175292547\n",
      "\n",
      "Training step  576 \treward  0.016510330968432956\n",
      "\n",
      "Training step  577 \treward  0.39849575493070816\n",
      "\n",
      "Training step  578 \treward  0.7078906582461463\n",
      "\n",
      "Training step  579 \treward  0.0\n",
      "\n",
      "Training step  580 \treward  0.029761085245344373\n",
      "\n",
      "Training step  581 \treward  0.0\n",
      "\n",
      "Training step  582 \treward  3.1991487748093075\n",
      "\n",
      "Training step  583 \treward  0.43796631031566197\n",
      "\n",
      "Training step  584 \treward  2.3792418986558914\n",
      "\n",
      "Training step  585 \treward  0.4177327315012614\n",
      "\n",
      "Training step  586 \treward  0.07865301503075493\n",
      "\n",
      "Training step  587 \treward  0.0\n",
      "\n",
      "Training step  588 \treward  0.0\n",
      "\n",
      "Training step  589 \treward  0.0\n",
      "\n",
      "Training step  590 \treward  0.0\n",
      "\n",
      "Training step  591 \treward  0.0\n",
      "\n",
      "Training step  592 \treward  0.0\n",
      "\n",
      "Training step  593 \treward  0.0\n",
      "\n",
      "Training step  594 \treward  0.2039547754658593\n",
      "\n",
      "Training step  595 \treward  0.18653220931688944\n",
      "\n",
      "Training step  596 \treward  0.15369212759865655\n",
      "\n",
      "Training step  597 \treward  0.6619811673959096\n",
      "\n",
      "Training step  598 \treward  3.5631637122895983\n",
      "\n",
      "Training step  599 \treward  0.4572836822933621\n",
      "\n",
      "Training step  600 \treward  1.2173719293541378\n",
      "\n",
      "Training step  601 \treward  0.6070709182156457\n",
      "\n",
      "Training step  602 \treward  0.5892718176047007\n",
      "\n",
      "Training step  603 \treward  0.0\n",
      "\n",
      "Training step  604 \treward  0.0\n",
      "\n",
      "Training step  605 \treward  3.96047405468093\n",
      "\n",
      "Training step  606 \treward  0.0\n",
      "\n",
      "Training step  607 \treward  5.2435308456420895\n",
      "\n",
      "Training step  608 \treward  0.0\n",
      "\n",
      "Training step  609 \treward  0.0\n",
      "\n",
      "Training step  610 \treward  3.654693373044332\n",
      "\n",
      "Training step  611 \treward  6.078562423917982\n",
      "\n",
      "Training step  612 \treward  -1.0\n",
      "\n",
      "Training step  613 \treward  0.0\n",
      "\n",
      "Training step  614 \treward  3.523270437452528\n",
      "\n",
      "Training step  615 \treward  0.6499051067564222\n",
      "\n",
      "Training step  616 \treward  3.213579723570082\n",
      "\n",
      "Training step  617 \treward  6.373445945315891\n",
      "\n",
      "Training step  618 \treward  8.63037088976966\n",
      "\n",
      "Training step  619 \treward  15.87025709549586\n",
      "\n",
      "Training step  620 \treward  3.4968359920713636\n",
      "\n",
      "Training step  621 \treward  0.0\n",
      "\n",
      "Training step  622 \treward  3.3453008704715304\n",
      "\n",
      "Training step  623 \treward  3.372421317630344\n",
      "\n",
      "Training step  624 \treward  -1.0\n",
      "\n",
      "Training step  625 \treward  0.0\n",
      "\n",
      "Training step  626 \treward  0.0\n",
      "\n",
      "Training step  627 \treward  3.0655178633001117\n",
      "\n",
      "Training step  628 \treward  6.771041649580002\n",
      "\n",
      "Training step  629 \treward  13.49523854719268\n",
      "\n",
      "Training step  630 \treward  99.54889279603958\n",
      "\n",
      "Training step  631 \treward  6.191710366143121\n",
      "\n",
      "Training step  632 \treward  92.87839037179947\n",
      "\n",
      "Training step  633 \treward  37.50509233772755\n",
      "\n",
      "Training step  634 \treward  19.887738415267734\n",
      "\n",
      "Training step  635 \treward  9.297814477814569\n",
      "\n",
      "Training step  636 \treward  15.342347188790638\n",
      "\n",
      "Training step  637 \treward  0.0\n",
      "\n",
      "Training step  638 \treward  0.0\n",
      "\n",
      "Training step  639 \treward  1.9307229889763726\n",
      "\n",
      "Training step  640 \treward  10.404280006885529\n",
      "\n",
      "Training step  641 \treward  149.52707183361053\n",
      "\n",
      "Training step  642 \treward  0.0\n",
      "\n",
      "Training step  643 \treward  14.974723648362689\n",
      "\n",
      "Training step  644 \treward  8.444134616851807\n",
      "\n",
      "Training step  645 \treward  29.521289984385174\n",
      "\n",
      "Training step  646 \treward  27.133310317993164\n",
      "\n",
      "Training step  647 \treward  30.278200070063274\n",
      "\n",
      "Training step  648 \treward  114.5875085969766\n",
      "\n",
      "Training step  649 \treward  3.6606677797105576\n",
      "\n",
      "Training step  650 \treward  21.778796762228012\n",
      "\n",
      "Training step  651 \treward  18.63452708721161\n",
      "\n",
      "Training step  652 \treward  49.799973638852435\n",
      "\n",
      "Training step  653 \treward  27.25399010380109\n",
      "\n",
      "Training step  654 \treward  23.04564030965169\n",
      "\n",
      "Training step  655 \treward  23.67836931347847\n",
      "\n",
      "Training step  656 \treward  27.324011584122974\n",
      "\n",
      "Training step  657 \treward  27.08229332168897\n",
      "\n",
      "Training step  658 \treward  71.88167524337769\n",
      "\n",
      "Training step  659 \treward  34.39908873769972\n",
      "\n",
      "Training step  660 \treward  49.40538001060486\n",
      "\n",
      "Training step  661 \treward  36.739772006869316\n",
      "\n",
      "Training step  662 \treward  28.521256536245346\n",
      "\n",
      "Training step  663 \treward  31.767820194363594\n",
      "\n",
      "Training step  664 \treward  18.04964044690132\n",
      "\n",
      "Training step  665 \treward  24.52878476679325\n",
      "\n",
      "Training step  666 \treward  16.33343569934368\n",
      "\n",
      "Training step  667 \treward  34.68171480298042\n",
      "\n",
      "Training step  668 \treward  33.64116454124451\n",
      "\n",
      "Training step  669 \treward  43.0208583176136\n",
      "\n",
      "Training step  670 \treward  34.81936179101467\n",
      "\n",
      "Training step  671 \treward  13.819816663861275\n",
      "\n",
      "Training step  672 \treward  -1.0\n",
      "\n",
      "Training step  673 \treward  50.067662209272385\n",
      "\n",
      "Training step  674 \treward  1.7384469628334045\n",
      "\n",
      "Training step  675 \treward  -1.0\n",
      "\n",
      "Training step  676 \treward  0.3383844594160716\n",
      "\n",
      "Training step  677 \treward  16.90061219268375\n",
      "\n",
      "Training step  678 \treward  28.689057886600494\n",
      "\n",
      "Training step  679 \treward  12.880465246571434\n",
      "\n",
      "Training step  680 \treward  32.71120697259903\n",
      "\n",
      "Training step  681 \treward  4.599135918749703\n",
      "\n",
      "Training step  682 \treward  9.9822708732552\n",
      "\n",
      "Training step  683 \treward  0.0\n",
      "\n",
      "Training step  684 \treward  4.858007351557414\n",
      "\n",
      "Training step  685 \treward  2.6466067830721536\n",
      "\n",
      "Training step  686 \treward  20.689152508974075\n",
      "\n",
      "Training step  687 \treward  6.414448512925042\n",
      "\n",
      "Training step  688 \treward  42.42562799155712\n",
      "\n",
      "Training step  689 \treward  33.78453987836838\n",
      "\n",
      "Training step  690 \treward  13.09190982580185\n",
      "\n",
      "Training step  691 \treward  0.8879599087768131\n",
      "\n",
      "Training step  692 \treward  32.64575591683388\n",
      "\n",
      "Training step  693 \treward  14.965176887644661\n",
      "\n",
      "Training step  694 \treward  5.263455778360367\n",
      "\n",
      "Training step  695 \treward  12.78110921382904\n",
      "\n",
      "Training step  696 \treward  17.223453372716904\n",
      "\n",
      "Training step  697 \treward  211.72284305095673\n",
      "\n",
      "Training step  698 \treward  189.35785961151123\n",
      "\n",
      "Training step  699 \treward  62.83234766125679\n",
      "\n",
      "Training step  700 \treward  10.970488399267197\n",
      "\n",
      "Training step  701 \treward  191.6103853583336\n",
      "\n",
      "Training step  702 \treward  337.4989011287689\n",
      "\n",
      "Training step  703 \treward  50.569791262017354\n",
      "\n",
      "Training step  704 \treward  -1.0\n",
      "\n",
      "Training step  705 \treward  61.78435301780701\n",
      "\n",
      "Training step  706 \treward  115.62989586061902\n",
      "\n",
      "Training step  707 \treward  119.25750311745537\n",
      "\n",
      "Training step  708 \treward  0.29052294890085856\n",
      "\n",
      "Training step  709 \treward  103.12054324414996\n",
      "\n",
      "Training step  710 \treward  213.73146259784698\n",
      "\n",
      "Training step  711 \treward  39.19202810525894\n",
      "\n",
      "Training step  712 \treward  24.711996418237685\n",
      "\n",
      "Training step  713 \treward  21.20934633281496\n",
      "\n",
      "Training step  714 \treward  50.03413964807987\n",
      "\n",
      "Training step  715 \treward  103.75448385079702\n",
      "\n",
      "Training step  716 \treward  40.49200757344564\n",
      "\n",
      "Training step  717 \treward  112.50496456027031\n",
      "\n",
      "Training step  718 \treward  37.82865771651268\n",
      "\n",
      "Training step  719 \treward  53.082422052489385\n",
      "\n",
      "Training step  720 \treward  56.852833315730095\n",
      "\n",
      "Training step  721 \treward  40.122004359960556\n",
      "\n",
      "Training step  722 \treward  464.053598344326\n",
      "\n",
      "Training step  723 \treward  371.45024436712265\n",
      "\n",
      "Training step  724 \treward  673.1630684137344\n",
      "\n",
      "Training step  725 \treward  527.8106831908226\n",
      "\n",
      "Training step  726 \treward  407.21255111694336\n",
      "\n",
      "Training step  727 \treward  327.23467272520065\n",
      "\n",
      "Training step  728 \treward  293.64535135030746\n",
      "\n",
      "Training step  729 \treward  252.89049327373505\n",
      "\n",
      "Training step  730 \treward  681.8475165963173\n",
      "\n",
      "Training step  731 \treward  653.217907845974\n",
      "\n",
      "Training step  732 \treward  670.2322922348976\n",
      "\n",
      "Training step  733 \treward  654.4719277024269\n",
      "\n",
      "Training step  734 \treward  651.2871901392937\n",
      "\n",
      "Training step  735 \treward  415.08213925361633\n",
      "\n",
      "Training step  736 \treward  345.08230543136597\n",
      "\n",
      "Training step  737 \treward  366.99381107091904\n",
      "\n",
      "Training step  738 \treward  67.75535923110114\n",
      "\n",
      "Training step  739 \treward  663.8606528043747\n",
      "\n",
      "Training step  740 \treward  92.10701600843005\n",
      "\n",
      "Training step  741 \treward  650.1514040231705\n",
      "\n",
      "Training step  742 \treward  59.84704441229503\n",
      "\n",
      "Training step  743 \treward  612.2190341949463\n",
      "\n",
      "Training step  744 \treward  647.1810603141785\n",
      "\n",
      "Training step  745 \treward  56.735466450452805\n",
      "\n",
      "Training step  746 \treward  122.5283609777689\n",
      "\n",
      "Training step  747 \treward  623.0432652235031\n",
      "\n",
      "Training step  748 \treward  657.2401898503304\n",
      "\n",
      "Training step  749 \treward  617.6409293413162\n",
      "\n",
      "Training step  750 \treward  655.4568647146225\n",
      "\n",
      "Training step  751 \treward  651.0132541060448\n",
      "\n",
      "Training step  752 \treward  648.460271179676\n",
      "\n",
      "Training step  753 \treward  650.826374411583\n",
      "\n",
      "Training step  754 \treward  651.2179944515228\n",
      "\n",
      "Training step  755 \treward  652.0691715478897\n",
      "\n",
      "Training step  756 \treward  650.6574305295944\n",
      "\n",
      "Training step  757 \treward  647.7178475260735\n",
      "\n",
      "Training step  758 \treward  655.2972356081009\n",
      "\n",
      "Training step  759 \treward  652.4442228078842\n",
      "\n",
      "Training step  760 \treward  650.3858587145805\n",
      "\n",
      "Training step  761 \treward  655.6019576191902\n",
      "\n",
      "Training step  762 \treward  654.2278201580048\n",
      "\n",
      "Training step  763 \treward  658.5902104973793\n",
      "\n",
      "Training step  764 \treward  657.2533968687057\n",
      "\n",
      "Training step  765 \treward  -1.0\n",
      "\n",
      "Training step  766 \treward  655.5579468607903\n",
      "\n",
      "Training step  767 \treward  644.4311495423317\n",
      "\n",
      "Training step  768 \treward  16.322241693735123\n",
      "\n",
      "Training step  769 \treward  483.7194781303406\n",
      "\n",
      "Training step  770 \treward  643.0103477239609\n",
      "\n",
      "Training step  771 \treward  366.31290182471275\n",
      "\n",
      "Training step  772 \treward  657.174624145031\n",
      "\n",
      "Training step  773 \treward  653.5759797096252\n",
      "\n",
      "Training step  774 \treward  655.6114529967308\n",
      "\n",
      "Training step  775 \treward  649.3844591975212\n",
      "\n",
      "Training step  776 \treward  655.7129605412483\n",
      "\n",
      "Training step  777 \treward  647.2473624944687\n",
      "\n",
      "Training step  778 \treward  650.899010181427\n",
      "\n",
      "Training step  779 \treward  651.8558605313301\n",
      "\n",
      "Training step  780 \treward  657.2218928933144\n",
      "\n",
      "Training step  781 \treward  645.1769806146622\n",
      "\n",
      "Training step  782 \treward  639.9952775835991\n",
      "\n",
      "Training step  783 \treward  633.1438879966736\n",
      "\n",
      "Training step  784 \treward  636.1250240206718\n",
      "\n",
      "Training step  785 \treward  16.7363041639328\n",
      "\n",
      "Training step  786 \treward  644.6512094140053\n",
      "\n",
      "Training step  787 \treward  651.4156869053841\n",
      "\n",
      "Training step  788 \treward  644.2657524347305\n",
      "\n",
      "Training step  789 \treward  626.4888732433319\n",
      "\n",
      "Training step  790 \treward  639.8011692762375\n",
      "\n",
      "Training step  791 \treward  637.1238993406296\n",
      "\n",
      "Training step  792 \treward  643.4006314873695\n",
      "\n",
      "Training step  793 \treward  645.5285533666611\n",
      "\n",
      "Training step  794 \treward  636.1171400249004\n",
      "\n",
      "Training step  795 \treward  635.7294597029686\n",
      "\n",
      "Training step  796 \treward  637.4968569874763\n",
      "\n",
      "Training step  797 \treward  628.5216539502144\n",
      "\n",
      "Training step  798 \treward  643.2418130040169\n",
      "\n",
      "Training step  799 \treward  637.6350311040878\n",
      "\n",
      "Training step  800 \treward  628.3578322529793\n",
      "\n",
      "Training step  801 \treward  641.5180844068527\n",
      "\n",
      "Training step  802 \treward  640.1345649957657\n",
      "\n",
      "Training step  803 \treward  629.9048672318459\n",
      "\n",
      "Training step  804 \treward  642.2205310463905\n",
      "\n",
      "Training step  805 \treward  646.1291329860687\n",
      "\n",
      "Training step  806 \treward  638.5716640353203\n",
      "\n",
      "Training step  807 \treward  630.2914897203445\n",
      "\n",
      "Training step  808 \treward  633.0521693825722\n",
      "\n",
      "Training step  809 \treward  639.5202969312668\n",
      "\n",
      "Training step  810 \treward  650.4823316931725\n",
      "\n",
      "Training step  811 \treward  646.5310289859772\n",
      "\n",
      "Training step  812 \treward  646.6606494784355\n",
      "\n",
      "Training step  813 \treward  634.8396329283714\n",
      "\n",
      "Training step  814 \treward  643.8137973546982\n",
      "\n",
      "Training step  815 \treward  635.3860357999802\n",
      "\n",
      "Training step  816 \treward  640.4774121046066\n",
      "\n",
      "Training step  817 \treward  632.6097427606583\n",
      "\n",
      "Training step  818 \treward  636.9348068237305\n",
      "\n",
      "Training step  819 \treward  625.5071794986725\n",
      "\n",
      "Training step  820 \treward  639.6489136219025\n",
      "\n",
      "Training step  821 \treward  648.0654186010361\n",
      "\n",
      "Training step  822 \treward  626.3015267848969\n",
      "\n",
      "Training step  823 \treward  645.5065854787827\n",
      "\n",
      "Training step  824 \treward  632.9494044184685\n",
      "\n",
      "Training step  825 \treward  639.1092940568924\n",
      "\n",
      "Training step  826 \treward  639.2835183143616\n",
      "\n",
      "Training step  827 \treward  636.569283246994\n",
      "\n",
      "Training step  828 \treward  637.4786695241928\n",
      "\n",
      "Training step  829 \treward  630.7819370031357\n",
      "\n",
      "Training step  830 \treward  334.6858716607094\n",
      "\n",
      "Training step  831 \treward  634.8226342797279\n",
      "\n",
      "Training step  832 \treward  643.8876379132271\n",
      "\n",
      "Training step  833 \treward  0.0331825315952301\n",
      "\n",
      "Training step  834 \treward  327.6990841627121\n",
      "\n",
      "Training step  835 \treward  337.68891559541225\n",
      "\n",
      "Training step  836 \treward  636.7325791120529\n",
      "\n",
      "Training step  837 \treward  655.8997861146927\n",
      "\n",
      "Training step  838 \treward  326.03659453988075\n",
      "\n",
      "Training step  839 \treward  640.0668992400169\n",
      "\n",
      "Training step  840 \treward  665.9708885550499\n",
      "\n",
      "Training step  841 \treward  32.901917695999146\n",
      "\n",
      "Training step  842 \treward  638.5509003400803\n",
      "\n",
      "Training step  843 \treward  325.6561606526375\n",
      "\n",
      "Training step  844 \treward  641.9024268388748\n",
      "\n",
      "Training step  845 \treward  634.744888484478\n",
      "\n",
      "Training step  846 \treward  641.8635846972466\n",
      "\n",
      "Training step  847 \treward  646.0963497161865\n",
      "\n",
      "Training step  848 \treward  624.3055518865585\n",
      "\n",
      "Training step  849 \treward  639.7856469750404\n",
      "\n",
      "Training step  850 \treward  371.25611594319344\n",
      "\n",
      "Training step  851 \treward  633.212997853756\n",
      "\n",
      "Training step  852 \treward  640.5088806152344\n",
      "\n",
      "Training step  853 \treward  643.4758363962173\n",
      "\n",
      "Training step  854 \treward  650.1124367117882\n",
      "\n",
      "Training step  855 \treward  651.5177853107452\n",
      "\n",
      "Training step  856 \treward  641.3126505613327\n",
      "\n",
      "Training step  857 \treward  123.54293650388718\n",
      "\n",
      "Training step  858 \treward  654.4837061166763\n",
      "\n",
      "Training step  859 \treward  638.4320175647736\n",
      "\n",
      "Training step  860 \treward  630.7073448896408\n",
      "\n",
      "Training step  861 \treward  640.0021013021469\n",
      "\n",
      "Training step  862 \treward  648.2494360804558\n",
      "\n",
      "Training step  863 \treward  630.6451035737991\n",
      "\n",
      "Training step  864 \treward  644.4067490100861\n",
      "\n",
      "Training step  865 \treward  633.8419108390808\n",
      "\n",
      "Training step  866 \treward  653.4262148141861\n",
      "\n",
      "Training step  867 \treward  642.6944230794907\n",
      "\n",
      "Training step  868 \treward  655.1859502196312\n",
      "\n",
      "Training step  869 \treward  637.7035059332848\n",
      "\n",
      "Training step  870 \treward  636.794762134552\n",
      "\n",
      "Training step  871 \treward  657.389969587326\n",
      "\n",
      "Training step  872 \treward  666.16478484869\n",
      "\n",
      "Training step  873 \treward  219.45564832687378\n",
      "\n",
      "Training step  874 \treward  635.2873146831989\n",
      "\n",
      "Training step  875 \treward  207.82896533608437\n",
      "\n",
      "Training step  876 \treward  173.0120674073696\n",
      "\n",
      "Training step  877 \treward  150.3030855357647\n",
      "\n",
      "Training step  878 \treward  644.9534977674484\n",
      "\n",
      "Training step  879 \treward  652.1491332650185\n",
      "\n",
      "Training step  880 \treward  652.8937643766403\n",
      "\n",
      "Training step  881 \treward  687.8118376135826\n",
      "\n",
      "Training step  882 \treward  673.5516175627708\n",
      "\n",
      "Training step  883 \treward  650.3350757956505\n",
      "\n",
      "Training step  884 \treward  358.8118407726288\n",
      "\n",
      "Training step  885 \treward  92.00626955827077\n",
      "\n",
      "Training step  886 \treward  597.6097296476364\n",
      "\n",
      "Training step  887 \treward  683.8219327330589\n",
      "\n",
      "Training step  888 \treward  694.3206163644791\n",
      "\n",
      "Training step  889 \treward  697.0199718475342\n",
      "\n",
      "Training step  890 \treward  673.7804828286171\n",
      "\n",
      "Training step  891 \treward  682.8815052509308\n",
      "\n",
      "Training step  892 \treward  665.3394594192505\n",
      "\n",
      "Training step  893 \treward  669.993506193161\n",
      "\n",
      "Training step  894 \treward  99.39564636150996\n",
      "\n",
      "Training step  895 \treward  697.3528463840485\n",
      "\n",
      "Training step  896 \treward  95.03195476796893\n",
      "\n",
      "Training step  897 \treward  702.5527936220169\n",
      "\n",
      "Training step  898 \treward  690.4811823368073\n",
      "\n",
      "Training step  899 \treward  700.4803109169006\n",
      "\n",
      "Training step  900 \treward  698.2892743349075\n",
      "\n",
      "Training step  901 \treward  692.2513792514801\n",
      "\n",
      "Training step  902 \treward  2.608944922685623\n",
      "\n",
      "Training step  903 \treward  655.44077283144\n",
      "\n",
      "Training step  904 \treward  1.076977789402008\n",
      "\n",
      "Training step  905 \treward  11.919625551170773\n",
      "\n",
      "Training step  906 \treward  8.526254512866338\n",
      "\n",
      "Training step  907 \treward  0.0\n",
      "\n",
      "Training step  908 \treward  0.0\n",
      "\n",
      "Training step  909 \treward  0.0\n",
      "\n",
      "Training step  910 \treward  0.0\n",
      "\n",
      "Training step  911 \treward  468.0798146724701\n",
      "\n",
      "Training step  912 \treward  23.273028854529063\n",
      "\n",
      "Training step  913 \treward  16.727876562542384\n",
      "\n",
      "Training step  914 \treward  39.8304459810257\n",
      "\n",
      "Training step  915 \treward  62.427302183045285\n",
      "\n",
      "Training step  916 \treward  737.8700198531151\n",
      "\n",
      "Training step  917 \treward  723.5487912297249\n",
      "\n",
      "Training step  918 \treward  338.6710974574089\n",
      "\n",
      "Training step  919 \treward  719.4687007665634\n",
      "\n",
      "Training step  920 \treward  720.2870476841927\n",
      "\n",
      "Training step  921 \treward  33.399335373772516\n",
      "\n",
      "Training step  922 \treward  84.8182284189595\n",
      "\n",
      "Training step  923 \treward  522.5221301913261\n",
      "\n",
      "Training step  924 \treward  734.0440217852592\n",
      "\n",
      "Training step  925 \treward  18.955071359872818\n",
      "\n",
      "Training step  926 \treward  68.99277591771549\n",
      "\n",
      "Training step  927 \treward  685.3333553671837\n",
      "\n",
      "Training step  928 \treward  717.6077881455421\n",
      "\n",
      "Training step  929 \treward  -1.0\n",
      "\n",
      "Training step  930 \treward  732.8004266023636\n",
      "\n",
      "Training step  931 \treward  720.9955668449402\n",
      "\n",
      "Training step  932 \treward  740.0033635497093\n",
      "\n",
      "Training step  933 \treward  730.591470003128\n",
      "\n",
      "Training step  934 \treward  718.7328547239304\n",
      "\n",
      "Training step  935 \treward  280.54282492399216\n",
      "\n",
      "Training step  936 \treward  -1.0\n",
      "\n",
      "Training step  937 \treward  735.121844291687\n",
      "\n",
      "Training step  938 \treward  738.8806442022324\n",
      "\n",
      "Training step  939 \treward  396.1085860272249\n",
      "\n",
      "Training step  940 \treward  488.8054309785366\n",
      "\n",
      "Training step  941 \treward  726.2292354106903\n",
      "\n",
      "Training step  942 \treward  675.5166845321655\n",
      "\n",
      "Training step  943 \treward  740.37984842062\n",
      "\n",
      "Training step  944 \treward  739.7469086050987\n",
      "\n",
      "Training step  945 \treward  345.8284673511982\n",
      "\n",
      "Training step  946 \treward  726.1912182569504\n",
      "\n",
      "Training step  947 \treward  740.6223155260086\n",
      "\n",
      "Training step  948 \treward  456.80042025115756\n",
      "\n",
      "Training step  949 \treward  399.0489290992419\n",
      "\n",
      "Training step  950 \treward  474.3560823202133\n",
      "\n",
      "Training step  951 \treward  735.659305691719\n",
      "\n",
      "Training step  952 \treward  540.8986282348633\n",
      "\n",
      "Training step  953 \treward  737.8640975952148\n",
      "\n",
      "Training step  954 \treward  408.51278495788574\n",
      "\n",
      "Training step  955 \treward  710.699457526207\n",
      "\n",
      "Training step  956 \treward  740.4057550430298\n",
      "\n",
      "Training step  957 \treward  504.0786681175232\n",
      "\n",
      "Training step  958 \treward  226.48804999192555\n",
      "\n",
      "Training step  959 \treward  735.6605733633041\n",
      "\n",
      "Training step  960 \treward  737.3144007921219\n",
      "\n",
      "Training step  961 \treward  308.31156838999857\n",
      "\n",
      "Training step  962 \treward  584.5854841172695\n",
      "\n",
      "Training step  963 \treward  136.65641844537524\n",
      "\n",
      "Training step  964 \treward  738.2819920778275\n",
      "\n",
      "Training step  965 \treward  741.2733929157257\n",
      "\n",
      "Training step  966 \treward  737.1802087426186\n",
      "\n",
      "Training step  967 \treward  739.9608992934227\n",
      "\n",
      "Training step  968 \treward  279.48566299676895\n",
      "\n",
      "Training step  969 \treward  738.2426836490631\n",
      "\n",
      "Training step  970 \treward  495.98392021656036\n",
      "\n",
      "Training step  971 \treward  744.3646747469902\n",
      "\n",
      "Training step  972 \treward  685.6265145540237\n",
      "\n",
      "Training step  973 \treward  736.8930344581604\n",
      "\n",
      "Training step  974 \treward  737.5233143568039\n",
      "\n",
      "Training step  975 \treward  738.4126179814339\n",
      "\n",
      "Training step  976 \treward  732.6700674295425\n",
      "\n",
      "Training step  977 \treward  726.8433258533478\n",
      "\n",
      "Training step  978 \treward  137.7606862001949\n",
      "\n",
      "Training step  979 \treward  56.829657997025386\n",
      "\n",
      "Training step  980 \treward  70.8992368777593\n",
      "\n",
      "Training step  981 \treward  146.5775295138359\n",
      "\n",
      "Training step  982 \treward  731.9087492227554\n",
      "\n",
      "Training step  983 \treward  601.4642509818077\n",
      "\n",
      "Training step  984 \treward  734.2772088050842\n",
      "\n",
      "Training step  985 \treward  737.9676413536072\n",
      "\n",
      "Training step  986 \treward  735.4855934381485\n",
      "\n",
      "Training step  987 \treward  478.4779751598835\n",
      "\n",
      "Training step  988 \treward  115.35689589712355\n",
      "\n",
      "Training step  989 \treward  727.7397693991661\n",
      "\n",
      "Training step  990 \treward  19.086296961042617\n",
      "\n",
      "Training step  991 \treward  747.5689197182655\n",
      "\n",
      "Training step  992 \treward  50.75509762763977\n",
      "\n",
      "Training step  993 \treward  13.569133669137955\n",
      "\n",
      "Training step  994 \treward  68.91174274682999\n",
      "\n",
      "Training step  995 \treward  50.6388897895813\n",
      "\n",
      "Training step  996 \treward  728.78913885355\n",
      "\n",
      "Training step  997 \treward  747.3222032785416\n",
      "\n",
      "Training step  998 \treward  730.0277841687202\n",
      "\n",
      "Training step  999 \treward  280.2149056196213\n",
      "\n",
      "Training step  1000 \treward  101.44898067514102\n",
      "\n",
      "Training step  1001 \treward  -1.0\n",
      "\n",
      "Training step  1002 \treward  637.0952902138233\n",
      "\n",
      "Training step  1003 \treward  746.8167441487312\n",
      "\n",
      "Training step  1004 \treward  736.0826561450958\n",
      "\n",
      "Training step  1005 \treward  714.26904463768\n",
      "\n",
      "Training step  1006 \treward  -1.0\n",
      "\n",
      "Training step  1007 \treward  726.1411794424057\n",
      "\n",
      "Training step  1008 \treward  -1.0\n",
      "\n",
      "Training step  1009 \treward  -1.0\n",
      "\n",
      "Training step  1010 \treward  -1.0\n",
      "\n",
      "Training step  1011 \treward  -1.0\n",
      "\n",
      "Training step  1012 \treward  -1.0\n",
      "\n",
      "Training step  1013 \treward  -1.0\n",
      "\n",
      "Training step  1014 \treward  -1.0\n",
      "\n",
      "Training step  1015 \treward  721.3835043907166\n",
      "\n",
      "Training step  1016 \treward  526.972813308239\n",
      "\n",
      "Training step  1017 \treward  -1.0\n",
      "\n",
      "Training step  1018 \treward  -1.0\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-89c664dbca9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0mexperiences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0mTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_q_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m   \u001b[0mcumulative_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training step \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\treward \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-dee542e9134d>\u001b[0m in \u001b[0;36mgenerate_trajectories\u001b[0;34m(env, q_net, buffer_size, epsilon)\u001b[0m\n\u001b[1;32m    106\u001b[0m       \u001b[0;31m# Generate an action for all the Agents that requested a decision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0;31m# Compute the values for each action given the observation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0mact1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecision_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecision_steps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e0263dfb9e92>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, visual_obs)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "# This code is used to close an env that might not have been closed before\n",
    "try:\n",
    "  env.close()\n",
    "except:\n",
    "  pass\n",
    "# -----------------\n",
    "\n",
    "from mlagents_envs.registry import default_registry\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create the GridWorld Environment from the registry\n",
    "env = UE(file_name='run32_training', seed=1, worker_id=2, side_channels=[])\n",
    "# env = default_registry[\"GridWorld\"].make()\n",
    "print(\"GridWorld environment created.\")\n",
    "\n",
    "# moved Qnet outside to reuse it\n",
    "\n",
    "# Create a new Q-Network. \n",
    "qnet = VisualQNetwork((44, 1), 126, 3)\n",
    "\n",
    "experiences: Buffer = []\n",
    "optim = torch.optim.Adam(qnet.parameters(), lr= 0.001)\n",
    "\n",
    "cumulative_rewards: List[float] = []\n",
    "\n",
    "# The number of training steps that will be performed\n",
    "NUM_TRAINING_STEPS = 10000000 #70\n",
    "# The number of experiences to collect per training step\n",
    "NUM_NEW_EXP = 1000\n",
    "# The maximum size of the Buffer\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "for n in range(NUM_TRAINING_STEPS):\n",
    "  new_exp,_ = Trainer.generate_trajectories(env, qnet, NUM_NEW_EXP, epsilon=0.1)\n",
    "  random.shuffle(experiences)\n",
    "  if len(experiences) > BUFFER_SIZE:\n",
    "    experiences = experiences[:BUFFER_SIZE]\n",
    "  experiences.extend(new_exp)\n",
    "  Trainer.update_q_net(qnet, optim, experiences, 3)\n",
    "  _, rewards = Trainer.generate_trajectories(env, qnet, 100, epsilon=0)\n",
    "  cumulative_rewards.append(rewards)\n",
    "  print(\"Training step \", n+1, \"\\treward \", rewards)\n",
    "  print()\n",
    "\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Show the training graph\n",
    "plt.plot(range(NUM_TRAINING_STEPS), cumulative_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2a79ffcf40>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1018), cumulative_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed environment\n"
     ]
    }
   ],
   "source": [
    "env.close()\n",
    "print(\"Closed environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.4.2-cp37-cp37m-macosx_10_9_x86_64.whl (7.2 MB)\n",
      "\u001b[K     || 7.2 MB 743 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp37-cp37m-macosx_10_9_x86_64.whl (61 kB)\n",
      "\u001b[K     || 61 kB 441 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/ribr/.virtualenvs/ultron/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.16 in /Users/ribr/.virtualenvs/ultron/lib/python3.7/site-packages (from matplotlib) (1.20.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/ribr/.virtualenvs/ultron/lib/python3.7/site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/ribr/.virtualenvs/ultron/lib/python3.7/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six in /Users/ribr/.virtualenvs/ultron/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n",
      "Installing collected packages: kiwisolver, cycler, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.3.1 matplotlib-3.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
