{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19e91d3-b254-404a-b3bd-72139a38191a",
   "metadata": {},
   "source": [
    "# Register Hunmingbird Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47451740-e4f0-408c-91ab-0edef336b46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Hummingbird Environments:\n",
      " HumMultiagent\n",
      " HumMultiagentNovisual\n",
      " HumSingleagent\n",
      " HumSingleAgentNovisual\n",
      " BirdSingleAgentNoStacked\n",
      " BirdSingleAgentNoStackedNoVisual\n"
     ]
    }
   ],
   "source": [
    "import unity_env_wrapper\n",
    "unity_env_wrapper.get_all_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd29f56d-1a98-4f48-af0d-0986ac2ff21a",
   "metadata": {},
   "source": [
    "# Simple Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6380759-f4be-47ca-90a5-8cf0b49304ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected to Unity environment with package version 2.0.0-exp.1 and communication version 1.5.0\n",
      "[INFO] Connected new brain: Hummingbird?team=0\n",
      "[WARNING] The environment contains multiple observations. You must define allow_multiple_obs=True to receive them all. Otherwise, only the first visual observation (or vector observation ifthere are no visual observations) will be provided in the observation.\n",
      "Using cuda device\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005760595 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.87       |\n",
      "|    explained_variance   | 0.0671      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 79.9        |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.00847    |\n",
      "|    value_loss           | 2.02e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'reset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c1f76b42baca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Evaluate the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mmean_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_eval_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Enjoy trained agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py\u001b[0m in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Avoid double reset, as VecEnv are reset automatically.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVecEnv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnot_reseted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mnot_reseted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reset'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Set a model id for saving\n",
    "model_id = \"sb_ppo_HumSingleNoStacked\"\n",
    "\n",
    "# Create environment\n",
    "# env = gym.make(\"BirdSingleAgentNoStacked-v1\")\n",
    "env = DummyVecEnv([lambda: gym.make(\"BirdSingleAgentNoStacked-v1\")])\n",
    "\n",
    "# Instantiate the agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1) # model = DQN('MlpPolicy', env, learning_rate=1e-3, prioritized_replay=True, verbose=1)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(2e3))\n",
    "# Save the agent\n",
    "model.save(model_id)\n",
    "del model  # delete trained model to demonstrate loading\n",
    "\n",
    "# Load the trained agent\n",
    "model = PPO.load(model_id)\n",
    "\n",
    "# Evaluate the agent\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "\n",
    "# Enjoy trained agent\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bb86fb3-457c-4ea4-95a6-f86e459f6ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56da1b2-2600-4533-974f-526386363424",
   "metadata": {},
   "source": [
    "# Vectorized Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d198ad0c-37b3-4e41-bc15-9c97e85473ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-994947833d97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-994947833d97>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mnum_cpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m  \u001b[0;31m# Number of processes to use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Create the vectorized environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSubprocVecEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_cpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MlpPolicy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_fns, start_method)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremotes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwork_remotes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mwork_remote\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremote\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_fn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwork_remotes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremotes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "def make_env(env_id, rank, seed=0):\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "\n",
    "    :param env_id: (str) the environment ID\n",
    "    :param num_env: (int) the number of environments you wish to have in subprocesses\n",
    "    :param seed: (int) the inital seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = gym.make(env_id)\n",
    "        env.seed(seed + rank)\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init\n",
    "\n",
    "def main():\n",
    "#     env_id = \"CartPole-v1\"\n",
    "    env_id = \"BirdSingleAgentNoStackedNoVisual-v1\"\n",
    "    model_id = \"sb_ppo_HumSingleNoStacked_vectorized\"\n",
    "\n",
    "    num_cpu = 1  # Number of processes to use\n",
    "    # Create the vectorized environment\n",
    "    env = SubprocVecEnv([make_env(env_id, i) for i in range(num_cpu)])\n",
    "\n",
    "    model = PPO('MlpPolicy', env, verbose=1)\n",
    "    model.learn(total_timesteps=25000)\n",
    "    \n",
    "    # Save the agent\n",
    "    model.save(model_id)\n",
    "    del model  # delete trained model to demonstrate loading\n",
    "\n",
    "    # Load the trained agent\n",
    "    model = PPO.load(model_id)\n",
    "\n",
    "    # Evaluate the agent\n",
    "    mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "\n",
    "    # Enjoy trained agent\n",
    "    obs = env.reset()\n",
    "    for i in range(1000):\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "        env.render()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bdb2e3-f947-4962-8213-3c760099e6d1",
   "metadata": {},
   "source": [
    "# Monitoring Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ab5abeee-47dc-442b-9948-b12a46d00005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected to Unity environment with package version 2.0.0-exp.1 and communication version 1.5.0\n",
      "[INFO] Connected new brain: Hummingbird?team=0\n",
      "[WARNING] The environment contains multiple observations. You must define allow_multiple_obs=True to receive them all. Otherwise, only the first visual observation (or vector observation ifthere are no visual observations) will be provided in the observation.\n",
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: 0.00\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 2000\n",
      "Best mean reward: 0.00 - Last mean reward per episode: 483.56\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 3000\n",
      "Best mean reward: 483.56 - Last mean reward per episode: 677.71\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 4000\n",
      "Best mean reward: 677.71 - Last mean reward per episode: 508.28\n",
      "Num timesteps: 5000\n",
      "Best mean reward: 677.71 - Last mean reward per episode: 477.07\n",
      "Num timesteps: 6000\n",
      "Best mean reward: 677.71 - Last mean reward per episode: 494.31\n",
      "Num timesteps: 7000\n",
      "Best mean reward: 677.71 - Last mean reward per episode: 469.07\n",
      "Num timesteps: 8000\n",
      "Best mean reward: 677.71 - Last mean reward per episode: 457.66\n",
      "Num timesteps: 9000\n",
      "Best mean reward: 677.71 - Last mean reward per episode: 411.89\n",
      "Num timesteps: 10000\n",
      "Best mean reward: 677.71 - Last mean reward per episode: 406.74\n",
      "Num timesteps: 11000\n",
      "Best mean reward: 677.71 - Last mean reward per episode: 462.60\n",
      "Num timesteps: 12000\n",
      "Best mean reward: 677.71 - Last mean reward per episode: 571.25\n",
      "Num timesteps: 13000\n",
      "Best mean reward: 677.71 - Last mean reward per episode: 717.64\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 14000\n",
      "Best mean reward: 717.64 - Last mean reward per episode: 835.05\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 15000\n",
      "Best mean reward: 835.05 - Last mean reward per episode: 840.06\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 16000\n",
      "Best mean reward: 840.06 - Last mean reward per episode: 808.86\n",
      "Num timesteps: 17000\n",
      "Best mean reward: 840.06 - Last mean reward per episode: 920.46\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 18000\n",
      "Best mean reward: 920.46 - Last mean reward per episode: 913.25\n",
      "Num timesteps: 19000\n",
      "Best mean reward: 920.46 - Last mean reward per episode: 927.84\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 20000\n",
      "Best mean reward: 927.84 - Last mean reward per episode: 960.51\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 21000\n",
      "Best mean reward: 960.51 - Last mean reward per episode: 974.28\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 22000\n",
      "Best mean reward: 974.28 - Last mean reward per episode: 1040.55\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 23000\n",
      "Best mean reward: 1040.55 - Last mean reward per episode: 1060.39\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 24000\n",
      "Best mean reward: 1060.39 - Last mean reward per episode: 1122.69\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 25000\n",
      "Best mean reward: 1122.69 - Last mean reward per episode: 1143.06\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 26000\n",
      "Best mean reward: 1143.06 - Last mean reward per episode: 1143.26\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 27000\n",
      "Best mean reward: 1143.26 - Last mean reward per episode: 1198.89\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 28000\n",
      "Best mean reward: 1198.89 - Last mean reward per episode: 1193.00\n",
      "Num timesteps: 29000\n",
      "Best mean reward: 1198.89 - Last mean reward per episode: 1215.17\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 30000\n",
      "Best mean reward: 1215.17 - Last mean reward per episode: 1195.28\n",
      "Num timesteps: 31000\n",
      "Best mean reward: 1215.17 - Last mean reward per episode: 1250.98\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 32000\n",
      "Best mean reward: 1250.98 - Last mean reward per episode: 1275.11\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 33000\n",
      "Best mean reward: 1275.11 - Last mean reward per episode: 1340.52\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 34000\n",
      "Best mean reward: 1340.52 - Last mean reward per episode: 1329.33\n",
      "Num timesteps: 35000\n",
      "Best mean reward: 1340.52 - Last mean reward per episode: 1382.76\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 36000\n",
      "Best mean reward: 1382.76 - Last mean reward per episode: 1443.47\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 37000\n",
      "Best mean reward: 1443.47 - Last mean reward per episode: 1460.90\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 38000\n",
      "Best mean reward: 1460.90 - Last mean reward per episode: 1488.39\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 39000\n",
      "Best mean reward: 1488.39 - Last mean reward per episode: 1523.66\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 40000\n",
      "Best mean reward: 1523.66 - Last mean reward per episode: 1561.89\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 41000\n",
      "Best mean reward: 1561.89 - Last mean reward per episode: 1534.95\n",
      "Num timesteps: 42000\n",
      "Best mean reward: 1561.89 - Last mean reward per episode: 1549.25\n",
      "Num timesteps: 43000\n",
      "Best mean reward: 1561.89 - Last mean reward per episode: 1533.27\n",
      "Num timesteps: 44000\n",
      "Best mean reward: 1561.89 - Last mean reward per episode: 1547.71\n",
      "Num timesteps: 45000\n",
      "Best mean reward: 1561.89 - Last mean reward per episode: 1588.96\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 46000\n",
      "Best mean reward: 1588.96 - Last mean reward per episode: 1615.27\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 47000\n",
      "Best mean reward: 1615.27 - Last mean reward per episode: 1652.05\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 48000\n",
      "Best mean reward: 1652.05 - Last mean reward per episode: 1681.31\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 49000\n",
      "Best mean reward: 1681.31 - Last mean reward per episode: 1722.68\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 50000\n",
      "Best mean reward: 1722.68 - Last mean reward per episode: 1738.44\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 51000\n",
      "Best mean reward: 1738.44 - Last mean reward per episode: 1772.24\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 52000\n",
      "Best mean reward: 1772.24 - Last mean reward per episode: 1812.99\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 53000\n",
      "Best mean reward: 1812.99 - Last mean reward per episode: 1818.38\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 54000\n",
      "Best mean reward: 1818.38 - Last mean reward per episode: 1860.44\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 55000\n",
      "Best mean reward: 1860.44 - Last mean reward per episode: 1898.21\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 56000\n",
      "Best mean reward: 1898.21 - Last mean reward per episode: 1916.38\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 57000\n",
      "Best mean reward: 1916.38 - Last mean reward per episode: 1941.37\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 58000\n",
      "Best mean reward: 1941.37 - Last mean reward per episode: 1966.95\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 59000\n",
      "Best mean reward: 1966.95 - Last mean reward per episode: 1995.93\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 60000\n",
      "Best mean reward: 1995.93 - Last mean reward per episode: 2028.01\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 61000\n",
      "Best mean reward: 2028.01 - Last mean reward per episode: 2060.02\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 62000\n",
      "Best mean reward: 2060.02 - Last mean reward per episode: 2084.90\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 63000\n",
      "Best mean reward: 2084.90 - Last mean reward per episode: 2117.45\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 64000\n",
      "Best mean reward: 2117.45 - Last mean reward per episode: 2120.83\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 65000\n",
      "Best mean reward: 2120.83 - Last mean reward per episode: 2150.70\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 66000\n",
      "Best mean reward: 2150.70 - Last mean reward per episode: 2172.39\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 67000\n",
      "Best mean reward: 2172.39 - Last mean reward per episode: 2197.69\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 68000\n",
      "Best mean reward: 2197.69 - Last mean reward per episode: 2228.38\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 69000\n",
      "Best mean reward: 2228.38 - Last mean reward per episode: 2257.57\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 70000\n",
      "Best mean reward: 2257.57 - Last mean reward per episode: 2276.21\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 71000\n",
      "Best mean reward: 2276.21 - Last mean reward per episode: 2259.76\n",
      "Num timesteps: 72000\n",
      "Best mean reward: 2276.21 - Last mean reward per episode: 2281.75\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 73000\n",
      "Best mean reward: 2281.75 - Last mean reward per episode: 2309.42\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 74000\n",
      "Best mean reward: 2309.42 - Last mean reward per episode: 2328.49\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 75000\n",
      "Best mean reward: 2328.49 - Last mean reward per episode: 2343.27\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 76000\n",
      "Best mean reward: 2343.27 - Last mean reward per episode: 2368.27\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 77000\n",
      "Best mean reward: 2368.27 - Last mean reward per episode: 2390.10\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 78000\n",
      "Best mean reward: 2390.10 - Last mean reward per episode: 2408.33\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 79000\n",
      "Best mean reward: 2408.33 - Last mean reward per episode: 2450.34\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 80000\n",
      "Best mean reward: 2450.34 - Last mean reward per episode: 2486.21\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 81000\n",
      "Best mean reward: 2486.21 - Last mean reward per episode: 2524.46\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 82000\n",
      "Best mean reward: 2524.46 - Last mean reward per episode: 2572.74\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 83000\n",
      "Best mean reward: 2572.74 - Last mean reward per episode: 2613.50\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 84000\n",
      "Best mean reward: 2613.50 - Last mean reward per episode: 2651.24\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 85000\n",
      "Best mean reward: 2651.24 - Last mean reward per episode: 2695.63\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 86000\n",
      "Best mean reward: 2695.63 - Last mean reward per episode: 2741.18\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 87000\n",
      "Best mean reward: 2741.18 - Last mean reward per episode: 2785.34\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 88000\n",
      "Best mean reward: 2785.34 - Last mean reward per episode: 2837.67\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 89000\n",
      "Best mean reward: 2837.67 - Last mean reward per episode: 2873.53\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 90000\n",
      "Best mean reward: 2873.53 - Last mean reward per episode: 2906.85\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 91000\n",
      "Best mean reward: 2906.85 - Last mean reward per episode: 2951.30\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 92000\n",
      "Best mean reward: 2951.30 - Last mean reward per episode: 2989.31\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 93000\n",
      "Best mean reward: 2989.31 - Last mean reward per episode: 3000.12\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 94000\n",
      "Best mean reward: 3000.12 - Last mean reward per episode: 3017.82\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 95000\n",
      "Best mean reward: 3017.82 - Last mean reward per episode: 3057.75\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 96000\n",
      "Best mean reward: 3057.75 - Last mean reward per episode: 3094.79\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 97000\n",
      "Best mean reward: 3094.79 - Last mean reward per episode: 3139.79\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 98000\n",
      "Best mean reward: 3139.79 - Last mean reward per episode: 3165.09\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 99000\n",
      "Best mean reward: 3165.09 - Last mean reward per episode: 3193.72\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 100000\n",
      "Best mean reward: 3193.72 - Last mean reward per episode: 3233.55\n",
      "Saving new best model to tmp/best_model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from stable_baselines3 import TD3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "\n",
    "\n",
    "\n",
    "# -----------------\n",
    "try:\n",
    "    unity_env.close()\n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "# -----------------\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "# env = gym.make('LunarLanderContinuous-v2')\n",
    "unity_env = UnityEnvironment(file_name=\"../run32_nostacked/run32_nostacked\")\n",
    "env = UnityToGymWrapper(unity_env, uint8_visual=False, allow_multiple_obs=False)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "env = gym.make(\"BirdSingleAgentNoStacked-v1\")\n",
    "\n",
    "\n",
    "# Add some action noise for exploration\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "# Because we use parameter noise, we should use a MlpPolicy with layer normalization\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "# model = TD3('MlpPolicy', env, action_noise=action_noise, verbose=0)\n",
    "\n",
    "# Define Timesteps\n",
    "timesteps = 1e5\n",
    "# Create the callback\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=int(timesteps/5), log_dir=log_dir)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(timesteps), callback=callback)\n",
    "\n",
    "plot_results([log_dir], timesteps, results_plotter.X_TIMESTEPS, \"TD3 LunarLander\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a7f902c7-babf-4b56-a35c-0798e0ac45f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stable-baselines[mpi]\n",
      "  Downloading stable_baselines-2.10.2-py3-none-any.whl (240 kB)\n",
      "\u001b[K     |████████████████████████████████| 240 kB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /home/ros/.local/lib/python3.8/site-packages (from stable-baselines[mpi]) (1.0.1)\n",
      "Requirement already satisfied: scipy in /home/ros/.local/lib/python3.8/site-packages (from stable-baselines[mpi]) (1.6.3)\n",
      "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /home/ros/.local/lib/python3.8/site-packages (from stable-baselines[mpi]) (0.18.3)\n",
      "Requirement already satisfied: numpy in /home/ros/.local/lib/python3.8/site-packages (from stable-baselines[mpi]) (1.20.3)\n",
      "Requirement already satisfied: cloudpickle>=0.5.5 in /home/ros/.local/lib/python3.8/site-packages (from stable-baselines[mpi]) (1.6.0)\n",
      "Requirement already satisfied: matplotlib in /home/ros/.local/lib/python3.8/site-packages (from stable-baselines[mpi]) (3.4.2)\n",
      "Requirement already satisfied: pandas in /home/ros/.local/lib/python3.8/site-packages (from stable-baselines[mpi]) (1.2.4)\n",
      "Requirement already satisfied: opencv-python in /home/ros/.local/lib/python3.8/site-packages (from stable-baselines[mpi]) (4.5.2.52)\n",
      "Requirement already satisfied: mpi4py; extra == \"mpi\" in /usr/lib/python3/dist-packages (from stable-baselines[mpi]) (3.0.3)\n",
      "Requirement already satisfied: Pillow<=8.2.0 in /home/ros/.local/lib/python3.8/site-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (8.2.0)\n",
      "Requirement already satisfied: pyglet<=1.5.15,>=1.4.0 in /home/ros/.local/lib/python3.8/site-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (1.5.15)\n",
      "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /home/ros/.local/lib/python3.8/site-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (0.2.9)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/ros/.local/lib/python3.8/site-packages (from matplotlib->stable-baselines[mpi]) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ros/.local/lib/python3.8/site-packages (from matplotlib->stable-baselines[mpi]) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ros/.local/lib/python3.8/site-packages (from matplotlib->stable-baselines[mpi]) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ros/.local/lib/python3.8/site-packages (from matplotlib->stable-baselines[mpi]) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ros/.local/lib/python3.8/site-packages (from pandas->stable-baselines[mpi]) (2021.1)\n",
      "Requirement already satisfied: six in /home/ros/.local/lib/python3.8/site-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (1.16.0)\n",
      "Installing collected packages: stable-baselines\n",
      "Successfully installed stable-baselines-2.10.2\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines[mpi]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ef5b955-0f7a-4941-931f-a42d3a970f21",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'VecMonitor' from 'stable_baselines3.common.vec_env' (/home/ros/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-91ccd8c759fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMonitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVecMonitor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVecVideoRecorder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HumSingleagent-v1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'VecMonitor' from 'stable_baselines3.common.vec_env' (/home/ros/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/__init__.py)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import VecMonitor, VecVideoRecorder, DummyVecEnv\n",
    "env = DummyVecEnv([lambda: gym.make(\"HumSingleagent-v1\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5128afca-6cf0-486f-a0c6-bf93092b97b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py\t dummy_vec_env.py     util.py\t\t  vec_normalize.py\n",
      "__pycache__\t obs_dict_wrapper.py  vec_check_nan.py\t  vec_transpose.py\n",
      "base_vec_env.py  subproc_vec_env.py   vec_frame_stack.py  vec_video_recorder.py\n"
     ]
    }
   ],
   "source": [
    "!ls /home/ros/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f5eefc-b56c-4a2e-9b2f-29bc35936d26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
